{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pysndfx SoundFile audiomentations pretrainedmodels efficientnet_pytorch resnest","metadata":{"id":"Yn1Ybf15VAqW","execution":{"iopub.status.busy":"2021-05-31T18:03:23.002657Z","iopub.execute_input":"2021-05-31T18:03:23.002972Z","iopub.status.idle":"2021-05-31T18:03:35.406557Z","shell.execute_reply.started":"2021-05-31T18:03:23.002942Z","shell.execute_reply":"2021-05-31T18:03:35.405574Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# No Random Sampling, only first 5 seconds.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install colorednoise\nimport colorednoise as cn\nimport numpy as np\nimport librosa as lb\nimport librosa\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport librosa.display as lbd\nimport soundfile as sf\nfrom  soundfile import SoundFile\nimport pandas as pd\nfrom  IPython.display import Audio\nfrom pathlib import Path\n\nimport glob\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom  torch.utils.data import Dataset, DataLoader\nimport torchvision\n\nfrom resnest.torch import resnest50\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json\nfrom  ast import literal_eval\n\n\nfrom IPython.display import Audio\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.notebook import tqdm\nimport joblib\nimport pytorch_lightning as pl\n\nfrom efficientnet_pytorch import EfficientNet\nimport pretrainedmodels\nfrom resnest.torch import resnest50_fast_1s1x64d\nfrom resnest.torch import resnest50\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\nfrom fastai.vision.all import *","metadata":{"id":"2dt7oG43VAqc","execution":{"iopub.status.busy":"2021-05-31T18:03:35.409967Z","iopub.execute_input":"2021-05-31T18:03:35.410251Z","iopub.status.idle":"2021-05-31T18:03:47.227062Z","shell.execute_reply.started":"2021-05-31T18:03:35.410223Z","shell.execute_reply":"2021-05-31T18:03:47.226137Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config Vars","metadata":{}},{"cell_type":"code","source":"NUM_CLASSES = 397 + 1\nSR = 32_000\nDURATION = 7\nMAX_READ_SAMPLES = 1 \naudio_image_store = None\nDATA_ROOT = Path(\"../input/birdclef-2021\")\n\n\nMEL_PATHS = sorted(Path(\"../input\").glob(\"kkiller-birdclef-mels-computer-d7-part?/rich_train_metadata.csv\"))\nTRAIN_LABEL_PATHS = sorted(Path(\"../input\").glob(\"kkiller-birdclef-mels-computer-d7-part?/LABEL_IDS.json\"))\nSOUNDSCAPES_PATH = '../input/birdclef-2021/train_soundscapes/'\nBACKGROUND_PATH = '../input/birdclef-background/audio_images/nocall/'\nBACKGROUND_CSV_PATH = '../input/birdclef-background/rich_train_metadata.csv'\nBACKGROUND_CSV = pd.read_csv(BACKGROUND_CSV_PATH).set_index(\"Unnamed: 0\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nResNestPath = '../input/birds-cp-1/resnest50_fast_1s1x64d_conf_0.pt'\nResNextPath = '../input/birds-cp-2/resnext50_32x4d__0.pt'\nCNN14Path = '../input/birdcall-pannsatt-aux-weak/best.pth'\nexternal_soundscapes = '../input/trainsoundscapes/audio_images/'\nexternal_soundscapes_csv = pd.read_csv('../input/trainsoundscapes/rich_train_metadata.csv')\nexternal_soundscapes_csv['secondary_labels'] = external_soundscapes_csv['second_labels']\ndel external_soundscapes_csv['second_labels']\nexternal_soundscapes_csv = external_soundscapes_csv.drop([np.where(external_soundscapes_csv['filename'] == 'SSW49_595')[0].item()])\nexternal_soundscapes_csv = external_soundscapes_csv.set_index(\"Unnamed: 0\")\n\nclass DataConfig:\n    # Stores Config vars pertaining to data\n    soundscapes_val = False # Means to put all of the soundscapes(training) as your vlaidation set\n    # IF False, validation becomes split between soundscapes(80/20 split) and monophone is added too.\n\nprint(\"Device:\", DEVICE)","metadata":{"id":"Iu56f-7VVAqf","outputId":"0f3fa344-0ed4-47d8-f3c0-218cf3bf5a78","execution":{"iopub.status.busy":"2021-05-31T18:03:47.22901Z","iopub.execute_input":"2021-05-31T18:03:47.229397Z","iopub.status.idle":"2021-05-31T18:03:47.292046Z","shell.execute_reply.started":"2021-05-31T18:03:47.229359Z","shell.execute_reply":"2021-05-31T18:03:47.290974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Up DF Functions","metadata":{}},{"cell_type":"code","source":"def clean_df(df):\n    # Cleans DataFrames from any erroneous entroes.\n    to_drop = []\n    for idx, row in tqdm(enumerate(df.iterrows())):\n        row = row[1]\n        if len(glob.glob(str(row.impath))) == 1:\n            continue\n        to_drop += [idx]\n    df = df.drop(to_drop)\n    return df\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:03:47.293554Z","iopub.execute_input":"2021-05-31T18:03:47.293883Z","iopub.status.idle":"2021-05-31T18:03:47.300424Z","shell.execute_reply.started":"2021-05-31T18:03:47.293849Z","shell.execute_reply":"2021-05-31T18:03:47.298361Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df(mel_paths=MEL_PATHS, train_label_paths=TRAIN_LABEL_PATHS):\n  df = None\n  LABEL_IDS = {}\n    \n  for file_path in mel_paths:\n    temp = pd.read_csv(str(file_path), index_col=0)\n    temp[\"impath\"] = temp.apply(lambda row: file_path.parent/\"audio_images/{}/{}.npy\".format(row.primary_label, row.filename), axis=1) \n    df = temp if df is None else df.append(temp)\n    \n  df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n\n  for file_path in train_label_paths:\n    with open(str(file_path)) as f:\n      LABEL_IDS.update(json.load(f))\n  LABEL_IDS['nocall'] = NUM_CLASSES - 1\n  return LABEL_IDS, df\nLABEL_IDS, df = get_df()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:03:47.301823Z","iopub.execute_input":"2021-05-31T18:03:47.302245Z","iopub.status.idle":"2021-05-31T18:03:50.880773Z","shell.execute_reply.started":"2021-05-31T18:03:47.302205Z","shell.execute_reply":"2021-05-31T18:03:50.87989Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_background(df):\n    impath = []\n    label_id = []\n    nocall_id = NUM_CLASSES - 1\n    secondary_class = []\n    all_background = glob.glob(f\"{BACKGROUND_PATH}*\")\n    to_drop = []\n    for idx, row in enumerate(df.iterrows()):\n        row = row[1]\n        path = BACKGROUND_PATH + row.filename + '.npy'\n        if path not in all_background:\n            to_drop += [idx]\n            continue\n        impath += [path]\n        label_id += [nocall_id]\n        secondary_class += [[]]\n    df = df.drop(to_drop)\n    df['impath'] = impath\n    df['label_id'] = label_id\n    df['secondary_labels'] = secondary_class\n    return df\ndef preprocess_external(df, base_path):\n    # Preprocesses the External Dataset into the correct format.\n    impath = []\n    label_id = []\n    secondary_label = []\n    for row in tqdm(df.iterrows()):\n        row = row[1]\n        path = f'{base_path}{row.primary_label}/{row.filename}.npy'\n        impath += [path]\n        label_id += [LABEL_IDS[row.primary_label]]\n    df['impath'] = impath\n    df['label_id'] = label_id\n    df['secondary_labels'] = [[]] * len(df)\n    return df\ndef preprocess_soundscapes(df):\n    # In order to model the test set, you need soundscapes.\n    impath = []\n    label_id = []\n    site = []\n    for row in df.iterrows():\n        row = row[1]\n        path = f'{external_soundscapes}{row.filename}.npy'\n        if 'SSW' in path:\n            site += ['SSW']\n        else:\n            site += ['COR']\n        impath += [path]\n        classes = row.primary_label.split()\n        im_id = None\n        classes_added = []\n        for class_name in classes:\n            if im_id is None:\n                im_id = str(LABEL_IDS[class_name]) \n                classes_added += [class_name]\n            else:\n                if class_name not in classes_added:\n                    im_id += f' {LABEL_IDS[class_name]}'\n                    classes_added += [class_name]\n        label_id += [im_id]\n    df['impath'] = impath \n    df['label_id'] = label_id\n    df['site'] = site\n    # Segregate into External Soundscapes and Train Sound Scapes\n    train_idx = []\n    external_idx = []\n    for idx, row in enumerate(df.iterrows()):\n        row = row[1]\n        filename = row.filename\n        # Strip the base from the name\n        base_filename = ''\n        for i in range(len(filename) - 1, -1, -1):\n            if filename[i] == '_':\n                base_filename = filename[:i]\n                break\n        # Check if it exists\n        num_files = len(glob.glob(f\"{SOUNDSCAPES_PATH}{base_filename}*\"))\n        if num_files > 0:\n            train_idx += [idx]\n        else:\n            external_idx += [idx]\n    train = df.iloc[train_idx]\n    external = df.iloc[external_idx] \n            \n    return train, external","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:03:50.882964Z","iopub.execute_input":"2021-05-31T18:03:50.883235Z","iopub.status.idle":"2021-05-31T18:03:50.899433Z","shell.execute_reply.started":"2021-05-31T18:03:50.883209Z","shell.execute_reply":"2021-05-31T18:03:50.8986Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Up Dfs","metadata":{}},{"cell_type":"code","source":"BACKGROUND_CSV = preprocess_background(BACKGROUND_CSV)\nexternal_soundscapes_csv, extra_soundscapes_csv = preprocess_soundscapes(external_soundscapes_csv)\n#external_data_csv = preprocess_external(external_data_csv, external_data)\n#external_data2_csv = preprocess_external(external_data2_csv, external_data2)\n#external_data3_csv = preprocess_external(external_data3_csv, external_data3)\n\n\ndf = clean_df(df)\nBACKGROUND_CSV = clean_df(BACKGROUND_CSV)\nexternal_soundscapes_csv =clean_df(external_soundscapes_csv)\nextra_soundscapes_csv = clean_df(extra_soundscapes_csv)\n#external_data_csv = clean_df(external_data_csv)\n#external_data2_csv = clean_df(external_data2_csv)\n#external_data3_csv = clean_df(external_data3_csv)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:03:50.900733Z","iopub.execute_input":"2021-05-31T18:03:50.901239Z","iopub.status.idle":"2021-05-31T18:05:47.746289Z","shell.execute_reply.started":"2021-05-31T18:03:50.901202Z","shell.execute_reply":"2021-05-31T18:05:47.745456Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Append Extra Data","metadata":{}},{"cell_type":"code","source":"# Append Extra Data\ndf = df.append(BACKGROUND_CSV)\n#df = df.append(external_data_csv) # 40000 -> 100000\n# Append External 2\n#df = df.append(external_data2_csv) # 100000 -> 130000 # All Xeno-Canto Data Added.\n#df = df.append(external_data3_csv)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:47.74906Z","iopub.execute_input":"2021-05-31T18:05:47.749352Z","iopub.status.idle":"2021-05-31T18:05:47.789546Z","shell.execute_reply.started":"2021-05-31T18:05:47.749322Z","shell.execute_reply":"2021-05-31T18:05:47.78866Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FOlds","metadata":{}},{"cell_type":"code","source":"def stratified_KFold():\n    # Special Splitting Strategy that splits the files from the soundspace separately(So they also have 4/5 split)\n    # Performs a shuffled Stratified Split on the DataFrame.\n    splitter = StratifiedKFold(shuffle = True, random_state = 42) \n    FOLDS = []\n    SOUNDSCAPES_FOLDS = []\n    if not DataConfig.soundscapes_val:\n        for idx, (train, test) in enumerate(splitter.split(np.zeros(len(external_soundscapes_csv)), external_soundscapes_csv.site)):\n            train_fold= df.append(external_soundscapes_csv.iloc[train]).append(extra_soundscapes_csv)\n            test_fold = external_soundscapes_csv.iloc[test]\n            FOLDS += [(train_fold, test_fold)]\n            SOUNDSCAPES_FOLDS += [(external_soundscapes_csv.iloc[train].append(extra_soundscapes_csv), external_soundscapes_csv.iloc[test])]\n    else:\n        # No Folds, all monophone goes in train, all soundscapes in val\n        FOLDS = [(df.append(extra_soundscapes_csv), external_soundscapes_csv)]\n        SOUNDSCAPES_FOLDS += [(extra_soundscapes_csv, external_soundscapes_csv)]\n    return FOLDS, SOUNDSCAPES_FOLDS","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:47.791544Z","iopub.execute_input":"2021-05-31T18:05:47.791901Z","iopub.status.idle":"2021-05-31T18:05:47.79914Z","shell.execute_reply.started":"2021-05-31T18:05:47.791864Z","shell.execute_reply":"2021-05-31T18:05:47.797982Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS, SOUNDSCAPES_FOLDS = stratified_KFold()\n\n# Now Append the External soundscapes Data\n#if DataConfig.soundscapes_val:\ndf = df.append(external_soundscapes_csv).append(extra_soundscapes_csv)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:47.800619Z","iopub.execute_input":"2021-05-31T18:05:47.801231Z","iopub.status.idle":"2021-05-31T18:05:48.470422Z","shell.execute_reply.started":"2021-05-31T18:05:47.80119Z","shell.execute_reply":"2021-05-31T18:05:48.469509Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definition","metadata":{}},{"cell_type":"code","source":"def init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n\n\ndef interpolate(x: torch.Tensor, ratio: int):\n    \"\"\"Interpolate data in time domain. This is used to compensate the\n    resolution reduction in downsampling of a CNN.\n\n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, ratio to interpolate\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n    is the same as the value of the last frame.\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    pad = framewise_output[:, -1:, :].repeat(\n        1, frames_num - framewise_output.shape[1], 1)\n    \"\"\"tensor for padding\"\"\"\n\n    output = torch.cat((framewise_output, pad), dim=1)\n    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n\n    return output\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n\n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            if pool_size == (1, 1):\n                x = F.avg_pool2d(x, kernel_size = (1, 1))\n            else:\n                x = F.avg_pool2d(x, kernel_size=pool_size, padding = 1, stride = 2)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n\n        return x\n\n\nclass AttBlock(nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 activation=\"linear\",\n                 temperature=1.0):\n        super().__init__()\n\n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n\n        self.bn_att = nn.BatchNorm1d(out_features)\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:48.473698Z","iopub.execute_input":"2021-05-31T18:05:48.473976Z","iopub.status.idle":"2021-05-31T18:05:48.497742Z","shell.execute_reply.started":"2021-05-31T18:05:48.473951Z","shell.execute_reply":"2021-05-31T18:05:48.496943Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DFTBase(nn.Module):\n    def __init__(self):\n        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n        super(DFTBase, self).__init__()\n\n    def dft_matrix(self, n):\n        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n        omega = np.exp(-2 * np.pi * 1j / n)\n        W = np.power(omega, x * y)\n        return W\n\n    def idft_matrix(self, n):\n        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n        omega = np.exp(2 * np.pi * 1j / n)\n        W = np.power(omega, x * y)\n        return W\n    \n    \nclass STFT(DFTBase):\n    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n        of librosa.core.stft\n        \"\"\"\n        super(STFT, self).__init__()\n\n        assert pad_mode in ['constant', 'reflect']\n\n        self.n_fft = n_fft\n        self.center = center\n        self.pad_mode = pad_mode\n\n        # By default, use the entire frame\n        if win_length is None:\n            win_length = n_fft\n\n        # Set the default hop, if it's not already specified\n        if hop_length is None:\n            hop_length = int(win_length // 4)\n\n        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n\n        # Pad the window out to n_fft size\n        fft_window = librosa.util.pad_center(fft_window, n_fft)\n\n        # DFT & IDFT matrix\n        self.W = self.dft_matrix(n_fft)\n\n        out_channels = n_fft // 2 + 1\n\n        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n            groups=1, bias=False)\n\n        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n            groups=1, bias=False)\n\n        self.conv_real.weight.data = torch.Tensor(\n            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n        # (n_fft // 2 + 1, 1, n_fft)\n\n        self.conv_imag.weight.data = torch.Tensor(\n            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n        # (n_fft // 2 + 1, 1, n_fft)\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, data_length)\n        Returns:\n          real: (batch_size, n_fft // 2 + 1, time_steps)\n          imag: (batch_size, n_fft // 2 + 1, time_steps)\n        \"\"\"\n\n        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n\n        if self.center:\n            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n\n        real = self.conv_real(x)\n        imag = self.conv_imag(x)\n        # (batch_size, n_fft // 2 + 1, time_steps)\n\n        real = real[:, None, :, :].transpose(2, 3)\n        imag = imag[:, None, :, :].transpose(2, 3)\n        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n\n        return real, imag\n    \n    \nclass Spectrogram(nn.Module):\n    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n        window='hann', center=True, pad_mode='reflect', power=2.0, \n        freeze_parameters=True):\n        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n        Conv1d. The function has the same output of librosa.core.stft\n        \"\"\"\n        super(Spectrogram, self).__init__()\n\n        self.power = power\n\n        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n            win_length=win_length, window=window, center=center, \n            pad_mode=pad_mode, freeze_parameters=True)\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n        Returns:\n          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n        \"\"\"\n\n        (real, imag) = self.stft.forward(input)\n        # (batch_size, n_fft // 2 + 1, time_steps)\n\n        spectrogram = real ** 2 + imag ** 2\n\n        if self.power == 2.0:\n            pass\n        else:\n            spectrogram = spectrogram ** (power / 2.0)\n\n        return spectrogram\n\n    \nclass LogmelFilterBank(nn.Module):\n    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n        the pytorch implementation of as librosa.filters.mel \n        \"\"\"\n        super(LogmelFilterBank, self).__init__()\n\n        self.is_log = is_log\n        self.ref = ref\n        self.amin = amin\n        self.top_db = top_db\n\n        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n            fmin=fmin, fmax=fmax).T\n        # (n_fft // 2 + 1, mel_bins)\n\n        self.melW = nn.Parameter(torch.Tensor(self.melW))\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, channels, time_steps)\n        \n        Output: (batch_size, time_steps, mel_bins)\n        \"\"\"\n\n        # Mel spectrogram\n        mel_spectrogram = torch.matmul(input, self.melW)\n\n        # Logmel spectrogram\n        if self.is_log:\n            output = self.power_to_db(mel_spectrogram)\n        else:\n            output = mel_spectrogram\n\n        return output\n\n\n    def power_to_db(self, input):\n        \"\"\"Power to db, this function is the pytorch implementation of \n        librosa.core.power_to_lb\n        \"\"\"\n        ref_value = self.ref\n        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n\n        if self.top_db is not None:\n            if self.top_db < 0:\n                raise ParameterError('top_db must be non-negative')\n            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n\n        return log_spec","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:48.499126Z","iopub.execute_input":"2021-05-31T18:05:48.499762Z","iopub.status.idle":"2021-05-31T18:05:48.527442Z","shell.execute_reply.started":"2021-05-31T18:05:48.499723Z","shell.execute_reply":"2021-05-31T18:05:48.526551Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DropStripes(nn.Module):\n    def __init__(self, dim, drop_width, stripes_num):\n        \"\"\"Drop stripes. \n        Args:\n          dim: int, dimension along which to drop\n          drop_width: int, maximum width of stripes to drop\n          stripes_num: int, how many stripes to drop\n        \"\"\"\n        super(DropStripes, self).__init__()\n\n        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n\n        self.dim = dim\n        self.drop_width = drop_width\n        self.stripes_num = stripes_num\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n\n        assert input.ndimension() == 4\n\n        if self.training is False:\n            return input\n\n        else:\n            batch_size = input.shape[0]\n            total_width = input.shape[self.dim]\n\n            for n in range(batch_size):\n                self.transform_slice(input[n], total_width)\n\n            return input\n\n\n    def transform_slice(self, e, total_width):\n        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n\n        for _ in range(self.stripes_num):\n            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n\n            if self.dim == 2:\n                e[:, bgn : bgn + distance, :] = 0\n            elif self.dim == 3:\n                e[:, :, bgn : bgn + distance] = 0\n\n\nclass SpecAugmentation(nn.Module):\n    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n        freq_stripes_num):\n        \"\"\"Spec augmetation. \n        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n        Args:\n          time_drop_width: int\n          time_stripes_num: int\n          freq_drop_width: int\n          freq_stripes_num: int\n        \"\"\"\n\n        super(SpecAugmentation, self).__init__()\n\n        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n            stripes_num=time_stripes_num)\n\n        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n            stripes_num=freq_stripes_num)\n\n    def forward(self, input):\n        x = self.time_dropper(input)\n        x = self.freq_dropper(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:48.528756Z","iopub.execute_input":"2021-05-31T18:05:48.529146Z","iopub.status.idle":"2021-05-31T18:05:48.543092Z","shell.execute_reply.started":"2021-05-31T18:05:48.529109Z","shell.execute_reply":"2021-05-31T18:05:48.542323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PANNsCNN14Att(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        \n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n        mel_bins = 64\n        self.interpolate_ratio = 32  # Downsampled ratio\n\n         \n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n        self.interpolate_ratio = 32  # Downsampled ratio\n        sample_rate = 32000\n        window_size =  1024\n        hop_size = 320\n        mel_bins = 64\n        fmin = 50\n        fmax = 14000\n        classes_num = 264\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(\n            n_fft=window_size,\n            hop_length=hop_size,\n            win_length=window_size,\n            window=window,\n            center=center,\n            pad_mode=pad_mode,\n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(\n            sr=sample_rate,\n            n_fft=window_size,\n            n_mels=mel_bins,\n            fmin=fmin,\n            fmax=fmax,\n            ref=ref,\n            amin=amin,\n            top_db=top_db,\n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(\n            time_drop_width=64,\n            time_stripes_num=2,\n            freq_drop_width=8,\n            freq_stripes_num=2)\n        \n        # Spectrogram extractor\n        self.bn0 = nn.BatchNorm2d(mel_bins)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n        classes_num = 264\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        \n    def cnn_feature_extractor(self, x):\n        x = self.conv_block1(x, pool_size=(3, 3), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(3, 3), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(3, 3), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(3, 3), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(3, 3), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        return x\n    \n   \n\n    def forward(self, x, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n        # Output shape (batch size, channels, time, frequency)\n        x = x.transpose(2, 3) # (B, C, T, F)\n        x = self.cnn_feature_extractor(x)\n        \n        return x.transpose(2, 3)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:48.544412Z","iopub.execute_input":"2021-05-31T18:05:48.544869Z","iopub.status.idle":"2021-05-31T18:05:48.563823Z","shell.execute_reply.started":"2021-05-31T18:05:48.544833Z","shell.execute_reply":"2021-05-31T18:05:48.563063Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tmp_model(pl.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.load_state_dict(torch.load(ResNestPath, map_location = DEVICE))\ndef load_prev_model(name):\n    if \"resnest\" in name:\n        model = resnest50_fast_1s1x64d(pretrained = False)\n    elif 'CNN14' in name:\n        model = PANNsCNN14Att()\n    elif 'densenet' in name:\n        model = torchvision.models.densenet121(pretrained = True)\n    else:\n        model = torchvision.models.resnext50_32x4d(pretrained=False)\n    #nb_ft = model.fc.in_features\n    #del model.fc\n    #num_cls = 264#397 if 'resnest' in name else 264\n    #model.fc = nn.Linear(nb_ft, num_cls)\n    #if 'resnest' in name:\n    #    model = tmp_model(model).model\n    #else:\n    if 'CNN14' in name:\n        model.load_state_dict(torch.load(CNN14Path, map_location = DEVICE)['model_state_dict'])\n        model.conv_block1 = ConvBlock(in_channels = 3, out_channels = 64)\n    return model\nclass FeatureExtractor(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model_name = ModelConfig.model_name\n        self.model = load_prev_model(self.model_name)\n        \n    def forward(self, x):\n        if 'densenet' in self.model_name:\n            return self.model.features(x)\n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:48.565002Z","iopub.execute_input":"2021-05-31T18:05:48.565421Z","iopub.status.idle":"2021-05-31T18:05:48.576966Z","shell.execute_reply.started":"2021-05-31T18:05:48.565383Z","shell.execute_reply":"2021-05-31T18:05:48.576202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Blocks, Added on Top of CNN Features","metadata":{}},{"cell_type":"code","source":"# Not Real Transformer, just 2 stacked MAH blocks(Similar to SED Attention, but for classification instead of segmentation of audio.)\nclass MultiHeadAttention(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_heads = ModelConfig.num_heads\n        self.in_dim = ModelConfig.transformer_dim\n        \n        self.drop_prob = ModelConfig.dropout\n        \n        self.MultiHeadAttention = nn.MultiheadAttention(self.in_dim, self.num_heads)\n        self.LayerNorm = nn.LayerNorm((self.in_dim))\n        self.dropout = nn.Dropout(self.drop_prob)\n    def forward(self, x):\n        return self.dropout(self.LayerNorm(self.MultiHeadAttention(key = x, value = x, query = x)[0]))\n        \nclass MAHHead(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.in_features = ModelConfig.feature_extractor_dim\n        self.inner_features = ModelConfig.transformer_dim\n        self.num_classes = ModelConfig.num_classes\n        self.ConvHead = ConvBlock(self.in_features, self.inner_features, (4, 1), 0, 1, 1, 1)\n        self.MAH = nn.Sequential(*[\n            MultiHeadAttention() for i in range(ModelConfig.num_layers)\n        ])\n        self.ConvStem = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1, 1)\n        self.Linear = nn.Linear(self.in_features, self.num_classes)\n    def forward(self, x):\n        # X: tensor(B, 2048, 4, 9)\n        x = torch.squeeze(self.ConvHead(x)).transpose(1, 2) # (B, 9, 768)\n        x = x.transpose(0, 1)\n        x = self.MAH(x).transpose(0, 1).transpose(1, 2).unsqueeze(2) # (B, 768, 1, 9)\n        x = self.ConvStem(x) # (B, 2048, 1, 9) \n        x = torch.sum(x, dim = (2, 3)) # (B, 2048)\n        return self.Linear(x)\n        \nclass AttentionHead(pl.LightningModule):\n    # Heng's 2D attention Head, not sure if it will work or not.\n    def __init__(self):\n        super().__init__()\n        self.in_features = ModelConfig.feature_extractor_dim\n        self.num_classes = ModelConfig.num_classes\n        self.attention = nn.Sequential(\n            nn.Conv2d(self.in_features, self.in_features * 2, kernel_size=3, padding=1, stride=1, bias=False),\n            nn.BatchNorm2d(self.in_features * 2),\n            nn.Tanh(),\n        ) #use 8x1 to make location-aware convolution\n        self.attention1 = nn.Conv2d(self.in_features // 2, 1, kernel_size=1)\n        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.drop_prob = ModelConfig.dropout\n        self.Dropout = nn.Dropout(self.drop_prob)\n        self.Linear =  nn.Linear(self.in_features, self.num_classes)\n\n    def forward(self, x):\n        B, C, Freq, L = x.shape\n        a = self.attention(x).mean(dim = 2) #x is 4x9 feature map\n      \n        a = a.reshape(-1, self.in_features // 2, Freq, L)\n        a = self.attention1(a)\n        a = F.softmax(a.reshape(-1, Freq * L),-1).reshape(-1, 1, Freq, L)\n\n        x = (a * x + x) # (B, 2048)\n        x = torch.squeeze(self.global_avg_pool(x))\n        x = self.Dropout(x)\n        return self.Linear(x)\nclass BaseLineHead(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.in_dim = ModelConfig.feature_extractor_dim\n        self.num_classes = ModelConfig.num_classes\n        self.drop_prob = ModelConfig.dropout\n        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(self.in_dim, self.num_classes)\n        \n    def forward(self, x):\n        # Tensor: (B, 768, 4, 9)\n        x = x.sum(dim = (2, 3)) # (B, 768)\n        x = self.Dropout(x)\n        linear = self.Linear(x) # (B, self.num_classes)\n        return linear","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:13:57.903447Z","iopub.execute_input":"2021-05-31T18:13:57.903801Z","iopub.status.idle":"2021-05-31T18:13:57.925257Z","shell.execute_reply.started":"2021-05-31T18:13:57.903768Z","shell.execute_reply":"2021-05-31T18:13:57.924378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FullModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.model_head = ModelConfig.head\n        if self.model_head == 'mah':\n            self.head = MAHHead()\n        elif self.model_head == 'baseline':\n            self.head = BaseLineHead()\n        else:\n            self.head = AttentionHead()\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        head = self.head(features)\n        return head\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:14:01.443627Z","iopub.execute_input":"2021-05-31T18:14:01.443945Z","iopub.status.idle":"2021-05-31T18:14:01.450073Z","shell.execute_reply.started":"2021-05-31T18:14:01.443913Z","shell.execute_reply":"2021-05-31T18:14:01.449139Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelConfig:\n    num_classes = NUM_CLASSES\n    bam_dilate = 3\n    reduce = 4\n    expand = 1.5\n    \n    \n    feature_extractor_dim = 1024\n    transformer_dim = 768\n    num_heads = 12\n    \n    model_name = 'densenet'\n    dropout = 0.5\n    num_layers = 2\n    head = 'Conv2DAtt'\n    num_channels = 3 # 1 for just Melspecs \n    \n    use_mixup = False # Whether or not to use mixup augmentation.\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:05:48.612747Z","iopub.execute_input":"2021-05-31T18:05:48.613211Z","iopub.status.idle":"2021-05-31T18:05:48.623217Z","shell.execute_reply.started":"2021-05-31T18:05:48.613178Z","shell.execute_reply":"2021-05-31T18:05:48.622476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PreCache The Dataset","metadata":{}},{"cell_type":"code","source":"def load_data(df):\n    def load_row(row):\n        # impath = TRAIN_IMAGES_ROOT/f\"{row.primary_label}/{row.filename}.npy\"\n        image = np.load(str(row.impath))\n        if len(image.shape) > 2:\n            image = image[:MAX_READ_SAMPLES]\n        else:\n            image = np.expand_dims(image, axis = 0)\n        return row.filename, image\n    pool = joblib.Parallel(4)\n    mapper = joblib.delayed(load_row)\n    tasks = [mapper(row) for row in df.itertuples(False)]\n    res = pool(tqdm(tasks))\n    res = dict(res)\n    return res","metadata":{"id":"7HYQwAyBCWs8","execution":{"iopub.status.busy":"2021-05-31T18:05:48.624472Z","iopub.execute_input":"2021-05-31T18:05:48.624918Z","iopub.status.idle":"2021-05-31T18:05:48.632994Z","shell.execute_reply.started":"2021-05-31T18:05:48.624885Z","shell.execute_reply":"2021-05-31T18:05:48.63216Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We cache the train set to reduce training time\n#audio_image_store = None\nif audio_image_store is None:\n    audio_image_store = load_data(df)","metadata":{"id":"Vw19bB7mCWs9","outputId":"09a5e374-7e5c-4c92-91e4-b60313cbb1a9","execution":{"iopub.status.busy":"2021-05-31T18:05:48.634983Z","iopub.execute_input":"2021-05-31T18:05:48.635418Z","iopub.status.idle":"2021-05-31T18:08:44.363875Z","shell.execute_reply.started":"2021-05-31T18:05:48.635381Z","shell.execute_reply":"2021-05-31T18:08:44.361664Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Definition","metadata":{}},{"cell_type":"code","source":"def freq_mask(spec, F=30, num_masks=1, replace_with_zero=False):\n    # Mel Spec Augments\n    cloned = spec.clone()\n    num_mel_channels = cloned.shape[0]\n    \n    for i in range(0, num_masks):        \n        f = random.randrange(0, F)\n        f_zero = random.randrange(0, num_mel_channels - f)\n\n        # avoids randrange error if values are equal and range is empty\n        if (f_zero == f_zero + f): return cloned\n\n        mask_end = random.randrange(f_zero, f_zero + f) \n        if (replace_with_zero): cloned[f_zero:mask_end] = 0\n        else: cloned[f_zero:mask_end] = cloned.mean()\n    \n    return cloned\n\n#Export\ndef time_mask(spec, T=40, num_masks=1, replace_with_zero=False):\n    cloned = spec.clone()\n    len_spectro = cloned.shape[1]\n    \n    for i in range(0, num_masks):\n        t = random.randrange(0, T)\n        t_zero = random.randrange(0, len_spectro - t)\n\n        # avoids randrange error if values are equal and range is empty\n        if (t_zero == t_zero + t): return cloned\n\n        mask_end = random.randrange(t_zero, t_zero + t)\n        if (replace_with_zero): cloned[:,t_zero:mask_end] = 0\n        else: cloned[:,t_zero:mask_end] = cloned.mean()\n    return cloned\ndef pad_tensor(y):\n    # Pads a Tensor to shape (128, 281) \n    shape = (128, 281)\n    new_tensor = torch.zeros(shape)\n    new_tensor[:, :y.shape[1]] = y\n    return new_tensor\ndef lower_gain(y):\n    # Lowers the Gain of the image\n    lower_bound = 0.5\n    upper_bound = 1\n    gain = random.uniform(lower_bound, upper_bound)\n    return y ** gain \ndef mixup_with_val(y, idx):\n    # Mixes up the melspec with the melspec from train_soundscapes(In order to produce \"artificial\" noise that exists in the test set.\n    # This is only ever used at train, so idx is guaranteed to be 0\n    soundscapes_df = SOUNDSCAPES_FOLDS[idx][0]\n    soundscapes_df = soundscapes_df[soundscapes_df.primary_label == 'nocall']\n    idx = random.randint(0, len(soundscapes_df) - 1)\n    row = torch.tensor(np.load(soundscapes_df.iloc[idx].impath))\n    background_mix = pad_tensor(row)\n    \n    mix_min = 0.4\n    mix_max = 1 \n    mix_quan = random.uniform(mix_min, mix_max)\n    return y + mix_quan * background_mix\n    \n    \ndef cutmix_with_val(y, idx):\n    soundscapes_df = SOUNDSCAPES_FOLDS[idx][0]\n    soundscapes_df = soundscapes_df[soundscapes_df.primary_label == 'nocall']\n    idx = random.randint(0, len(soundscapes_df) - 1) \n    row = torch.tensor(np.load(soundscapes_df.iloc[idx].impath))\n    background_mix = pad_tensor(row)\n    _, L = background_mix.shape\n    start_idx = random.randint(0, L - 1)\n    end_idx = random.randint(start_idx + 1, L)\n    initial = y[:, :start_idx]\n    end = y[:, end_idx:]\n    middle= background_mix[:, start_idx:end_idx] / 255.0\n    y = torch.cat([initial, middle, end], dim = -1)\n    return y\n#def cutout(y):\n#    # Cutouts a Box in the Melspec, alternative to specAugment\n    \n    \ndef augment(y, idx):\n    # Augmentation on MelSpecs\n    \n    if random.random() < 0.25:\n        y = time_mask(y, num_masks = 2)\n    if random.random() < 0.25:\n        y = freq_mask(y, num_masks = 2)\n    #if random.random() < 0.5:\n    #    y = mixup_with_val(y, idx)\n    if random.random() < 0.5:\n        y = lower_gain(y)\n    return y # Augmentations aren't working right now.","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:44.365506Z","iopub.execute_input":"2021-05-31T18:08:44.365853Z","iopub.status.idle":"2021-05-31T18:08:44.3855Z","shell.execute_reply.started":"2021-05-31T18:08:44.365812Z","shell.execute_reply":"2021-05-31T18:08:44.383709Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BirdClefDataset(Dataset):\n\n    def __init__(self, audio_image_store, idx, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n        \n        self.idx = idx\n        self.number = 0 if is_train else 1\n        self.audio_image_store = audio_image_store\n        self.meta = FOLDS[idx][self.number].copy().reset_index(drop=True)\n        self.sr = sr\n        self.is_train = is_train\n        self.num_classes = num_classes\n        self.duration = duration\n        \n        self.audio_length = self.duration*self.sr\n        \n        self.mean_train = np.array([0.485, 0.456, 0.406])\n        self.mean_train =torch.tensor(np.expand_dims(np.expand_dims(self.mean_train, axis = -1), axis = -1))\n        self.std_train = np.array([0.229, 0.224, 0.225])\n        self.std_train = torch.tensor(np.expand_dims(np.expand_dims(self.std_train, axis = -1), axis = -1))\n        self.stats = (self.mean_train, self.std_train) # ImageNet Stats \n        self.shape = (128, 281)\n    \n    \n   \n\n    def __len__(self):\n        return len(self.meta)\n    \n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        #secondary_labels = row.secondary_tensors\n        image = self.audio_image_store[row.filename]\n        image = torch.tensor(image[0]).to(torch.float32) / 255.0 # Sample From the Front.\n\n        padded = torch.zeros(self.shape)\n        padded[:, :image.shape[1]] = image\n        image = padded\n        if self.is_train:\n            image = augment(image, self.idx)\n        # One image is Regular power to db, another is cleaned powered power to db, one is pcen\n        if ModelConfig.num_channels == 3:\n            pcen = lb.pcen(image.numpy())\n            mel = image\n            power = image ** 1.5\n            image = torch.stack([torch.tensor(pcen), torch.tensor(mel), torch.tensor(power)], dim = 0)\n        else:\n            image = torch.stack([image, image, image])\n       \n        \n        t = np.zeros(self.num_classes, dtype=np.float32) \n        label_id = row.label_id\n        try:\n            label_id = label_id.item()\n        except:\n            pass\n        \n        if type(label_id) == type(1.0) or type(label_id) == type(1):\n            t[row.label_id] = 1\n        else:\n            \n            # Soundscape, so multiple things\n            labels = row.label_id.split()\n            for label in labels:\n                t[int(label)] = 1\n                \n        \n        return torch.tensor(image).to(torch.float32), torch.tensor(t).to(torch.float32)","metadata":{"id":"OWSkCXyhCWs-","execution":{"iopub.status.busy":"2021-05-31T18:08:44.387049Z","iopub.execute_input":"2021-05-31T18:08:44.387409Z","iopub.status.idle":"2021-05-31T18:08:44.403927Z","shell.execute_reply.started":"2021-05-31T18:08:44.387373Z","shell.execute_reply":"2021-05-31T18:08:44.403225Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = BirdClefDataset(audio_image_store, 0, is_train = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:44.408119Z","iopub.execute_input":"2021-05-31T18:08:44.4085Z","iopub.status.idle":"2021-05-31T18:08:44.452338Z","shell.execute_reply.started":"2021-05-31T18:08:44.408452Z","shell.execute_reply":"2021-05-31T18:08:44.45151Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor images, labels in dataset:\n    lbd.specshow(images[1].numpy())\n    plt.show()\n    count += 1\n    if count == 32:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:44.453894Z","iopub.execute_input":"2021-05-31T18:08:44.454384Z","iopub.status.idle":"2021-05-31T18:08:47.466066Z","shell.execute_reply.started":"2021-05-31T18:08:44.454346Z","shell.execute_reply":"2021-05-31T18:08:47.465241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_smooth(primary): \n    primary = primary.clone() \n    ones = primary == 1 \n    not_ones = primary == 0 \n    primary[ones] = 0.995 \n    primary[not_ones] = 0.0025 \n    return primary","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:47.469786Z","iopub.execute_input":"2021-05-31T18:08:47.471833Z","iopub.status.idle":"2021-05-31T18:08:47.478151Z","shell.execute_reply.started":"2021-05-31T18:08:47.471793Z","shell.execute_reply":"2021-05-31T18:08:47.477376Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mixup and cutmix.","metadata":{}},{"cell_type":"code","source":"def mixup(x, y):\n    # Performs Mixup on Melspecs\n    # x: Tensor(B, 128, 281)\n    # Mixup sampled from beta distribution.\n    beta = 0.4\n    gamma = random.beta(beta, beta)\n    gamma = max(1-gamma, gamma)\n    shuffle = torch.randperm(x.shape[0]).to(x.device)\n    x = gamma*x + (1-gamma)*x[shuffle]\n    y = gamma*y + (1-gamma)*y[shuffle]\n    # for hard mixup, anything that isn't 0 is set to 1.\n    return x, y # efficient mixup impletation(not completely random, but it should be fine.)\n    \ndef cutmix(batch, y):\n    # Performs Time wise Cutmix on batch of melspecs\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:47.482686Z","iopub.execute_input":"2021-05-31T18:08:47.485437Z","iopub.status.idle":"2021-05-31T18:08:47.494657Z","shell.execute_reply.started":"2021-05-31T18:08:47.485393Z","shell.execute_reply":"2021-05-31T18:08:47.493867Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# collate functions","metadata":{}},{"cell_type":"code","source":"def train_collate(values):\n    images = torch.stack([value[0] for value in values])\n    labels = torch.stack([value[1] for value in values]) # (B, 128, 281)\n    # ----------MixUp--------(or Cutmix?)\n    if ModelConfig.use_mixup:\n        images, labels = mixup(images, labels)\n    return images, labels\ndef val_collate(values):\n    images = torch.stack([value[0] for value in values])\n    labels = torch.stack([value[1] for value in values])\n    return images, labels \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:47.499245Z","iopub.execute_input":"2021-05-31T18:08:47.501535Z","iopub.status.idle":"2021-05-31T18:08:47.510526Z","shell.execute_reply.started":"2021-05-31T18:08:47.501498Z","shell.execute_reply":"2021-05-31T18:08:47.509602Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{"id":"F56zXq8CVAqn"}},{"cell_type":"code","source":"CRITERION = nn.BCEWithLogitsLoss(reduction = 'mean') # TODO: Mask Secondary Outputs Loss.\ndef loss_fn(y_pred, primary):\n    # Primary: Tensor(B)\n    # Secondary: Tensor(B, C)\n    # Y_Pred: Tensor(B, C)\n    # TODO Split the Losses.\n    B, C = primary.shape\n    smooth = label_smooth(primary)\n    y_pred = y_pred.float()\n    loss = CRITERION(y_pred, smooth)\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:47.514724Z","iopub.execute_input":"2021-05-31T18:08:47.517078Z","iopub.status.idle":"2021-05-31T18:08:47.524371Z","shell.execute_reply.started":"2021-05-31T18:08:47.517014Z","shell.execute_reply":"2021-05-31T18:08:47.523591Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"class F1_score(Metric):\n    def __init__(self):\n        self.thresholds = np.arange(0.0, 1.0, 0.01)\n        self.f1_scores = [0.0 for i in range(len(self.thresholds))]\n        self.count = 0\n    def round_pred(self, y_pred, thresh):\n        ones = y_pred >= thresh\n        logits = torch.zeros_like(y_pred, device = y_pred.device)\n        logits = logits + ones.int()\n        return logits\n    def round_true(self, y_true):\n        ones = y_true >= 0.5\n        logits = torch.zeros_like(y_true, device = y_true.device)\n        logits = logits + ones.int()\n        return logits\n    def metric(self, y_pred, target, thresh_idx):\n        # Given y_pred = Tensor(B, C) and primary = Tensor(B, C), Computes the Row-wise F1 Score\n        # Threshold Predictions\n        predictions = self.round_pred(y_pred, self.thresholds[thresh_idx])\n        # PRedictions: (B, C)\n        zeros = torch.sum(predictions, axis = -1) == 0\n        predictions[zeros, NUM_CLASSES - 1] = 1 # No Call\n        tp = (predictions * target).sum(1)\n        fp = (predictions * (1 - target)).sum(1)\n        fn = ((1 - predictions) * target).sum(1)\n        \n        eps = 1e-9\n        f1 = (tp + eps) / (tp + (fp + fn) / 2 + eps)\n        # Accumulate the f1 score\n        self.f1_scores[thresh_idx] += f1.mean().item()\n    def compute_f1_score(self, y_pred, primary):\n        primary = self.round_true(primary)\n        for th_idx in range(len(self.thresholds)):\n            y_p = self.round_pred(y_pred, self.thresholds[th_idx])\n            self.metric(y_p, primary, th_idx)\n        self.count += 1\n    def accumulate(self, learn):\n        # y_pred: Tensor(B, C)\n        y_pred = torch.sigmoid(learn.pred).cpu()\n        primary = learn.y.cpu()\n        self.compute_f1_score(y_pred, primary)\n        \n    def reset(self):\n        self.f1_scores = [0.0 for i in range(len(self.f1_scores))]\n        self.count = 0\n        \n    @property\n    def value(self):\n        eps = 1e-8\n        # Computes the best threshold and result\n        best = 0\n        best_th = 0\n        for th_idx in range(len(self.f1_scores)):\n            if self.f1_scores[th_idx] > best:\n                best = self.f1_scores[th_idx]\n                best_th = self.thresholds[th_idx]\n        best = (best + eps) / (self.count + eps)\n        print(f\"F1Score: {best}\")\n        return best\nclass F1Score_th(Metric):\n    def __init__(self):\n        self.thresholds = np.arange(0.0, 1.0, 0.01)\n        self.f1_scores = [0.0 for i in range(len(self.thresholds))]\n        self.count = 0\n    def round_pred(self, y_pred, thresh):\n        ones = y_pred >= thresh\n        logits = torch.zeros_like(y_pred, device = y_pred.device)\n        logits = logits + ones.int()\n        return logits\n    def round_true(self, y_true):\n        ones = y_true >= 0.5\n        logits = torch.zeros_like(y_true, device = y_true.device)\n        logits = logits + ones.int()\n        return logits\n    def metric(self, y_pred, target, thresh_idx):\n        # Given y_pred = Tensor(B, C) and primary = Tensor(B, C), Computes the Row-wise F1 Score\n        # Threshold Predictions\n        predictions = self.round_pred(y_pred, self.thresholds[thresh_idx])\n    \n        tp = (predictions * target).sum(1)\n        fp = (predictions * (1 - target)).sum(1)\n        fn = ((1 - predictions) * target).sum(1)\n        \n        eps = 1e-9\n        f1 = (tp + eps) / (tp + (fp + fn) / 2 + eps)\n        # Accumulate the f1 score\n        self.f1_scores[thresh_idx] += f1.mean().item()\n    def compute_f1_score(self, y_pred, primary):\n        primary = self.round_true(primary)\n        for th_idx in range(len(self.thresholds)):\n            y_p = self.round_pred(y_pred, self.thresholds[th_idx])\n            self.metric(y_p, primary, th_idx)\n        self.count += 1\n    def accumulate(self, learn):\n        # y_pred: Tensor(B, C)\n        y_pred = torch.sigmoid(learn.pred).cpu()\n        primary = learn.y.cpu()\n        self.compute_f1_score(y_pred, primary)\n        \n    def reset(self):\n        self.f1_scores = [0.0 for i in range(len(self.f1_scores))]\n        self.count = 0\n        \n    @property\n    def value(self):\n        eps = 1e-8\n        # Computes the best threshold and result\n        best = 0\n        best_th = 0\n        for th_idx in range(len(self.f1_scores)):\n            if self.f1_scores[th_idx] > best:\n                best = self.f1_scores[th_idx]\n                best_th = self.thresholds[th_idx]\n        best = (best + eps) / (self.count + eps)\n        return best_th\n        \n    \nclass Accuracy(Metric):\n    def __init__(self):\n        self.accuracy = 0\n        self.count = 0\n    def round_pred(self, y_pred):\n        y_pred = torch.sigmoid(y_pred)\n        ones = y_pred >= 0.5\n        scores = torch.zeros_like(y_pred, device = y_pred.device)\n        scores[ones] = 1\n        return scores\n    def round_true(self, y_true):\n        ones = y_true >= 0.5\n        logits = torch.zeros_like(ones, device = ones.device)\n        logits = logits + ones.int()\n        return logits\n    def accumulate(self,learn):\n        y_pred = self.round_pred(learn.pred)\n        primary = self.round_true(learn.y)\n        # y_pred: Tensor(B, C)\n        # primary: Tensor(B, C)\n        B, C = y_pred.shape\n        tp = torch.sum(y_pred == primary)\n        acc = tp / (B * C)\n        \n        self.accuracy += acc.item()\n        self.count += 1\n    def reset(self):\n        self.accuracy = 0 \n        self.count = 0\n    @property\n    def value(self):\n        eps = 1e-8\n        return round((self.accuracy + eps) / (self.count + eps), 3)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:08:47.528648Z","iopub.execute_input":"2021-05-31T18:08:47.531083Z","iopub.status.idle":"2021-05-31T18:08:47.577116Z","shell.execute_reply.started":"2021-05-31T18:08:47.531047Z","shell.execute_reply":"2021-05-31T18:08:47.576275Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRaining Config","metadata":{}},{"cell_type":"code","source":"class TrainingConfig:\n    start_lr = 1e-3\n    max_lr = 4e-3\n    min_lr = 1e-5\n    warm_steps = 0.1\n    peak_steps = 0.2 # 0.3 ramp up, 0.7 rabsamp down = super-convergence\n    \n    num_epochs = 16\n    train_steps = len(FOLDS[0][0])\n    \n    total_steps = num_epochs * train_steps\n    \n    weight_decay = 0\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:14:04.179977Z","iopub.execute_input":"2021-05-31T18:14:04.180356Z","iopub.status.idle":"2021-05-31T18:14:04.184782Z","shell.execute_reply.started":"2021-05-31T18:14:04.180327Z","shell.execute_reply":"2021-05-31T18:14:04.183994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Fn","metadata":{}},{"cell_type":"markdown","source":"# DataLoader Config","metadata":{}},{"cell_type":"code","source":"class DataLoaderConfig:\n    def __init__(self, is_train):\n        num_workers = 4\n        pin_memory = True\n        bs = 64\n        create_batch = train_collate if is_train else val_collate\n        shuffle = is_train\n        self.config = {'num_workers': num_workers, 'pin_memory': pin_memory, 'bs': bs, 'shuffle': shuffle, 'create_batch': create_batch}\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:14:04.602833Z","iopub.execute_input":"2021-05-31T18:14:04.60315Z","iopub.status.idle":"2021-05-31T18:14:04.607934Z","shell.execute_reply.started":"2021-05-31T18:14:04.603122Z","shell.execute_reply":"2021-05-31T18:14:04.60717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_folds_fast_ai(folds):\n    # Fast Ai, with their variety of tricks, trains faster and better\n    for fold_idx in folds:\n        # Overfit Testing\n        train = BirdClefDataset(audio_image_store, fold_idx, is_train = True)\n        val = BirdClefDataset(audio_image_store,fold_idx, is_train = False)#BirdClefDataset(audio_image_store, val, is_train = False)\n        \n        train_config = DataLoaderConfig(is_train = True)\n        val_config = DataLoaderConfig(is_train = False)\n        \n        train = DataLoader(train, **train_config.config)\n        val = DataLoader(val, **val_config.config)\n        dls = DataLoaders(train, val)\n        model = FullModel()\n        if torch.cuda.is_available(): dls.cuda(),model.cuda()\n        learn = Learner(dls, model, loss_func=loss_fn,\n                metrics=[F1Score_th(), F1_score], opt_func = Adam, lr = TrainingConfig.start_lr, wd = TrainingConfig.weight_decay).to_fp16()\n        cbs = [\n            SaveModelCallback(monitor = 'f1_score',comp = np.greater, fname = f'model_{fold_idx}'),\n            EarlyStoppingCallback(monitor = 'f1_score', comp = np.greater, patience = 5),\n            ReduceLROnPlateau(monitor = 'f1_score', comp = np.greater, patience = 3, min_lr = 1e-7)\n        ]\n        learn.fit_one_cycle(TrainingConfig.num_epochs, lr_max = TrainingConfig.max_lr, cbs = cbs, wd = TrainingConfig.weight_decay)\n        learn.save(f\"final_{fold_idx}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:14:06.059952Z","iopub.execute_input":"2021-05-31T18:14:06.060292Z","iopub.status.idle":"2021-05-31T18:14:06.070812Z","shell.execute_reply.started":"2021-05-31T18:14:06.060262Z","shell.execute_reply":"2021-05-31T18:14:06.069472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"FOLDS_IDX = [1]\nmodel = train_folds_fast_ai(FOLDS_IDX)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:14:07.088176Z","iopub.execute_input":"2021-05-31T18:14:07.088501Z","iopub.status.idle":"2021-05-31T18:14:49.066237Z","shell.execute_reply.started":"2021-05-31T18:14:07.088474Z","shell.execute_reply":"2021-05-31T18:14:49.061843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]}]}