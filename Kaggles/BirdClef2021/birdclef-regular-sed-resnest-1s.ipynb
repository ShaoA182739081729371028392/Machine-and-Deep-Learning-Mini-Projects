{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pysndfx SoundFile audiomentations pretrainedmodels efficientnet_pytorch resnest","metadata":{"id":"Yn1Ybf15VAqW","execution":{"iopub.status.busy":"2021-05-31T11:58:29.399606Z","iopub.execute_input":"2021-05-31T11:58:29.400026Z","iopub.status.idle":"2021-05-31T11:58:41.73903Z","shell.execute_reply.started":"2021-05-31T11:58:29.39994Z","shell.execute_reply":"2021-05-31T11:58:41.738006Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# No Random Sampling, only first 5 seconds.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install colorednoise\nimport colorednoise as cn\nimport numpy as np\nimport librosa as lb\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport librosa.display as lbd\nimport soundfile as sf\nfrom  soundfile import SoundFile\nimport pandas as pd\nfrom  IPython.display import Audio\nfrom pathlib import Path\n\nimport glob\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom  torch.utils.data import Dataset, DataLoader\nimport torchvision\n\nfrom resnest.torch import resnest50\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json\nfrom  ast import literal_eval\n\n\nfrom IPython.display import Audio\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.notebook import tqdm\nimport joblib\nimport pytorch_lightning as pl\n\nfrom efficientnet_pytorch import EfficientNet\nimport pretrainedmodels\nfrom resnest.torch import resnest50_fast_1s1x64d\nfrom resnest.torch import resnest50\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\nfrom fastai.vision.all import *","metadata":{"id":"2dt7oG43VAqc","execution":{"iopub.status.busy":"2021-05-31T11:58:41.740851Z","iopub.execute_input":"2021-05-31T11:58:41.741171Z","iopub.status.idle":"2021-05-31T11:58:55.342536Z","shell.execute_reply.started":"2021-05-31T11:58:41.741136Z","shell.execute_reply":"2021-05-31T11:58:55.341622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config Vars","metadata":{}},{"cell_type":"code","source":"NUM_CLASSES = 397\nSR = 32_000\nDURATION = 20\nMAX_READ_SAMPLES = 1 \naudio_image_store = None\nDATA_ROOT = Path(\"../input/birdclef-2021\")\nimport glob\n\nMEL_PATHS = sorted([Path(x) for x in glob.glob('../input/*birdclef-mels-computer-public*/rich_train_metadata.csv')] + [Path(x) for x in glob.glob(\"../input/k/andrewshao2005/*birdclef-*/rich_train_metadata.csv\")])\nTRAIN_LABEL_PATHS = sorted([Path(x) for x in glob.glob(\"../input/*birdclef-mels-computer-public*/LABEL_IDS.json\")] +  [Path(x) for x in glob.glob(\"../input/k/andrewshao2005/*birdclef-*/LABEL_IDS.json\")])\nSOUNDSCAPES_PATH = '../input/birdclef-2021/train_soundscapes/'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nResNestPath = '../input/birds-cp-1/resnest50_fast_1s1x64d_conf_0.pt'\nResNextPath = '../input/birds-cp-2/resnext50_32x4d__0.pt'\n\nexternal_soundscapes = '../input/external-xenocanto-data-soundscapes/audio_images/'\nexternal_soundscapes_csv = pd.read_csv('../input/external-xenocanto-data-soundscapes/rich_train_metadata.csv')\nexternal_soundscapes_csv['secondary_labels'] = external_soundscapes_csv['second_labels']\ndel external_soundscapes_csv['second_labels']\nexternal_soundscapes_csv = external_soundscapes_csv.set_index(\"Unnamed: 0\")\n\nclass DataConfig:\n    # Stores Config vars pertaining to data\n    soundscapes_val = False # Means to put all of the soundscapes(training) as your vlaidation set\n    # IF False, validation becomes split between soundscapes(80/20 split) and monophone is added too.\n\nprint(\"Device:\", DEVICE)","metadata":{"id":"Iu56f-7VVAqf","outputId":"0f3fa344-0ed4-47d8-f3c0-218cf3bf5a78","execution":{"iopub.status.busy":"2021-05-31T11:58:55.34449Z","iopub.execute_input":"2021-05-31T11:58:55.344798Z","iopub.status.idle":"2021-05-31T11:58:55.416615Z","shell.execute_reply.started":"2021-05-31T11:58:55.344757Z","shell.execute_reply":"2021-05-31T11:58:55.415408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Up DF Functions","metadata":{}},{"cell_type":"code","source":"def clean_df(df):\n    # Cleans DataFrames from any erroneous entroes.\n    to_drop = []\n    for idx, row in tqdm(enumerate(df.iterrows())):\n        row = row[1]\n        if len(glob.glob(str(row.impath))) == 1:\n            continue\n        to_drop += [idx]\n    df = df.reset_index()\n    df = df.drop(to_drop)\n    return df\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:58:55.418157Z","iopub.execute_input":"2021-05-31T11:58:55.418497Z","iopub.status.idle":"2021-05-31T11:58:55.424927Z","shell.execute_reply.started":"2021-05-31T11:58:55.418456Z","shell.execute_reply":"2021-05-31T11:58:55.423719Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df(mel_paths=MEL_PATHS, train_label_paths=TRAIN_LABEL_PATHS):\n  df = None\n  LABEL_IDS = {}\n    \n  for file_path in mel_paths:\n    temp = pd.read_csv(str(file_path), index_col=0)\n    temp[\"impath\"] = temp.apply(lambda row: file_path.parent/\"audio_images/{}/{}.npy\".format(row.primary_label, row.filename), axis=1) \n    df = temp if df is None else df.append(temp)\n    \n    \n  #df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n\n  for file_path in train_label_paths:\n    with open(str(file_path)) as f:\n      LABEL_IDS.update(json.load(f))\n  LABEL_IDS['nocall'] = NUM_CLASSES - 1\n  return LABEL_IDS, df\nLABEL_IDS, df = get_df()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:58:55.426499Z","iopub.execute_input":"2021-05-31T11:58:55.426975Z","iopub.status.idle":"2021-05-31T11:58:58.350177Z","shell.execute_reply.started":"2021-05-31T11:58:55.426936Z","shell.execute_reply":"2021-05-31T11:58:58.349321Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_external(df, base_path):\n    # Preprocesses the External Dataset into the correct format.\n    impath = []\n    label_id = []\n    secondary_label = []\n    for row in tqdm(df.iterrows()):\n        row = row[1]\n        path = f'{base_path}{row.primary_label}/{row.filename}.npy'\n        impath += [path]\n        label_id += [LABEL_IDS[row.primary_label]]\n    df['impath'] = impath\n    df['label_id'] = label_id\n    df['secondary_labels'] = [[]] * len(df)\n    return df\ndef preprocess_soundscapes(df):\n    # In order to model the test set, you need soundscapes.\n    impath = []\n    label_id = []\n    site = []\n    for row in df.iterrows():\n        row = row[1]\n        path = f'{external_soundscapes}{row.filename}.npy'\n        if 'SSW' in path:\n            site += ['SSW']\n        else:\n            site += ['COR']\n        impath += [path]\n        classes = row.primary_label.split()\n        im_id = None\n        classes_added = []\n        for class_name in classes:\n            try:\n                \n                if im_id is None:\n                    if class_name == 'nocall':\n                        im_id = '-1'\n                        break\n                    im_id = str(LABEL_IDS[class_name]) \n                    classes_added += [class_name]\n                else:\n                    if class_name not in classes_added:\n                        im_id += f' {LABEL_IDS[class_name]}'\n                        classes_added += [class_name]\n            except:\n                continue\n        label_id += [im_id]\n    df['impath'] = impath \n    df['label_id'] = label_id\n    df['site'] = site\n    # Segregate into External Soundscapes and Train Sound Scapes\n    train_idx = []\n    external_idx = []\n    for idx, row in enumerate(df.iterrows()):\n        row = row[1]\n        filename = row.filename\n        # Strip the base from the name\n        base_filename = ''\n        for i in range(len(filename) - 1, -1, -1):\n            if filename[i] == '_':\n                base_filename = filename[:i]\n                break\n        # Check if it exists\n        num_files = len(glob.glob(f\"{SOUNDSCAPES_PATH}{base_filename}*\"))\n        if num_files > 0:\n            train_idx += [idx]\n        else:\n            external_idx += [idx]\n    train = df.iloc[train_idx]\n    external = df.iloc[external_idx] \n            \n    return train, external","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:58:58.35143Z","iopub.execute_input":"2021-05-31T11:58:58.351768Z","iopub.status.idle":"2021-05-31T11:58:58.364866Z","shell.execute_reply.started":"2021-05-31T11:58:58.351733Z","shell.execute_reply":"2021-05-31T11:58:58.364061Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Up Dfs","metadata":{}},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BACKGROUND_CSV = preprocess_background(BACKGROUND_CSV)\nexternal_soundscapes_csv, extra_soundscapes_csv = preprocess_soundscapes(external_soundscapes_csv)\n#external_data_csv = preprocess_external(external_data_csv, external_data)\n#external_data2_csv = preprocess_external(external_data2_csv, external_data2)\n#external_data3_csv = preprocess_external(external_data3_csv, external_data3)\n\n\ndf = clean_df(df)\n#BACKGROUND_CSV = clean_df(BACKGROUND_CSV)\nexternal_soundscapes_csv =clean_df(external_soundscapes_csv)\nextra_soundscapes_csv = clean_df(extra_soundscapes_csv)\n#external_data_csv = clean_df(external_data_csv)\n#external_data2_csv = clean_df(external_data2_csv)\n#external_data3_csv = clean_df(external_data3_csv)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:58:58.366199Z","iopub.execute_input":"2021-05-31T11:58:58.366753Z","iopub.status.idle":"2021-05-31T12:01:16.864056Z","shell.execute_reply.started":"2021-05-31T11:58:58.366719Z","shell.execute_reply":"2021-05-31T12:01:16.863219Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Append Extra Data","metadata":{}},{"cell_type":"code","source":"# Append Extra Data\n#df = df.append(BACKGROUND_CSV)\n#df = df.append(external_data_csv) # 40000 -> 100000\n# Append External 2\n#df = df.append(external_data2_csv) # 100000 -> 130000 # All Xeno-Canto Data Added.\n#df = df.append(external_data3_csv)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:01:16.867722Z","iopub.execute_input":"2021-05-31T12:01:16.868014Z","iopub.status.idle":"2021-05-31T12:01:16.873586Z","shell.execute_reply.started":"2021-05-31T12:01:16.867987Z","shell.execute_reply":"2021-05-31T12:01:16.872643Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FOlds","metadata":{}},{"cell_type":"code","source":"def stratified_KFold():\n    # Special Splitting Strategy that splits the files from the soundspace separately(So they also have 4/5 split)\n    # Performs a shuffled Stratified Split on the DataFrame.\n    splitter = StratifiedKFold(shuffle = True, random_state = 42) \n    FOLDS = []\n    SOUNDSCAPES_FOLDS = []\n    if not DataConfig.soundscapes_val:\n        for idx, (train, test) in enumerate(splitter.split(np.zeros(len(external_soundscapes_csv)), external_soundscapes_csv.site)):\n            train_fold= df.append(external_soundscapes_csv.iloc[train]).append(extra_soundscapes_csv)\n            test_fold = external_soundscapes_csv.iloc[test]\n            FOLDS += [(train_fold, test_fold)]\n            SOUNDSCAPES_FOLDS += [(external_soundscapes_csv.iloc[train].append(extra_soundscapes_csv), external_soundscapes_csv.iloc[test])]\n    else:\n        # No Folds, all monophone goes in train, all soundscapes in val\n        FOLDS = [(df.append(extra_soundscapes_csv), external_soundscapes_csv)]\n        SOUNDSCAPES_FOLDS += [(extra_soundscapes_csv, external_soundscapes_csv)]\n    return FOLDS, SOUNDSCAPES_FOLDS","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:01:16.877396Z","iopub.execute_input":"2021-05-31T12:01:16.877826Z","iopub.status.idle":"2021-05-31T12:01:16.887002Z","shell.execute_reply.started":"2021-05-31T12:01:16.877774Z","shell.execute_reply":"2021-05-31T12:01:16.886207Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS, SOUNDSCAPES_FOLDS = stratified_KFold()\n\n# Now Append the External soundscapes Data\n#if DataConfig.soundscapes_val:\ndf = df.append(external_soundscapes_csv).append(extra_soundscapes_csv)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:01:16.888485Z","iopub.execute_input":"2021-05-31T12:01:16.888864Z","iopub.status.idle":"2021-05-31T12:01:17.330264Z","shell.execute_reply.started":"2021-05-31T12:01:16.888837Z","shell.execute_reply":"2021-05-31T12:01:17.329357Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definition","metadata":{}},{"cell_type":"code","source":"class tmp_model(pl.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.load_state_dict(torch.load(ResNestPath, map_location = DEVICE))\ndef load_prev_model(name):\n    if \"resnest\" in name:\n        model = resnest50_fast_1s1x64d()\n    else:\n        model = torchvision.models.resnext50_32x4d(pretrained=False)\n    nb_ft = model.fc.in_features\n    del model.fc\n    num_cls =  264\n    model.fc = nn.Linear(nb_ft, num_cls)\n    model.load_state_dict(torch.load(ResNestPath, map_location = DEVICE))\n    model.fc = nn.Identity()\n    model.avgpool = nn.Identity()\n    return model\nclass FeatureExtractor(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model_name = ModelConfig.model_name\n        self.model = load_prev_model(self.model_name)\n        \n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.relu\n        self.maxpool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4\n        del self.model\n    def forward(self, x):\n        x = self.maxpool(self.bn1(self.act1(self.conv1(x))))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:01:17.332352Z","iopub.execute_input":"2021-05-31T12:01:17.332614Z","iopub.status.idle":"2021-05-31T12:01:17.344716Z","shell.execute_reply.started":"2021-05-31T12:01:17.332588Z","shell.execute_reply":"2021-05-31T12:01:17.343882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Blocks, Added on Top of CNN Features","metadata":{}},{"cell_type":"code","source":"class SEDAttention(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # SED Attention, for Clipwise and Framewise Preds\n        self.in_features = ModelConfig.feature_extractor_dim\n        self.out_features = ModelConfig.num_classes\n        \n        self.framewise = nn.Conv1d(self.in_features, self.out_features, 1)\n        self.attention = nn.Conv1d(self.in_features, self.out_features, 1)\n    def forward(self, x):\n        attention = F.softmax(self.attention(x), dim = -1) # (B, C, L)\n        framewise = self.framewise(x) # (B, C, L)\n        x = attention * framewise # (B, C, L)\n        # Sum over time\n        clipwise = x.sum(dim = -1) # (B, C)\n        return clipwise\n        \n        \n        \n    \nclass SEDHead(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = ModelConfig.feature_extractor_dim\n        self.n_mels = 128\n        self.dim_reduce = 32\n        self.drop_prob = ModelConfig.dropout\n        #self.Conv2d = nn.Conv2d(self.feature_extractor, self.feature_extractor, (self.n_mels // self.dim_reduce, 1))\n        self.drop1 = nn.Dropout(self.drop_prob)\n        self.drop2 = nn.Dropout(self.drop_prob)\n        \n        self.fc = nn.Linear(self.feature_extractor, self.feature_extractor) \n        self.relu = nn.ReLU(inplace = True)\n        \n        self.attention = SEDAttention()\n    def forward(self, x):\n        # X; Tensor(B, 2048, F, L):\n        x = torch.mean(x, dim=2)  # BS x nb_ft x t\n\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2  # BS x nb_ft x t\n        x = self.drop1(x) \n        x = x.transpose(1, 2) # (B, L, 2048) \n        # --------FC-------\n        x = self.relu(self.fc(x)) # (B, L, 2048)\n        x = x.transpose(1, 2)\n        x = self.drop2(x) # (B, 2048, L)\n        # ---------Attention---------------\n        return self.attention(x) # (B, C)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:18:25.264333Z","iopub.execute_input":"2021-05-31T12:18:25.264666Z","iopub.status.idle":"2021-05-31T12:18:25.279184Z","shell.execute_reply.started":"2021-05-31T12:18:25.264634Z","shell.execute_reply":"2021-05-31T12:18:25.278341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FullModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.head = SEDHead()\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        head = self.head(features)\n        return head\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:18:26.009262Z","iopub.execute_input":"2021-05-31T12:18:26.009576Z","iopub.status.idle":"2021-05-31T12:18:26.014653Z","shell.execute_reply.started":"2021-05-31T12:18:26.009548Z","shell.execute_reply":"2021-05-31T12:18:26.013573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelConfig:\n    num_classes = NUM_CLASSES\n    \n    \n    feature_extractor_dim = 2048\n    transformer_dim = 768\n    num_heads = 8\n    \n    model_name = 'resnest'\n    dropout = 0.5\n    num_layers = 2\n    head = 'Conv2DAtt'\n    num_channels = 3 # 1 for just Melspecs \n    \n    use_mixup = False # Whether or not to use mixup augmentation.\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:01:17.380772Z","iopub.execute_input":"2021-05-31T12:01:17.381226Z","iopub.status.idle":"2021-05-31T12:01:17.393065Z","shell.execute_reply.started":"2021-05-31T12:01:17.381198Z","shell.execute_reply":"2021-05-31T12:01:17.392142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PreCache The Dataset","metadata":{}},{"cell_type":"code","source":"def load_data(df):\n    def load_row(row):\n        # impath = TRAIN_IMAGES_ROOT/f\"{row.primary_label}/{row.filename}.npy\"\n        image = np.load(str(row.impath))\n        \n        if len(image.shape) > 2:\n            image = image[:MAX_READ_SAMPLES]\n        else:\n            image = np.expand_dims(image, axis = 0)\n        return row.filename, image\n    pool = joblib.Parallel(4)\n    mapper = joblib.delayed(load_row)\n    tasks = [mapper(row) for row in df.itertuples(False)]\n    res = pool(tqdm(tasks))\n    res = dict(res)\n    return res","metadata":{"id":"7HYQwAyBCWs8","execution":{"iopub.status.busy":"2021-05-31T12:01:17.395994Z","iopub.execute_input":"2021-05-31T12:01:17.396303Z","iopub.status.idle":"2021-05-31T12:01:17.404842Z","shell.execute_reply.started":"2021-05-31T12:01:17.39625Z","shell.execute_reply":"2021-05-31T12:01:17.40405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We cache the train set to reduce training time\n#audio_image_store = None\nif audio_image_store is None:\n    audio_image_store = load_data(df)","metadata":{"id":"Vw19bB7mCWs9","outputId":"09a5e374-7e5c-4c92-91e4-b60313cbb1a9","execution":{"iopub.status.busy":"2021-05-31T12:01:17.406314Z","iopub.execute_input":"2021-05-31T12:01:17.406676Z","iopub.status.idle":"2021-05-31T12:02:44.874645Z","shell.execute_reply.started":"2021-05-31T12:01:17.40664Z","shell.execute_reply":"2021-05-31T12:02:44.87366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Definition","metadata":{}},{"cell_type":"code","source":"def freq_mask(spec, F=30, num_masks=1, replace_with_zero=False):\n    # Mel Spec Augments\n    cloned = spec.clone()\n    num_mel_channels = cloned.shape[0]\n    \n    for i in range(0, num_masks):        \n        f = random.randrange(0, F)\n        f_zero = random.randrange(0, num_mel_channels - f)\n\n        # avoids randrange error if values are equal and range is empty\n        if (f_zero == f_zero + f): return cloned\n\n        mask_end = random.randrange(f_zero, f_zero + f) \n        if (replace_with_zero): cloned[f_zero:mask_end] = 0\n        else: cloned[f_zero:mask_end] = cloned.mean()\n    \n    return cloned\n\n#Export\ndef time_mask(spec, T=40, num_masks=1, replace_with_zero=False):\n    cloned = spec.clone()\n    len_spectro = cloned.shape[1]\n    \n    for i in range(0, num_masks):\n        t = random.randrange(0, T)\n        t_zero = random.randrange(0, len_spectro - t)\n\n        # avoids randrange error if values are equal and range is empty\n        if (t_zero == t_zero + t): return cloned\n\n        mask_end = random.randrange(t_zero, t_zero + t)\n        if (replace_with_zero): cloned[:,t_zero:mask_end] = 0\n        else: cloned[:,t_zero:mask_end] = cloned.mean()\n    return cloned\ndef pad_tensor(y):\n    # Pads a Tensor to shape (128, 281) \n    shape = (128, 281)\n    new_tensor = torch.zeros(shape)\n    new_tensor[:, :y.shape[1]] = y\n    return new_tensor\ndef lower_gain(y):\n    # Lowers the Gain of the image\n    lower_bound = 0.5\n    upper_bound = 1\n    gain = random.uniform(lower_bound, upper_bound)\n    return y ** gain \ndef mixup_with_val(y, idx):\n    # Mixes up the melspec with the melspec from train_soundscapes(In order to produce \"artificial\" noise that exists in the test set.\n    # This is only ever used at train, so idx is guaranteed to be 0\n    soundscapes_df = SOUNDSCAPES_FOLDS[idx][0]\n    soundscapes_df = soundscapes_df[soundscapes_df.primary_label == 'nocall']\n    idx = random.randint(0, len(soundscapes_df) - 1)\n    row = torch.tensor(np.load(soundscapes_df.iloc[idx].impath))\n    background_mix = pad_tensor(row)\n    \n    mix_min = 0.4\n    mix_max = 1 \n    mix_quan = random.uniform(mix_min, mix_max)\n    return y + mix_quan * background_mix\n    \n    \ndef cutmix_with_val(y, idx):\n    soundscapes_df = SOUNDSCAPES_FOLDS[idx][0]\n    soundscapes_df = soundscapes_df[soundscapes_df.primary_label == 'nocall']\n    idx = random.randint(0, len(soundscapes_df) - 1) \n    row = torch.tensor(np.load(soundscapes_df.iloc[idx].impath))\n    background_mix = pad_tensor(row)\n    _, L = background_mix.shape\n    start_idx = random.randint(0, L - 1)\n    end_idx = random.randint(start_idx + 1, L)\n    initial = y[:, :start_idx]\n    end = y[:, end_idx:]\n    middle= background_mix[:, start_idx:end_idx] / 255.0\n    y = torch.cat([initial, middle, end], dim = -1)\n    return y\n#def cutout(y):\n#    # Cutouts a Box in the Melspec, alternative to specAugment\n    \ndef noise(y):\n    \n    return y + min(1,max(0.1, np.random.exponential(0.25))) \ndef augment(y, idx):\n    # Augmentation on MelSpecs\n    \n    if random.random() < 0.25:\n        y = time_mask(y, num_masks = 2)\n    if random.random() < 0.25:\n        y = freq_mask(y, num_masks = 2)\n    #if random.random() < 0.25:\n    #    print(\"Noised\")\n    #    y = noise(y)\n    #if random.random() < 0.5:\n    #    y = mixup_with_val(y, idx)\n    if random.random() < 0.5:\n        y = lower_gain(y)\n    return y # Augmentations aren't working right now.","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:44.876258Z","iopub.execute_input":"2021-05-31T12:02:44.876814Z","iopub.status.idle":"2021-05-31T12:02:44.896018Z","shell.execute_reply.started":"2021-05-31T12:02:44.876762Z","shell.execute_reply":"2021-05-31T12:02:44.89526Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BirdClefDataset(Dataset):\n\n    def __init__(self, audio_image_store, idx, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n        \n        self.idx = idx\n        self.number = 0 if is_train else 1\n        self.audio_image_store = audio_image_store\n        self.meta = FOLDS[idx][self.number].copy().reset_index(drop=True)\n        self.sr = sr\n        self.is_train = is_train\n        self.num_classes = num_classes\n        self.duration = duration\n        \n        self.audio_length = self.duration*self.sr\n        \n        self.mean_train = np.array([0.485, 0.456, 0.406])\n        self.mean_train =torch.tensor(np.expand_dims(np.expand_dims(self.mean_train, axis = -1), axis = -1))\n        self.std_train = np.array([0.229, 0.224, 0.225])\n        self.std_train = torch.tensor(np.expand_dims(np.expand_dims(self.std_train, axis = -1), axis = -1))\n        self.stats = (self.mean_train, self.std_train) # ImageNet Stats \n        self.shape = (128, 801)\n    \n    \n   \n\n    def __len__(self):\n        return len(self.meta)\n    \n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        #secondary_labels = row.secondary_tensors\n        image = self.audio_image_store[row.filename]\n        image = torch.tensor(image[0]).to(torch.float32) / 255.0 # Sample From the Front.\n\n        padded = torch.zeros(self.shape)\n        padded[:, :image.shape[1]] = image\n        image = padded\n        if self.is_train:\n            image = augment(image, self.idx)\n        # One image is Regular power to db, another is cleaned powered power to db, one is pcen\n        if ModelConfig.num_channels == 3:\n            pcen = lb.pcen(image.numpy())\n            mel = image\n            power = image ** 1.5\n            image = torch.stack([torch.tensor(pcen), torch.tensor(mel), torch.tensor(power)], dim = 0)\n        else:\n            image = torch.stack([image, image, image])\n       \n        \n        t = np.zeros(self.num_classes, dtype=np.float32) \n        label_id = row.label_id\n        try:\n            label_id = label_id.item()\n        except:\n            pass\n        \n        if type(label_id) == type(1.0) or type(label_id) == type(1):\n            t[row.label_id] = 1\n        else:\n            \n            # Soundscape, so multiple things\n            labels = row.label_id.split()\n            for label in labels:\n                if label == '-1':\n                    t = np.zeros(self.num_classes, dtype = np.float32)\n                    break\n                t[int(label)] = 1\n                \n        \n        return torch.tensor(image).to(torch.float32), torch.tensor(t).to(torch.float32)","metadata":{"id":"OWSkCXyhCWs-","execution":{"iopub.status.busy":"2021-05-31T12:02:44.897279Z","iopub.execute_input":"2021-05-31T12:02:44.897725Z","iopub.status.idle":"2021-05-31T12:02:44.915458Z","shell.execute_reply.started":"2021-05-31T12:02:44.897677Z","shell.execute_reply":"2021-05-31T12:02:44.91463Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = BirdClefDataset(audio_image_store, 0, is_train = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:44.916743Z","iopub.execute_input":"2021-05-31T12:02:44.917204Z","iopub.status.idle":"2021-05-31T12:02:44.950486Z","shell.execute_reply.started":"2021-05-31T12:02:44.917166Z","shell.execute_reply":"2021-05-31T12:02:44.94966Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor images, labels in dataset:\n    lbd.specshow(images[1].numpy())\n    plt.show()\n    count += 1\n    if count == 32:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:44.951697Z","iopub.execute_input":"2021-05-31T12:02:44.952052Z","iopub.status.idle":"2021-05-31T12:02:48.870622Z","shell.execute_reply.started":"2021-05-31T12:02:44.952018Z","shell.execute_reply":"2021-05-31T12:02:48.869819Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_smooth(primary): \n    primary = primary.clone() \n    ones = primary == 1 \n    not_ones = primary == 0 \n    primary[ones] = 0.995 \n    primary[not_ones] = 0.0025 \n    return primary","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:48.871955Z","iopub.execute_input":"2021-05-31T12:02:48.872442Z","iopub.status.idle":"2021-05-31T12:02:48.877112Z","shell.execute_reply.started":"2021-05-31T12:02:48.872404Z","shell.execute_reply":"2021-05-31T12:02:48.876327Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mixup and cutmix.","metadata":{}},{"cell_type":"code","source":"def mixup(x, y):\n    # Performs Mixup on Melspecs\n    # x: Tensor(B, 128, 281)\n    # Mixup sampled from beta distribution.\n    beta = 0.4\n    gamma = random.beta(beta, beta)\n    gamma = max(1-gamma, gamma)\n    shuffle = torch.randperm(x.shape[0]).to(x.device)\n    x = gamma*x + (1-gamma)*x[shuffle]\n    y = gamma*y + (1-gamma)*y[shuffle]\n    # for hard mixup, anything that isn't 0 is set to 1.\n    return x, y # efficient mixup impletation(not completely random, but it should be fine.)\n    \ndef cutmix(batch, y):\n    # Performs Time wise Cutmix on batch of melspecs\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:48.878371Z","iopub.execute_input":"2021-05-31T12:02:48.878883Z","iopub.status.idle":"2021-05-31T12:02:48.887632Z","shell.execute_reply.started":"2021-05-31T12:02:48.878846Z","shell.execute_reply":"2021-05-31T12:02:48.886866Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# collate functions","metadata":{}},{"cell_type":"code","source":"def train_collate(values):\n    images = torch.stack([value[0] for value in values])\n    labels = torch.stack([value[1] for value in values]) # (B, 128, 281)\n    # ----------MixUp--------(or Cutmix?)\n    if ModelConfig.use_mixup:\n        images, labels = mixup(images, labels)\n    return images, labels\ndef val_collate(values):\n    images = torch.stack([value[0] for value in values])\n    labels = torch.stack([value[1] for value in values])\n    return images, labels \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:48.892405Z","iopub.execute_input":"2021-05-31T12:02:48.892668Z","iopub.status.idle":"2021-05-31T12:02:48.901261Z","shell.execute_reply.started":"2021-05-31T12:02:48.892641Z","shell.execute_reply":"2021-05-31T12:02:48.900369Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{"id":"F56zXq8CVAqn"}},{"cell_type":"code","source":"CRITERION = nn.BCEWithLogitsLoss(reduction = 'mean') # TODO: Mask Secondary Outputs Loss.\ndef loss_fn(y_pred, primary):\n    # Primary: Tensor(B)\n    # Secondary: Tensor(B, C)\n    # Y_Pred: Tensor(B, C)\n    # TODO Split the Losses.\n    B, C = primary.shape\n    smooth = label_smooth(primary)\n    y_pred = y_pred.float()\n    loss = CRITERION(y_pred, smooth)\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:48.902953Z","iopub.execute_input":"2021-05-31T12:02:48.903392Z","iopub.status.idle":"2021-05-31T12:02:48.909099Z","shell.execute_reply.started":"2021-05-31T12:02:48.903354Z","shell.execute_reply":"2021-05-31T12:02:48.908256Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"class F1_score(Metric):\n    def __init__(self):\n        self.thresholds = np.arange(0.0, 1.0, 0.01)\n        self.f1_scores = [0.0 for i in range(len(self.thresholds))]\n        self.count = 0\n    def round_pred(self, y_pred, thresh):\n        ones = y_pred >= thresh\n        logits = torch.zeros_like(y_pred, device = y_pred.device)\n        logits = logits + ones.int()\n        return logits\n    def round_true(self, y_true):\n        ones = y_true >= 0.5\n        logits = torch.zeros_like(y_true, device = y_true.device)\n        logits = logits + ones.int()\n        return logits\n    def metric(self, y_pred, target, thresh_idx):\n        # Given y_pred = Tensor(B, C) and primary = Tensor(B, C), Computes the Row-wise F1 Score\n        # Threshold Predictions\n        predictions = self.round_pred(y_pred, self.thresholds[thresh_idx])\n        # PRedictions: (B, C)\n        zeros = torch.sum(predictions, axis = -1) == 0\n        predictions[zeros, NUM_CLASSES - 1] = 1 # No Call\n        tp = (predictions * target).sum(1)\n        fp = (predictions * (1 - target)).sum(1)\n        fn = ((1 - predictions) * target).sum(1)\n        \n        eps = 1e-9\n        f1 = (tp + eps) / (tp + (fp + fn) / 2 + eps)\n        # Accumulate the f1 score\n        self.f1_scores[thresh_idx] += f1.mean().item()\n    def compute_f1_score(self, y_pred, primary):\n        primary = self.round_true(primary)\n        for th_idx in range(len(self.thresholds)):\n            y_p = self.round_pred(y_pred, self.thresholds[th_idx])\n            self.metric(y_p, primary, th_idx)\n        self.count += 1\n    def accumulate(self, learn):\n        # y_pred: Tensor(B, C)\n        y_pred = torch.sigmoid(learn.pred).cpu()\n        primary = learn.y.cpu()\n        self.compute_f1_score(y_pred, primary)\n        \n    def reset(self):\n        self.f1_scores = [0.0 for i in range(len(self.f1_scores))]\n        self.count = 0\n        \n    @property\n    def value(self):\n        eps = 1e-8\n        # Computes the best threshold and result\n        best = 0\n        best_th = 0\n        for th_idx in range(len(self.f1_scores)):\n            if self.f1_scores[th_idx] > best:\n                best = self.f1_scores[th_idx]\n                best_th = self.thresholds[th_idx]\n        best = (best + eps) / (self.count + eps)\n        print(f\"F1Score: {best}\")\n        return best\nclass F1Score_th(Metric):\n    def __init__(self):\n        self.thresholds = np.arange(0.0, 1.0, 0.01)\n        self.f1_scores = [0.0 for i in range(len(self.thresholds))]\n        self.count = 0\n    def round_pred(self, y_pred, thresh):\n        ones = y_pred >= thresh\n        logits = torch.zeros_like(y_pred, device = y_pred.device)\n        logits = logits + ones.int()\n        return logits\n    def round_true(self, y_true):\n        ones = y_true >= 0.5\n        logits = torch.zeros_like(y_true, device = y_true.device)\n        logits = logits + ones.int()\n        return logits\n    def metric(self, y_pred, target, thresh_idx):\n        # Given y_pred = Tensor(B, C) and primary = Tensor(B, C), Computes the Row-wise F1 Score\n        # Threshold Predictions\n        predictions = self.round_pred(y_pred, self.thresholds[thresh_idx])\n    \n        tp = (predictions * target).sum(1)\n        fp = (predictions * (1 - target)).sum(1)\n        fn = ((1 - predictions) * target).sum(1)\n        \n        eps = 1e-9\n        f1 = (tp + eps) / (tp + (fp + fn) / 2 + eps)\n        # Accumulate the f1 score\n        self.f1_scores[thresh_idx] += f1.mean().item()\n    def compute_f1_score(self, y_pred, primary):\n        primary = self.round_true(primary)\n        for th_idx in range(len(self.thresholds)):\n            y_p = self.round_pred(y_pred, self.thresholds[th_idx])\n            self.metric(y_p, primary, th_idx)\n        self.count += 1\n    def accumulate(self, learn):\n        # y_pred: Tensor(B, C)\n        y_pred = torch.sigmoid(learn.pred).cpu()\n        primary = learn.y.cpu()\n        self.compute_f1_score(y_pred, primary)\n        \n    def reset(self):\n        self.f1_scores = [0.0 for i in range(len(self.f1_scores))]\n        self.count = 0\n        \n    @property\n    def value(self):\n        eps = 1e-8\n        # Computes the best threshold and result\n        best = 0\n        best_th = 0\n        for th_idx in range(len(self.f1_scores)):\n            if self.f1_scores[th_idx] > best:\n                best = self.f1_scores[th_idx]\n                best_th = self.thresholds[th_idx]\n        best = (best + eps) / (self.count + eps)\n        return best_th\n        \n    \nclass Accuracy(Metric):\n    def __init__(self):\n        self.accuracy = 0\n        self.count = 0\n    def round_pred(self, y_pred):\n        y_pred = torch.sigmoid(y_pred)\n        ones = y_pred >= 0.5\n        scores = torch.zeros_like(y_pred, device = y_pred.device)\n        scores[ones] = 1\n        return scores\n    def round_true(self, y_true):\n        ones = y_true >= 0.5\n        logits = torch.zeros_like(ones, device = ones.device)\n        logits = logits + ones.int()\n        return logits\n    def accumulate(self,learn):\n        y_pred = self.round_pred(learn.pred)\n        primary = self.round_true(learn.y)\n        # y_pred: Tensor(B, C)\n        # primary: Tensor(B, C)\n        B, C = y_pred.shape\n        tp = torch.sum(y_pred == primary)\n        acc = tp / (B * C)\n        \n        self.accuracy += acc.item()\n        self.count += 1\n    def reset(self):\n        self.accuracy = 0 \n        self.count = 0\n    @property\n    def value(self):\n        eps = 1e-8\n        return round((self.accuracy + eps) / (self.count + eps), 3)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:02:48.910645Z","iopub.execute_input":"2021-05-31T12:02:48.911258Z","iopub.status.idle":"2021-05-31T12:02:48.940913Z","shell.execute_reply.started":"2021-05-31T12:02:48.911222Z","shell.execute_reply":"2021-05-31T12:02:48.940062Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRaining Config","metadata":{}},{"cell_type":"code","source":"class TrainingConfig:\n    start_lr = 1e-3\n    max_lr = 5e-3\n    min_lr = 1e-4\n    warm_steps = 0.1\n    peak_steps = 0.2 # 0.3 ramp up, 0.7 rabsamp down = super-convergence\n    \n    num_epochs = 9\n    train_steps = len(FOLDS[0][0])\n    \n    total_steps = num_epochs * train_steps\n    \n    weight_decay = 0\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:14:19.224963Z","iopub.execute_input":"2021-05-31T12:14:19.225305Z","iopub.status.idle":"2021-05-31T12:14:19.230546Z","shell.execute_reply.started":"2021-05-31T12:14:19.225278Z","shell.execute_reply":"2021-05-31T12:14:19.22958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Fn","metadata":{}},{"cell_type":"markdown","source":"# DataLoader Config","metadata":{}},{"cell_type":"code","source":"class DataLoaderConfig:\n    def __init__(self, is_train):\n        num_workers = 4\n        pin_memory = True\n        bs = 32\n        create_batch = train_collate if is_train else val_collate\n        shuffle = is_train\n        self.config = {'num_workers': num_workers, 'pin_memory': pin_memory, 'bs': bs, 'shuffle': shuffle, 'create_batch': create_batch}\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:14:20.584512Z","iopub.execute_input":"2021-05-31T12:14:20.584889Z","iopub.status.idle":"2021-05-31T12:14:20.589892Z","shell.execute_reply.started":"2021-05-31T12:14:20.584857Z","shell.execute_reply":"2021-05-31T12:14:20.588964Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_folds_fast_ai(folds, load_prev = False):\n    # Fast Ai, with their variety of tricks, trains faster and better\n    for fold_idx in folds:\n        # Overfit Testing\n        train = BirdClefDataset(audio_image_store, fold_idx, is_train = True)\n        val = BirdClefDataset(audio_image_store,fold_idx, is_train = False)#BirdClefDataset(audio_image_store, val, is_train = False)\n        \n        train_config = DataLoaderConfig(is_train = True)\n        val_config = DataLoaderConfig(is_train = False)\n        \n        train = DataLoader(train, **train_config.config)\n        val = DataLoader(val, **val_config.config)\n        dls = DataLoaders(train, val)\n        model = FullModel()\n        if torch.cuda.is_available(): dls.cuda(),model.cuda()\n        learn = Learner(dls, model, loss_func=loss_fn,\n                metrics=[F1Score_th(), F1_score], opt_func = Adam, lr = TrainingConfig.start_lr, wd = TrainingConfig.weight_decay).to_fp16()\n        cbs = [\n            SaveModelCallback(monitor = 'f1_score',comp = np.greater, fname = f'model_{fold_idx}'),\n            EarlyStoppingCallback(monitor = 'f1_score', comp = np.greater, patience = 5),\n            ReduceLROnPlateau(monitor = 'f1_score', comp = np.greater, patience = 3, min_lr = 1e-7)\n        ]\n        if load_prev:\n            learn.load(f'final_{fold_idx}')\n        learn.fit_one_cycle(TrainingConfig.num_epochs, lr_max = TrainingConfig.max_lr, cbs = cbs, wd = TrainingConfig.weight_decay)\n        learn.save(f\"final_{fold_idx}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:14:20.759509Z","iopub.execute_input":"2021-05-31T12:14:20.75981Z","iopub.status.idle":"2021-05-31T12:14:20.768509Z","shell.execute_reply.started":"2021-05-31T12:14:20.759761Z","shell.execute_reply":"2021-05-31T12:14:20.76759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/sed1s-bird-clef/models/ ./","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS_IDX = [2]\nmodel = train_folds_fast_ai(FOLDS_IDX, load_prev = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T12:18:31.550707Z","iopub.execute_input":"2021-05-31T12:18:31.551075Z"},"trusted":true},"execution_count":null,"outputs":[]}]}