{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genuine-place",
   "metadata": {
    "papermill": {
     "duration": 0.02271,
     "end_time": "2021-06-01T14:41:12.737998",
     "exception": false,
     "start_time": "2021-06-01T14:41:12.715288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-bibliography",
   "metadata": {
    "papermill": {
     "duration": 0.020913,
     "end_time": "2021-06-01T14:41:12.780616",
     "exception": false,
     "start_time": "2021-06-01T14:41:12.759703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this kenel, I'm going to use a classical **ResneSt50** for bird identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "studied-diagnosis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:41:12.829159Z",
     "iopub.status.busy": "2021-06-01T14:41:12.828521Z",
     "iopub.status.idle": "2021-06-01T14:41:40.870764Z",
     "shell.execute_reply": "2021-06-01T14:41:40.870102Z",
     "shell.execute_reply.started": "2021-06-01T11:22:29.330153Z"
    },
    "papermill": {
     "duration": 28.069154,
     "end_time": "2021-06-01T14:41:40.870913",
     "exception": false,
     "start_time": "2021-06-01T14:41:12.801759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import resnest\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -q \"../input/resnest50-fast-package/resnest-0.0.6b20200701/resnest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepted-israeli",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-01T14:41:40.925844Z",
     "iopub.status.busy": "2021-06-01T14:41:40.924932Z",
     "iopub.status.idle": "2021-06-01T14:42:11.580776Z",
     "shell.execute_reply": "2021-06-01T14:42:11.579187Z",
     "shell.execute_reply.started": "2021-06-01T11:22:57.5251Z"
    },
    "id": "kSCcqxf0c_O7",
    "papermill": {
     "duration": 30.686877,
     "end_time": "2021-06-01T14:42:11.580927",
     "exception": false,
     "start_time": "2021-06-01T14:41:40.894050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/audiomentations/audiomentations-0.15.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy<2,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from audiomentations==0.15.0) (1.5.4)\r\n",
      "Requirement already satisfied: librosa<=0.8.0,>=0.6.1 in /opt/conda/lib/python3.7/site-packages (from audiomentations==0.15.0) (0.8.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from audiomentations==0.15.0) (1.19.5)\r\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (0.52.0)\r\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (4.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (1.0.1)\r\n",
      "Requirement already satisfied: soundfile>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (0.10.3.post1)\r\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2.1.9)\r\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (0.2.2)\r\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (0.24.1)\r\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.7/site-packages (from librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (1.3.0)\r\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (0.35.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (1.4.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2.25.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (20.9)\r\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.7/site-packages (from resampy>=0.2.2->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (1.15.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2.1.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (1.14.5)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2.20)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations==0.15.0) (1.26.3)\r\n",
      "Installing collected packages: audiomentations\r\n",
      "Successfully installed audiomentations-0.15.0\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "from resnest.torch import resnest50_fast_1s1x64d\n",
    "from resnest.torch import resnest50\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "!pip install ../input/audiomentations/audiomentations-0.15.0-py3-none-any.whl\n",
    "\n",
    "import audiomentations\n",
    "from torchvision.models.resnet import ResNet, Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-annotation",
   "metadata": {
    "papermill": {
     "duration": 0.022664,
     "end_time": "2021-06-01T14:42:11.627240",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.604576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rational-business",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:11.726960Z",
     "iopub.status.busy": "2021-06-01T14:42:11.726384Z",
     "iopub.status.idle": "2021-06-01T14:42:11.737948Z",
     "shell.execute_reply": "2021-06-01T14:42:11.737078Z",
     "shell.execute_reply.started": "2021-06-01T11:23:28.47644Z"
    },
    "id": "rJhYZVIDc_O9",
    "papermill": {
     "duration": 0.088271,
     "end_time": "2021-06-01T14:42:11.738146",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.649875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 397 \n",
    "SR = 32_000\n",
    "THRESH = 0.18\n",
    "WEIGHT = 0.00\n",
    "\n",
    "DURATION = 5\n",
    "TOP_N = 3 # max preds = 3\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/test_soundscapes\")\n",
    "SAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\n",
    "TARGET_PATH = None\n",
    "    \n",
    "if not len(list(TEST_AUDIO_ROOT.glob(\"*.ogg\"))):\n",
    "    TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/train_soundscapes\")\n",
    "    SAMPLE_SUB_PATH = None\n",
    "    # SAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\n",
    "    TARGET_PATH = Path(\"../input/birdclef-2021/train_soundscape_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-pride",
   "metadata": {
    "papermill": {
     "duration": 0.022843,
     "end_time": "2021-06-01T14:42:11.784723",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.761880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "level-litigation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:11.838209Z",
     "iopub.status.busy": "2021-06-01T14:42:11.837554Z",
     "iopub.status.idle": "2021-06-01T14:42:11.840491Z",
     "shell.execute_reply": "2021-06-01T14:42:11.840104Z",
     "shell.execute_reply.started": "2021-06-01T11:23:28.556841Z"
    },
    "id": "4fwNhdbJc_O-",
    "papermill": {
     "duration": 0.03295,
     "end_time": "2021-06-01T14:42:11.840602",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.807652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MelSpecComputer:\n",
    "    def __init__(self, sr, n_mels, fmin, fmax, **kwargs):\n",
    "        self.sr = sr\n",
    "        self.n_mels = n_mels\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        kwargs[\"n_fft\"] = kwargs.get(\"n_fft\", self.sr//10)\n",
    "        kwargs[\"hop_length\"] = kwargs.get(\"hop_length\", self.sr//(10*4))\n",
    "        self.kwargs = kwargs\n",
    "        self.tta1 = audiomentations.Normalize(p = 1.0)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        y1 = y.copy()\n",
    "        #y2 = self.tta1(y.copy(), sample_rate = 32000)\n",
    "        \n",
    "        ys = [y1]#, y2]\n",
    "        melspecs = []\n",
    "        for y in ys:\n",
    "            melspec1 = lb.feature.melspectrogram(\n",
    "                y, sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax, **self.kwargs,\n",
    "            )\n",
    "            melspec1 = lb.power_to_db(melspec1).astype(np.float32)\n",
    "            melspecs += [melspec1]\n",
    "        return melspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numeric-wheat",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:11.893521Z",
     "iopub.status.busy": "2021-06-01T14:42:11.892822Z",
     "iopub.status.idle": "2021-06-01T14:42:11.895702Z",
     "shell.execute_reply": "2021-06-01T14:42:11.895293Z",
     "shell.execute_reply.started": "2021-06-01T11:23:28.577179Z"
    },
    "id": "Nk0haSTIc_O_",
    "papermill": {
     "duration": 0.032391,
     "end_time": "2021-06-01T14:42:11.895811",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.863420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "    \n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def crop_or_pad(y, length):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, length - np.zeros(len(y))])\n",
    "    elif len(y) > length:\n",
    "        y = y[:length]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "basic-marathon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:11.958119Z",
     "iopub.status.busy": "2021-06-01T14:42:11.957348Z",
     "iopub.status.idle": "2021-06-01T14:42:11.960067Z",
     "shell.execute_reply": "2021-06-01T14:42:11.959652Z",
     "shell.execute_reply.started": "2021-06-01T11:23:28.597088Z"
    },
    "papermill": {
     "duration": 0.041271,
     "end_time": "2021-06-01T14:42:11.960172",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.918901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, data, sr=SR, n_mels=128, fmin=0, fmax=None, duration=DURATION, step=None, res_type=\"kaiser_fast\", resample=True):\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.sr = sr\n",
    "        self.n_mels = n_mels\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax or self.sr//2\n",
    "\n",
    "        self.duration = duration\n",
    "        self.audio_length = self.duration*self.sr\n",
    "        self.step = step or self.audio_length\n",
    "        \n",
    "        self.res_type = res_type\n",
    "        self.resample = resample\n",
    "\n",
    "        self.mel_spec_computer = MelSpecComputer(sr=self.sr, n_mels=self.n_mels, fmin=self.fmin,\n",
    "                                                 fmax=self.fmax)\n",
    "    \n",
    "        self.mean_train = np.array([0.485, 0.456, 0.406])\n",
    "        self.mean_train =torch.tensor(np.expand_dims(np.expand_dims(self.mean_train, axis = -1), axis = -1))\n",
    "        self.std_train = np.array([0.229, 0.224, 0.225])\n",
    "        self.std_train = torch.tensor(np.expand_dims(np.expand_dims(self.std_train, axis = -1), axis = -1))\n",
    "        self.stats = (self.mean_train, self.std_train) # ImageNet Stats \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def normalize(self, image):\n",
    "        image = image - self.mean_train\n",
    "        image = image / self.std_train\n",
    "        return image\n",
    "    \n",
    "    def audio_to_image(self, audio):\n",
    "        melspecs = self.mel_spec_computer(audio) \n",
    "        images = []\n",
    "        for melspec in melspecs:\n",
    "            image = torch.tensor(mono_to_color(melspec))\n",
    "            image = image / 255.0\n",
    "\n",
    "            pcen = lb.pcen(image.numpy())\n",
    "            mel = image\n",
    "            power = image ** 1.5\n",
    "            image = torch.stack([torch.tensor(pcen), torch.tensor(mel), torch.tensor(power)], dim = 0)\n",
    "            images += [image]\n",
    "        return torch.stack(images)[0]\n",
    "\n",
    "    def read_file(self, filepath):\n",
    "        old_audio, orig_sr = sf.read(filepath, dtype=\"float32\")\n",
    "\n",
    "        if self.resample and orig_sr != self.sr:\n",
    "            old_audio = lb.resample(old_audio, orig_sr, self.sr, res_type=self.res_type)\n",
    "          \n",
    "        audios = []\n",
    "        for i in range(self.audio_length, len(old_audio) + self.step, self.step):\n",
    "            start = max(0, i - self.audio_length)\n",
    "            end = start + self.audio_length\n",
    "            audios.append(old_audio[start:end])\n",
    "            \n",
    "        if len(audios[-1]) < self.audio_length:\n",
    "            audios = audios[:-1]\n",
    "            \n",
    "        images = [self.audio_to_image(audio) for audio in audios]\n",
    "        images = torch.stack(images)\n",
    "        \n",
    "        images2 = self.audio_to_image(old_audio)[:, :, :-1]\n",
    "        \n",
    "        return images2, images\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.read_file(self.data.loc[idx, \"filepath\"]) # Chop off the last bit for clean 24000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "material-envelope",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:12.010403Z",
     "iopub.status.busy": "2021-06-01T14:42:12.009903Z",
     "iopub.status.idle": "2021-06-01T14:42:12.044804Z",
     "shell.execute_reply": "2021-06-01T14:42:12.045386Z",
     "shell.execute_reply.started": "2021-06-01T11:23:28.628655Z"
    },
    "id": "rVwlOrbxc_PD",
    "outputId": "9b53ec99-634a-4b30-f9b5-75036184482b",
    "papermill": {
     "duration": 0.062381,
     "end_time": "2021-06-01T14:42:12.045550",
     "exception": false,
     "start_time": "2021-06-01T14:42:11.983169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>site</th>\n",
       "      <th>date</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20152_SSW_20170805</td>\n",
       "      <td>20152</td>\n",
       "      <td>SSW</td>\n",
       "      <td>20170805</td>\n",
       "      <td>../input/birdclef-2021/train_soundscapes/20152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57610_COR_20190904</td>\n",
       "      <td>57610</td>\n",
       "      <td>COR</td>\n",
       "      <td>20190904</td>\n",
       "      <td>../input/birdclef-2021/train_soundscapes/57610...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7843_SSW_20170325</td>\n",
       "      <td>7843</td>\n",
       "      <td>SSW</td>\n",
       "      <td>20170325</td>\n",
       "      <td>../input/birdclef-2021/train_soundscapes/7843_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42907_SSW_20170708</td>\n",
       "      <td>42907</td>\n",
       "      <td>SSW</td>\n",
       "      <td>20170708</td>\n",
       "      <td>../input/birdclef-2021/train_soundscapes/42907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7019_COR_20190904</td>\n",
       "      <td>7019</td>\n",
       "      <td>COR</td>\n",
       "      <td>20190904</td>\n",
       "      <td>../input/birdclef-2021/train_soundscapes/7019_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename     id site      date  \\\n",
       "0  20152_SSW_20170805  20152  SSW  20170805   \n",
       "1  57610_COR_20190904  57610  COR  20190904   \n",
       "2   7843_SSW_20170325   7843  SSW  20170325   \n",
       "3  42907_SSW_20170708  42907  SSW  20170708   \n",
       "4   7019_COR_20190904   7019  COR  20190904   \n",
       "\n",
       "                                            filepath  \n",
       "0  ../input/birdclef-2021/train_soundscapes/20152...  \n",
       "1  ../input/birdclef-2021/train_soundscapes/57610...  \n",
       "2  ../input/birdclef-2021/train_soundscapes/7843_...  \n",
       "3  ../input/birdclef-2021/train_soundscapes/42907...  \n",
       "4  ../input/birdclef-2021/train_soundscapes/7019_...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(TEST_AUDIO_ROOT).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"id\", \"site\", \"date\", \"filepath\"]\n",
    ")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lasting-barcelona",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:12.098597Z",
     "iopub.status.busy": "2021-06-01T14:42:12.097973Z",
     "iopub.status.idle": "2021-06-01T14:42:12.560228Z",
     "shell.execute_reply": "2021-06-01T14:42:12.559515Z",
     "shell.execute_reply.started": "2021-06-01T11:23:28.676125Z"
    },
    "papermill": {
     "duration": 0.491042,
     "end_time": "2021-06-01T14:42:12.560409",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.069367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/birdclef-2021/train_metadata.csv\")\n",
    "\n",
    "LABEL_IDS = {label: label_id for label_id,label in enumerate(sorted(df_train[\"primary_label\"].unique()))}\n",
    "INV_LABEL_IDS = {val: key for key,val in LABEL_IDS.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-portable",
   "metadata": {
    "papermill": {
     "duration": 0.038737,
     "end_time": "2021-06-01T14:42:12.641278",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.602541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Config (Load Learned Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-patio",
   "metadata": {
    "papermill": {
     "duration": 0.024223,
     "end_time": "2021-06-01T14:42:12.694600",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.670377",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "partial-tutorial",
   "metadata": {
    "papermill": {
     "duration": 0.024165,
     "end_time": "2021-06-01T14:42:12.743081",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.718916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confused-mustang",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:12.814420Z",
     "iopub.status.busy": "2021-06-01T14:42:12.813141Z",
     "iopub.status.idle": "2021-06-01T14:42:12.815623Z",
     "shell.execute_reply": "2021-06-01T14:42:12.816044Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.050464Z"
    },
    "papermill": {
     "duration": 0.048842,
     "end_time": "2021-06-01T14:42:12.816172",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.767330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1:, :].repeat(\n",
    "        1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "\n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            if pool_size == (1, 1):\n",
    "                x = F.avg_pool2d(x, kernel_size = (1, 1))\n",
    "            else:\n",
    "                x = F.avg_pool2d(x, kernel_size=pool_size, padding = 1, stride = 2)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prerequisite-storage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:12.889389Z",
     "iopub.status.busy": "2021-06-01T14:42:12.876193Z",
     "iopub.status.idle": "2021-06-01T14:42:12.891954Z",
     "shell.execute_reply": "2021-06-01T14:42:12.891503Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.075135Z"
    },
    "papermill": {
     "duration": 0.052184,
     "end_time": "2021-06-01T14:42:12.892057",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.839873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n",
    "        of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame\n",
    "        if win_length is None:\n",
    "            win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified\n",
    "        if hop_length is None:\n",
    "            hop_length = int(win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size\n",
    "        fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, data_length)\n",
    "        Returns:\n",
    "          real: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "          imag: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "    \n",
    "    \n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0, \n",
    "        freeze_parameters=True):\n",
    "        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n",
    "            win_length=win_length, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        Returns:\n",
    "          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    \n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n",
    "        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps)\n",
    "        \n",
    "        Output: (batch_size, time_steps, mel_bins)\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        \"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.core.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "precious-control",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:12.949635Z",
     "iopub.status.busy": "2021-06-01T14:42:12.949132Z",
     "iopub.status.idle": "2021-06-01T14:42:12.953094Z",
     "shell.execute_reply": "2021-06-01T14:42:12.952652Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.105937Z"
    },
    "papermill": {
     "duration": 0.037712,
     "end_time": "2021-06-01T14:42:12.953194",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.915482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            elif self.dim == 3:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conscious-fruit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.017857Z",
     "iopub.status.busy": "2021-06-01T14:42:13.016649Z",
     "iopub.status.idle": "2021-06-01T14:42:13.019546Z",
     "shell.execute_reply": "2021-06-01T14:42:13.019156Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.122782Z"
    },
    "papermill": {
     "duration": 0.042902,
     "end_time": "2021-06-01T14:42:13.019647",
     "exception": false,
     "start_time": "2021-06-01T14:42:12.976745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PANNsCNN14Att(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        mel_bins = 64\n",
    "        self.interpolate_ratio = 32  # Downsampled ratio\n",
    "\n",
    "         \n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 32  # Downsampled ratio\n",
    "        sample_rate = 32000\n",
    "        window_size =  1024\n",
    "        hop_size = 320\n",
    "        mel_bins = 64\n",
    "        fmin = 50\n",
    "        fmax = 14000\n",
    "        classes_num = 264\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(\n",
    "            sr=sample_rate,\n",
    "            n_fft=window_size,\n",
    "            n_mels=mel_bins,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            ref=ref,\n",
    "            amin=amin,\n",
    "            top_db=top_db,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(\n",
    "            time_drop_width=64,\n",
    "            time_stripes_num=2,\n",
    "            freq_drop_width=8,\n",
    "            freq_stripes_num=2)\n",
    "        \n",
    "        # Spectrogram extractor\n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "        classes_num = 264\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def cnn_feature_extractor(self, x):\n",
    "        x = self.conv_block1(x, pool_size=(3, 3), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(3, 3), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(3, 3), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(3, 3), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(3, 3), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return x\n",
    "    \n",
    "   \n",
    "\n",
    "    def forward(self, x, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        # Output shape (batch size, channels, time, frequency)\n",
    "        x = x.transpose(2, 3) # (B, C, T, F)\n",
    "        x = self.cnn_feature_extractor(x)\n",
    "        \n",
    "        return x.transpose(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "olive-disability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.079608Z",
     "iopub.status.busy": "2021-06-01T14:42:13.079039Z",
     "iopub.status.idle": "2021-06-01T14:42:13.082107Z",
     "shell.execute_reply": "2021-06-01T14:42:13.081710Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.144362Z"
    },
    "papermill": {
     "duration": 0.038849,
     "end_time": "2021-06-01T14:42:13.082210",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.043361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    if \"resnest50-1s\" in name:\n",
    "        print('resnest1s')\n",
    "        model = resnest50_fast_1s1x64d()\n",
    "    elif 'resnest' in name:\n",
    "        print(\"resnest\")\n",
    "        model = resnest50()\n",
    "    elif 'CNN14' in name:\n",
    "        print('cnn14')\n",
    "        model = PANNsCNN14Att()\n",
    "    elif 'densenet121' in name:\n",
    "        print('dense121')\n",
    "        model = torchvision.models.densenet121()\n",
    "        return model\n",
    "    elif 'densenet169' in name:\n",
    "        print('dense169')\n",
    "        model = torchvision.models.densenet169()\n",
    "        return model\n",
    "    elif 'densenet201' in name:\n",
    "        print('dense201')\n",
    "        model = torchvision.models.densenet201()\n",
    "        return model\n",
    "    else:\n",
    "        print('resnext')\n",
    "        model = torchvision.models.resnext50_32x4d(pretrained=False)\n",
    "    if 'CNN14' not in name:\n",
    "        nb_ft = model.fc.in_features\n",
    "        del model.fc\n",
    "        num_cls = 397\n",
    "        model.fc = nn.Linear(nb_ft, num_cls)\n",
    "    model.conv_block1 = ConvBlock(in_channels = 3, out_channels = 64)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return model\n",
    "class FeatureExtractor(pl.LightningModule):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.model = load_model(self.model_name)\n",
    "        if self.model_name != 'CNN14' and 'densenet' not in self.model_name :\n",
    "            self.conv1 = self.model.conv1\n",
    "            self.bn1 = self.model.bn1\n",
    "            self.act1 = self.model.relu\n",
    "            self.maxpool = self.model.maxpool\n",
    "\n",
    "            self.layer1 = self.model.layer1\n",
    "            self.layer2 = self.model.layer2\n",
    "            self.layer3 = self.model.layer3\n",
    "            self.layer4 = self.model.layer4\n",
    "            #self.fc = self.model.fc\n",
    "            #self.global_Avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            del self.model\n",
    "    def forward(self, x):\n",
    "        if 'densenet' in self.model_name:\n",
    "            return self.model.features(x)\n",
    "        if self.model_name == 'CNN14':\n",
    "            return self.model(x)\n",
    "        x = self.maxpool(self.bn1(self.act1(self.conv1(x))))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "third-perry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.136451Z",
     "iopub.status.busy": "2021-06-01T14:42:13.135921Z",
     "iopub.status.idle": "2021-06-01T14:42:13.138999Z",
     "shell.execute_reply": "2021-06-01T14:42:13.138593Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.161521Z"
    },
    "papermill": {
     "duration": 0.032688,
     "end_time": "2021-06-01T14:42:13.139101",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.106413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    sed_model_path = [\n",
    "        #'../input/sedresnest/models/model_2.pth',\n",
    "        #'../input/cnn14-sed/models/model_1.pth',\n",
    "        '../input/sed1s-bird-clef/models/model_2.pth',\n",
    "        #'../input/densenetsed/models/model_1.pth'\n",
    "    ]\n",
    "    sed_model_names = [\n",
    "        #'resnest',\n",
    "        #'CNN14',\n",
    "        'resnest50-1s',\n",
    "        #'densenet121'\n",
    "    ]\n",
    "    \n",
    "    cls_model_path = [\n",
    "        '../input/birdcleftrained/models/model.pth',\n",
    "        '../input/birdclefbaselinefold1/models/model_1.pth',\n",
    "        '../input/birdclefbaselinefold3/models/model_3.pth',\n",
    "        '../input/resnextbirdclef/models/model_2.pth',\n",
    "        '../input/densenet121birdclef/models/model_1.pth',\n",
    "        '../input/densenet169birdclef/models/model_1.pth',\n",
    "        '../input/densenet201birdclef/models/model_4.pth'\n",
    "    ]\n",
    "    cls_model_names = [\n",
    "        'resnest',\n",
    "        'resnest',\n",
    "        'resnest',\n",
    "        'resnext',\n",
    "        'densenet121',\n",
    "        'densenet169',\n",
    "        'densenet201'\n",
    "    ]\n",
    "    num_classes = 397\n",
    "    sed_feature_extractor_dim = [\n",
    "    #    2048,\n",
    "    #    2048,\n",
    "        2048,\n",
    "    #    1024\n",
    "    ]\n",
    "    cls_feature_extractor_dim = [\n",
    "        2048,\n",
    "        2048,\n",
    "        2048,\n",
    "        2048,\n",
    "        1024,\n",
    "        1664,\n",
    "        1920\n",
    "    ]\n",
    "    transformer_dim = 768\n",
    "    num_heads = 8\n",
    "    \n",
    "    model_name = 'resnest'\n",
    "    dropout = 0.5\n",
    "    num_layers = 2\n",
    "    head = 'Conv2DAtt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sharing-anderson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.196732Z",
     "iopub.status.busy": "2021-06-01T14:42:13.195084Z",
     "iopub.status.idle": "2021-06-01T14:42:13.197328Z",
     "shell.execute_reply": "2021-06-01T14:42:13.197733Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.173899Z"
    },
    "papermill": {
     "duration": 0.034876,
     "end_time": "2021-06-01T14:42:13.197848",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.162972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionHead(pl.LightningModule):\n",
    "    # Heng's 2D attention Head, not sure if it will work or not.\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = feature_dim\n",
    "        self.num_classes = ModelConfig.num_classes + 1\n",
    "        self.attention = nn.Sequential(*[\n",
    "            nn.Conv2d(self.in_features, self.in_features * 2, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(self.in_features * 2),\n",
    "            nn.Tanh(),\n",
    "        ]) #use 8x1 to make location-aware convolution\n",
    "        self.attention1 = nn.Conv2d(self.in_features // 2, 1, kernel_size=1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.Linear =  nn.Linear(self.in_features, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, Freq, T = x.shape\n",
    "        a = self.attention(x).mean(dim = 2) #x is 4x9 feature map\n",
    "        a = a.reshape(B, self.in_features // 2, Freq, -1)\n",
    "        a = self.attention1(a)\n",
    "        a = F.softmax(a.reshape(B ,-1),-1).reshape(B, 1, Freq, -1)\n",
    "\n",
    "        x = (a * x + x) # (B, 2048)\n",
    "        x = torch.squeeze(self.global_avg_pool(x))\n",
    "        return self.Linear(x)[:, :-1] # Cut off the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "earned-occupation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.259490Z",
     "iopub.status.busy": "2021-06-01T14:42:13.258932Z",
     "iopub.status.idle": "2021-06-01T14:42:13.261981Z",
     "shell.execute_reply": "2021-06-01T14:42:13.261581Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.193619Z"
    },
    "papermill": {
     "duration": 0.040623,
     "end_time": "2021-06-01T14:42:13.262089",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.221466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEDAttention(pl.LightningModule):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        # SED Attention, for Clipwise and Framewise Preds\n",
    "        self.in_features =feature_dim\n",
    "        self.out_features = ModelConfig.num_classes\n",
    "        \n",
    "        self.framewise = nn.Conv1d(self.in_features, self.out_features, 1)\n",
    "        self.attention = nn.Conv1d(self.in_features, self.out_features, 1)\n",
    "        self.ratio = 32\n",
    "        self.duration = 600\n",
    "        self.seg_length = 5\n",
    "        self.num_clips = self.duration // self.seg_length\n",
    "        \n",
    "    def interpolate(self, x):\n",
    "        \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "        resolution reduction in downsampling of a CNN.\n",
    "\n",
    "        Args:\n",
    "          x: (batch_size, time_steps, classes_num)\n",
    "          ratio: int, ratio to interpolate\n",
    "        Returns:\n",
    "          upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "        \"\"\"\n",
    "        (batch_size, time_steps, classes_num) = x.shape\n",
    "        upsampled = x[:, :, None, :].repeat(1, 1, self.ratio, 1)\n",
    "        upsampled = upsampled.reshape(batch_size, time_steps * self.ratio, classes_num)\n",
    "        return upsampled\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention = F.softmax(self.attention(x), dim = -1).transpose(1, 2) # (B, L, C)\n",
    "        framewise = self.framewise(x).transpose(1, 2) # (B, L, C)\n",
    "    \n",
    "        # Compute the Clipwise\n",
    "        x = attention * framewise # (B, L, C)\n",
    "        # Sum over time\n",
    "        clipwise = x.sum(dim = 1) # (B, C)\n",
    "        # Convert Framewise to Segwise\n",
    "        framewise = self.interpolate(framewise) # (B, 32 * L, C)\n",
    "\n",
    "        B, L, C = framewise.shape\n",
    "        segwise = framewise.view(B, self.num_clips, L // self.num_clips, C)\n",
    "        segwise = segwise.transpose(0, 1) # (NumClips, B, 200, C)\n",
    "        # Compute the Sum\n",
    "        segwise = segwise.mean(dim = 2) # (N, B, C)\n",
    "        return segwise, clipwise\n",
    "        \n",
    "        \n",
    "    \n",
    "class SEDHead(pl.LightningModule):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_dim\n",
    "        self.n_mels = 128\n",
    "        self.dim_reduce = 32\n",
    "        self.drop_prob = ModelConfig.dropout\n",
    "        #self.Conv2d = nn.Conv2d(self.feature_extractor, self.feature_extractor, (self.n_mels // self.dim_reduce, 1))\n",
    "        self.drop1 = nn.Dropout(self.drop_prob)\n",
    "        self.drop2 = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(self.feature_extractor, self.feature_extractor) \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.attention = SEDAttention(self.feature_extractor)\n",
    "    def forward(self, x):\n",
    "        # X; Tensor(B, 2048, F, L):\n",
    "        x = torch.mean(x, dim=2)  # BS x nb_ft x t\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2  # BS x nb_ft x t\n",
    "        x = self.drop1(x) \n",
    "        x = x.transpose(1, 2) # (B, L, 2048) \n",
    "        # --------FC-------\n",
    "        x = self.relu(self.fc(x)) # (B, L, 2048)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.drop2(x) # (B, 2048, L)\n",
    "        # ---------Attention---------------\n",
    "        return self.attention(x) # (B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "joint-craft",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.324732Z",
     "iopub.status.busy": "2021-06-01T14:42:13.324161Z",
     "iopub.status.idle": "2021-06-01T14:42:13.328320Z",
     "shell.execute_reply": "2021-06-01T14:42:13.327870Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.215573Z"
    },
    "papermill": {
     "duration": 0.042775,
     "end_time": "2021-06-01T14:42:13.328426",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.285651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_logits(o):\n",
    "    desired_mean = 0.5\n",
    "    desired_std = 0.1\n",
    "    \n",
    "    mean = o.mean()\n",
    "    std = o.std()\n",
    "    \n",
    "    o = (o - mean) * (desired_std / std) + desired_mean\n",
    "    return o\n",
    "class FullModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, feature_dim, sed = True):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor(model_name)\n",
    "        self.head = SEDHead(feature_dim) if sed else AttentionHead(feature_dim)\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        head = self.head(features)\n",
    "        return head\n",
    "class TestingModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sed_model_path = ModelConfig.sed_model_path\n",
    "        self.sed_model_names = ModelConfig.sed_model_names\n",
    "        self.cls_model_path = ModelConfig.cls_model_path\n",
    "        self.cls_model_names = ModelConfig.cls_model_names\n",
    "        self.cls_feature = ModelConfig.cls_feature_extractor_dim\n",
    "        self.sed_feature = ModelConfig.sed_feature_extractor_dim\n",
    "        self.sed_model = nn.ModuleList([FullModel(self.sed_model_names[i], self.sed_feature[i]) for i in range(len(self.sed_model_path))])\n",
    "        for idx in range(len(self.sed_model_path)):\n",
    "            self.sed_model[idx].load_state_dict(torch.load(self.sed_model_path[idx], map_location = DEVICE))\n",
    "        \n",
    "        self.cls_model = nn.ModuleList([FullModel(self.cls_model_names[i], self.cls_feature[i], sed = False) for i in range(len(self.cls_model_path))])\n",
    "        for idx in range(len(self.cls_model_path)):\n",
    "            self.cls_model[idx].load_state_dict(torch.load(self.cls_model_path[idx], map_location = DEVICE))\n",
    "        \n",
    "    def forward(self, x, x2):\n",
    "        self.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            framewise_output = None\n",
    "            count =0\n",
    "            \n",
    "            for idx in range(len(self.sed_model)):\n",
    "                if framewise_output is None:\n",
    "                    framewise_output, _ = self.sed_model[idx](x)\n",
    "                    framewise_output = torch.sigmoid(torch.squeeze(framewise_output))\n",
    "                    #framewise_output = scale_logits(framewise_output)\n",
    "                else:\n",
    "                    f, _= self.sed_model[idx](x)    \n",
    "                    f = torch.sigmoid(f)\n",
    "                    f = torch.squeeze(f)\n",
    "                    #f = scale_logits(torch.squeeze(f))\n",
    "                    framewise_output = framewise_output + f\n",
    "                count += 1\n",
    "            for idx in range(len(self.cls_model)):\n",
    "                if framewise_output is None:\n",
    "                    f = torch.sigmoid(self.cls_model[idx](x2))\n",
    "                    #f = scale_logits(f)\n",
    "                    framewise_output = f\n",
    "                    count = 1\n",
    "\n",
    "                else:\n",
    "                    f = torch.sigmoid(self.cls_model[idx](x2))\n",
    "                    #f = scale_logits(f)\n",
    "                    framewise_output = framewise_output + f\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "            framewise_output =framewise_output / count\n",
    "            return framewise_output\n",
    "                    \n",
    "def get_model():\n",
    "    model = TestingModel()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-slope",
   "metadata": {
    "id": "BNsivZZZc_PG",
    "papermill": {
     "duration": 0.0246,
     "end_time": "2021-06-01T14:42:13.377481",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.352881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n",
    "- Sliding Window PPA \n",
    "- overlapping Window PPA?\n",
    "- Threshold -> Additive Averaging.\n",
    "- Post Process CSV instead of during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "elementary-special",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.431999Z",
     "iopub.status.busy": "2021-06-01T14:42:13.431349Z",
     "iopub.status.idle": "2021-06-01T14:42:13.435117Z",
     "shell.execute_reply": "2021-06-01T14:42:13.434693Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.235443Z"
    },
    "papermill": {
     "duration": 0.033908,
     "end_time": "2021-06-01T14:42:13.435218",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.401310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_thresh_preds(out, thresh=None, clipwise_thresh = None):\n",
    "    framewise = out  # (120, B, C), (B, C)\n",
    "    thresh = thresh or THRESH\n",
    "    # Use ClipWise to mask out framewise\n",
    "    #clipwise_threshed = clipwise >= clipwise_thresh # (B, C)\n",
    "    #framewise[:, clipwise_threshed] = 0.0 # mask out\n",
    "    \n",
    "    o = (-framewise).argsort(1) # (120, B, C)\n",
    "    all_npreds = (framewise >= thresh).sum(1) # (120, B)\n",
    "    \n",
    "    preds = []\n",
    "    for ooo, npreds in zip(o, all_npreds):\n",
    "        preds.append(ooo[:min(npreds, TOP_N)].cpu().numpy().tolist())\n",
    "       \n",
    "    return preds\n",
    "def get_bird_names(preds):\n",
    "    ex_names = []\n",
    "    for pred in preds:\n",
    "\n",
    "        if not pred:\n",
    "            ex_names.append(\"nocall\")\n",
    "        else:\n",
    "\n",
    "            ex_names.append(\" \".join([INV_LABEL_IDS[bird_id] for bird_id in pred]))\n",
    "    \n",
    "    return ex_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accompanied-stopping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.490524Z",
     "iopub.status.busy": "2021-06-01T14:42:13.488824Z",
     "iopub.status.idle": "2021-06-01T14:42:13.491128Z",
     "shell.execute_reply": "2021-06-01T14:42:13.491510Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.25172Z"
    },
    "papermill": {
     "duration": 0.032826,
     "end_time": "2021-06-01T14:42:13.491626",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.458800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(nets, test_data, names=True):\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(test_data))):\n",
    "            xb, xb2 = test_data[idx]\n",
    "            xb = xb.to(DEVICE).to(torch.float32)\n",
    "            xb2 = xb2.to(DEVICE).to(torch.float32)\n",
    "            if len(xb.shape) == 3:\n",
    "                xb = xb.unsqueeze(0)\n",
    "                \n",
    "            pred = nets[0](xb, xb2)\n",
    "            \n",
    "            if names:\n",
    "                pred = get_bird_names(get_thresh_preds(pred))\n",
    "            \n",
    "            preds.append(pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "above-criticism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:13.542821Z",
     "iopub.status.busy": "2021-06-01T14:42:13.542278Z",
     "iopub.status.idle": "2021-06-01T14:42:52.280802Z",
     "shell.execute_reply": "2021-06-01T14:42:52.279503Z",
     "shell.execute_reply.started": "2021-06-01T11:23:29.261735Z"
    },
    "papermill": {
     "duration": 38.76595,
     "end_time": "2021-06-01T14:42:52.280972",
     "exception": false,
     "start_time": "2021-06-01T14:42:13.515022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnest1s\n",
      "resnest\n",
      "resnest\n",
      "resnest\n",
      "resnext\n",
      "dense121\n",
      "dense169\n",
      "dense201\n"
     ]
    }
   ],
   "source": [
    "model = get_model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "registered-property",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:52.336881Z",
     "iopub.status.busy": "2021-06-01T14:42:52.336266Z",
     "iopub.status.idle": "2021-06-01T14:42:52.341561Z",
     "shell.execute_reply": "2021-06-01T14:42:52.341167Z",
     "shell.execute_reply.started": "2021-06-01T11:23:44.653426Z"
    },
    "papermill": {
     "duration": 0.034405,
     "end_time": "2021-06-01T14:42:52.341683",
     "exception": false,
     "start_time": "2021-06-01T14:42:52.307278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = BirdCLEFDataset(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "digital-joseph",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:42:52.397164Z",
     "iopub.status.busy": "2021-06-01T14:42:52.396633Z",
     "iopub.status.idle": "2021-06-01T14:44:51.872955Z",
     "shell.execute_reply": "2021-06-01T14:44:51.872490Z",
     "shell.execute_reply.started": "2021-06-01T11:23:44.661851Z"
    },
    "papermill": {
     "duration": 119.505361,
     "end_time": "2021-06-01T14:44:51.873085",
     "exception": false,
     "start_time": "2021-06-01T14:42:52.367724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facf7a18412b4a0a9ec327ced899569b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "pred_probas = predict([model], dataset, names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "roman-toddler",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:44:51.929991Z",
     "iopub.status.busy": "2021-06-01T14:44:51.929492Z",
     "iopub.status.idle": "2021-06-01T14:44:52.286398Z",
     "shell.execute_reply": "2021-06-01T14:44:52.285906Z",
     "shell.execute_reply.started": "2021-06-01T11:25:32.715358Z"
    },
    "papermill": {
     "duration": 0.386692,
     "end_time": "2021-06-01T14:44:52.286516",
     "exception": false,
     "start_time": "2021-06-01T14:44:51.899824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_string = [get_bird_names(get_thresh_preds(pred, thresh=THRESH)) for pred in pred_probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fuzzy-wonder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:44:52.345462Z",
     "iopub.status.busy": "2021-06-01T14:44:52.344875Z",
     "iopub.status.idle": "2021-06-01T14:44:52.348043Z",
     "shell.execute_reply": "2021-06-01T14:44:52.347630Z",
     "shell.execute_reply.started": "2021-06-01T11:25:33.131922Z"
    },
    "papermill": {
     "duration": 0.035261,
     "end_time": "2021-06-01T14:44:52.348146",
     "exception": false,
     "start_time": "2021-06-01T14:44:52.312885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preds_as_df(data, preds):\n",
    "    sub = {\n",
    "        \"row_id\": [],\n",
    "        \"birds\": [],\n",
    "    }\n",
    "    \n",
    "    for row, pred in zip(data.itertuples(False), preds):\n",
    "        row_id = [f\"{row.id}_{row.site}_{5*i}\" for i in range(1, len(pred)+1)]\n",
    "        sub[\"birds\"] += pred\n",
    "        sub[\"row_id\"] += row_id\n",
    "    \n",
    "    sub = pd.DataFrame(sub)\n",
    "    \n",
    "    if SAMPLE_SUB_PATH:\n",
    "        sample_sub = pd.read_csv(SAMPLE_SUB_PATH, usecols=[\"row_id\"])\n",
    "        sub = sample_sub.merge(sub, on=\"row_id\", how=\"left\")\n",
    "        sub[\"birds\"] = sub[\"birds\"].fillna(\"nocall\")\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "proved-invasion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-01T14:44:52.408097Z",
     "iopub.status.busy": "2021-06-01T14:44:52.407313Z",
     "iopub.status.idle": "2021-06-01T14:44:52.514011Z",
     "shell.execute_reply": "2021-06-01T14:44:52.513516Z",
     "shell.execute_reply.started": "2021-06-01T11:25:33.142437Z"
    },
    "papermill": {
     "duration": 0.139495,
     "end_time": "2021-06-01T14:44:52.514139",
     "exception": false,
     "start_time": "2021-06-01T14:44:52.374644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = preds_as_df(data, pred_string)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 226.028263,
   "end_time": "2021-06-01T14:44:53.651180",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-01T14:41:07.622917",
   "version": "2.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0ec0c23d7eb744aa8a1590ea8a2a128e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1d4a8161d5984168a868f678a2117ec8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_85637ddde95f433fae00a4055e3d1d95",
       "placeholder": "​",
       "style": "IPY_MODEL_2eef9c71421040819f2788e89a4decfb",
       "value": " 20/20 [01:59&lt;00:00,  6.01s/it]"
      }
     },
     "27d76625e1a749a39ed6bfafff4e54a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cf0155cc90254bc19ef7e0e783bcb330",
       "placeholder": "​",
       "style": "IPY_MODEL_d6e79f36ba7d4e79b2bfb675d6e60d04",
       "value": "100%"
      }
     },
     "2eef9c71421040819f2788e89a4decfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "62b704fb88c04e76aee16b333403e4b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f3a97fefd68a4fd19c130a65fd2a1b28",
       "max": 20.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0ec0c23d7eb744aa8a1590ea8a2a128e",
       "value": 20.0
      }
     },
     "85637ddde95f433fae00a4055e3d1d95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5ee751861b04c9eb84176848e8d2424": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf0155cc90254bc19ef7e0e783bcb330": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d6e79f36ba7d4e79b2bfb675d6e60d04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f3a97fefd68a4fd19c130a65fd2a1b28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "facf7a18412b4a0a9ec327ced899569b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_27d76625e1a749a39ed6bfafff4e54a8",
        "IPY_MODEL_62b704fb88c04e76aee16b333403e4b3",
        "IPY_MODEL_1d4a8161d5984168a868f678a2117ec8"
       ],
       "layout": "IPY_MODEL_c5ee751861b04c9eb84176848e8d2424"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
