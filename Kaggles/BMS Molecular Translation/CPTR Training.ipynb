{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# Installs\n\nprint(\"\\n... PIP INSTALLS COMPLETE ...\\n\")\n\n#!pip install -q --upgrade pip\n#!pip install -q git+https://github.com/qubvel/efficientnet.git\n#import efficientnet.keras as efn\nprint(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow_addons as tfa\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom glob import glob\nimport imageio\nimport random\nimport math\nimport time\nimport io\nimport os\nimport gc\nimport re\n\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport cv2\n\nAUTO = tf.data.experimental.AUTOTUNE\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n    \nprint(\"\\n... SEEDING FOR DETERMINISTIC BEHAVIOUR ...\\n\")\nseed_it_all()\n\nimport sys\nsys.path.append(\"/kaggle/input/experimental-efficientnetv2/automl\")\nsys.path.append(\"/kaggle/input/experimental-efficientnetv2/automl/brain_automl\")\nsys.path.append(\"/kaggle/input/experimental-efficientnetv2/automl/brain_automl/efficientnetv2\")\n\n# Google brain Imports\n\n# EfficientNet Module Imports\nimport brain_automl\nfrom brain_automl import efficientnetv2\nfrom efficientnetv2 import effnetv2_model\nfrom efficientnetv2 import effnetv2_configs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T23:09:58.809217Z","iopub.execute_input":"2021-05-22T23:09:58.809736Z","iopub.status.idle":"2021-05-22T23:10:05.02701Z","shell.execute_reply.started":"2021-05-22T23:09:58.809644Z","shell.execute_reply":"2021-05-22T23:10:05.026012Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n    tf.keras.mixed_precision.set_global_policy('mixed_bfloat16' if TPU else 'float32')\n    tf.config.optimizer.set_jit(True)\nexcept:\n    TPU = None\n    strategy = tf.distribute.get_strategy() \nN_REPLICAS = strategy.num_replicas_in_sync\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:05.028433Z","iopub.execute_input":"2021-05-22T23:10:05.028725Z","iopub.status.idle":"2021-05-22T23:10:10.942748Z","shell.execute_reply.started":"2021-05-22T23:10:05.028695Z","shell.execute_reply":"2021-05-22T23:10:10.941438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataModule:\n    base_dir = 'bmsdatasetlong'\n    DATA_DIR = KaggleDatasets().get_gcs_path(base_dir)\n    if not TPU:\n        DATA_DIR = f\"/kaggle/input/{base_dir}\"\n    TARGET_DTYPE = tf.bfloat16 if TPU else tf.float32\n    TOKEN_LIST = [\"<PAD>\", \"InChI=1S/\", \"<END>\", \"/c\", \"/h\", \"/m\", \"/t\", \"/b\", \"/s\", \"/i\"] +\\\n             ['Si', 'Br', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', 'C', 'H', 'B', ] +\\\n             [str(i) for i in range(167,-1,-1)] +\\\n             [\"\\+\", \"\\(\", \"\\)\", \"\\-\", \",\", \"D\", \"T\"]\n    # Get rid of Backslashs\n    for token_idx in range(len(TOKEN_LIST)):\n        TOKEN_LIST[token_idx] = TOKEN_LIST[token_idx].strip('\\\\')\n        \n    # The start/end/pad tokens will be removed from the string when computing the Levenshtein distance\n    # We want them as tf.constant's so they will operate properly within the @tf.function context\n    VOCAB_LEN = len(TOKEN_LIST)\n    START_TOKEN = tf.constant(TOKEN_LIST.index(\"InChI=1S/\"), dtype=tf.uint8)\n    END_TOKEN = tf.constant(TOKEN_LIST.index(\"<END>\"), dtype=tf.uint8)\n    PAD_TOKEN = tf.constant(TOKEN_LIST.index(\"<PAD>\"), dtype=tf.uint8)\n\n    # Prefixes and Their Respective Ordering/Format\n    #      -- ORDERING --> {c}{h/None}{b/None}{t/None}{m/None}{s/None}{i/None}{h/None}{t/None}{m/None}\n    PREFIX_ORDERING = \"chbtmsihtm\"\n    print(f\"\\n... PREFIX ORDERING IS {PREFIX_ORDERING} ...\")\n\n    # Paths to Respective Image Directories\n    TRAIN_DIR = os.path.join(DATA_DIR, \"train_records\")\n    VAL_DIR = os.path.join(DATA_DIR, \"val_records\")\n    TEST_DIR = os.path.join(DATA_DIR, \"test_records\")\n\n    # Get the Full Paths to The Individual TFRecord Files\n    TRAIN_TFREC_PATHS = sorted(\n        tf.io.gfile.glob(os.path.join(TRAIN_DIR, \"*.tfrec\")), \n        key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n    VAL_TFREC_PATHS = sorted(\n        tf.io.gfile.glob(os.path.join(VAL_DIR, \"*.tfrec\")), \n        key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n    TEST_TFREC_PATHS = sorted(\n        tf.io.gfile.glob(os.path.join(TEST_DIR, \"*.tfrec\")), \n        key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n\n   \n    # Paths to relevant CSV files containing training and submission information\n    TRAIN_CSV_PATH = os.path.join(\"/kaggle/input\", \"bms-molecular-translation\", \"train_labels.csv\")\n    SS_CSV_PATH    = os.path.join(\"/kaggle/input\", \"bms-molecular-translation\", \"sample_submission.csv\")\n    # When debug is true we use a smaller batch size and smaller model\n    DEBUG=False \n    \n    train_df = pd.read_csv(TRAIN_CSV_PATH)\n    ss_df    = pd.read_csv(SS_CSV_PATH)\n\n    # --- Distribution Information ---\n    N_EX    = len(train_df)\n    N_TEST  = len(ss_df)\n    N_VAL   = 80000 # Fixed from dataset creation information\n    N_TRAIN = N_EX-N_VAL\n    \n    BATCH_SIZE_DEBUG   = 2\n    REPLICA_BATCH_SIZE = 32 # Lower Batch Size -> Increase Model Capacity.\n    if DEBUG:\n        REPLICA_BATCH_SIZE = BATCH_SIZE_DEBUG\n    OVERALL_BATCH_SIZE = REPLICA_BATCH_SIZE*N_REPLICAS\n\n\n    # --- Input Image Information ---\n    IMAGE_SIZE = 384\n    IMG_SHAPE = (192,384,3)\n\n    # --- Autocalculate Training/Validation Information ---\n    TRAIN_STEPS = (N_TRAIN // OVERALL_BATCH_SIZE) + 1 \n    VAL_STEPS   = (N_VAL   // OVERALL_BATCH_SIZE) + 1\n    \n    # --- Modelling Information --\n    FEEDFORWARD_DIM = 2560 # Increase to 256\n    DECODER_DIM   = 640 # increase to 1024 later \n    \n    INPUT_LEN = 277 # 140\n    MAX_LEN = 140\n    seed = 7\n    \n    mean_train = np.array([0.485, 0.456, 0.406])#np.array([0.9871, 0.9871, 0.9871])\n    mean_train = np.expand_dims(np.expand_dims(mean_train, axis = 0), axis = 0)\n    std_train = np.array([0.229, 0.224, 0.225])#np.array([0.0888,0.0888,0.0888])\n    std_train = np.expand_dims(np.expand_dims(std_train, axis = 0), axis = 0)\n    stats = (mean_train, std_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:10.945709Z","iopub.execute_input":"2021-05-22T23:10:10.946249Z","iopub.status.idle":"2021-05-22T23:10:23.766591Z","shell.execute_reply.started":"2021-05-22T23:10:10.946198Z","shell.execute_reply":"2021-05-22T23:10:23.765392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingConfig:\n    NUM_EPOCHS = 6\n    PRINT_EVERY = 100\n    \n \n    label_smoothing = 0.1\n    weight_decay = 1e-6\n    \n    clip_grad = 10.\n    \n    TRAIN_STEPS = DataModule.TRAIN_STEPS\n    TOTAL_STEPS = TRAIN_STEPS * NUM_EPOCHS\n    WARM_STEPS = 0.1 # Increase to 0.25 Later.\n    PEAK_STEPS = 0.2 # 0.3 is ramp up, 0.7 is slowly ramp down. # Decrease to 0.1 later\n    \n    # --------First Run -------------------(Half, 6 EPOCHS_)\n    WARM_START_LR = 1e-6\n    PEAK_START_LR = 5e-4 # Manual Scaling when loading and reloading models.\n    FINAL_LR = 1e-5\n    # ---------Second Run ----------------(Half, 6 EPOCHS)\n    #WARM_START_LR = 1e-5\n    #PEAK_START_LR = 3e-5\n    #FINAL_LR = 1e-6\n    # ---------Third Run -----------------(Full Length, 4 EPOCHS)\n    #WARM_START_LR = 1e-7\n    #PEAK_START_LR = 5e-6\n    #FINAL_LR = 1e-7#  ABsolute Minimum LR before nothing learns.\n    # ---------Fourth Run ----------------(Full Length, 4 EPOCHS)\n    #WARM_START_LR = 1e-8\n    #PEAK_START+LR = 1e-7\n    #FINAL_LR = 1e-8\n    GRAD_ACCUMULATION = 1\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:23.76831Z","iopub.execute_input":"2021-05-22T23:10:23.768598Z","iopub.status.idle":"2021-05-22T23:10:23.775053Z","shell.execute_reply.started":"2021-05-22T23:10:23.768569Z","shell.execute_reply":"2021-05-22T23:10:23.774176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dropout(image, CT = 10, CT_WIDTH = 0.01):\n    # input - one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    DIM = cfg.IMAGE_SIZE\n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = random.randint(0, DIM - 1)\n        y = random.randint(0, DIM- 1)\n\n        # COMPUTE SQUARE \n        WIDTH = DIM * CT_WIDTH\n        ya = int(max(0,y-WIDTH//2))\n        yb = int(min(DIM,y+WIDTH//2))\n        xa = int(max(0,x-WIDTH//2))\n        xb = int(min(DIM, x+WIDTH//2))\n\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa, 3]) \n        three = image[ya:yb,xb:DIM,:]\n\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n\n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n    image = tf.reshape(image,[DIM,DIM,3])\n    return image\ndef transform(image):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = cfg.IMAGE_SIZE\n    XDIM = cfg.IMAGE_SIZE%2 #fix for size 331\n    shift = (0.0625 * DIM) * tf.random.uniform([2],dtype='float32', minval = -1, maxval = 1)\n    \n    rot = 360. * tf.random.uniform([1],dtype='float32', minval = -1, maxval = 1)\n    h_zoom = 1.0 + tf.random.uniform([1],dtype='float32', minval = 0, maxval = 0.5)/10.\n    w_zoom = 1.0 + tf.random.uniform([1],dtype='float32', minval = 0, maxval = 0.5)/10.\n    \n    # GET TRANSFORMATION MATRIX\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    m = tf.reshape( tf.concat([one/h_zoom, zero, zero, zero, one/w_zoom, zero, zero, zero, one],axis=0),[3,3])\n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM)\n    y = tf.tile( tf.range(-DIM//2, DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    trans = tf.transpose(idx3)\n    d = tf.gather_nd(image, trans)\n    image = tf.reshape(d,[DIM,DIM,3])\n    image = tfa.image.rotate(image, rot, fill_mode = 'reflect')\n    image = tfa.image.translate(image, shift)\n    return image\n@tf.function\ndef shift_scale_rotate(img):\n    return transform(img)\ndef normalize(image):\n    mean, std = DataModule.stats\n    image = image - mean\n    image = image / std\n    return image\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:23.776254Z","iopub.execute_input":"2021-05-22T23:10:23.776658Z","iopub.status.idle":"2021-05-22T23:10:23.804487Z","shell.execute_reply.started":"2021-05-22T23:10:23.776626Z","shell.execute_reply":"2021-05-22T23:10:23.802898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cut_off(x, y):\n    return x, y[:DataModule.MAX_LEN]\ndef load_image(one_sample, augment = True):\n    \n    feature_dict = {\n        'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=''),\n        'inchi': tf.io.FixedLenFeature(shape=[DataModule.INPUT_LEN], dtype=tf.int64, default_value=[0]*DataModule.INPUT_LEN)\n    }\n    features = tf.io.parse_single_example(one_sample, features=feature_dict)\n\n    image = tf.io.decode_png(features['image'], channels = 3)\n    GT = features['inchi']\n    GT = tf.cast(GT, tf.uint8)\n    image = tf.reshape(image, DataModule.IMG_SHAPE)\n    image = tf.cast(image, tf.float32) / 255.0\n    if augment == True:\n        pass # nothing yet\n        #if tf.random.uniform(()) < 0.5:\n        #    image = tf.image.flip_left_right(image)    \n        #if tf.random.uniform(()) < 0.5:\t        \n        #    image = tf.image.flip_up_down(image)\t      \n        #if tf.random.uniform(()) < 0.5:\n        #    # Transpose\n        #    image = tf.image.transpose(image)\n        #if tf.random.uniform(()) < 0.5:\n        #    image = tf.image.rot90(image) \n        #if tf.random.uniform(()) < 0.5:\n        #    image = shift_scale_rotate(image)\n    \n        #if tf.random.uniform(()) < 0.5:\n        #    image = tfa.image.gaussian_filter2d(image, filter_shape = (3, 7), sigma = (0.8, 1.4))\n        \n        #if tf.random.uniform(()) < 0.25:\n        #    image = dropout(image)\n    # Normalize\n    #image = normalize(image)\n    return tf.cast(image, DataModule.TARGET_DTYPE), GT","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:23.806279Z","iopub.execute_input":"2021-05-22T23:10:23.806699Z","iopub.status.idle":"2021-05-22T23:10:23.822759Z","shell.execute_reply.started":"2021-05-22T23:10:23.806654Z","shell.execute_reply":"2021-05-22T23:10:23.82177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset():\n    train_dataset = tf.data.TFRecordDataset(DataModule.TRAIN_TFREC_PATHS,\n     num_parallel_reads= AUTO\n    )\n    \n    options = tf.data.Options()\n    options.experimental_deterministic = False\n    \n    # Pseudo Labelled Data - 1616107\n    # Turned off augmentation\n    train_dataset = train_dataset.with_options(options)\n    train_dataset = train_dataset.map(lambda x: load_image(x, augment = True), num_parallel_calls = AUTO, deterministic = False)\n    train_dataset = train_dataset.map(lambda x, y: cut_off(x, y), num_parallel_calls = AUTO, deterministic = False)\n    \n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.shuffle(128, seed = DataModule.seed) # High Memory Consumption I think. I think it's tradeoff memory for performance:<\n    train_dataset = train_dataset.batch(DataModule.OVERALL_BATCH_SIZE,drop_remainder=True)\n    train_dataset = train_dataset.prefetch(AUTO)\n    \n    val_dataset = tf.data.TFRecordDataset(DataModule.VAL_TFREC_PATHS,\n        num_parallel_reads = AUTO\n    )\n    val_dataset = val_dataset.with_options(options)\n    val_dataset = val_dataset.map(lambda x: load_image(x, augment = False),  num_parallel_calls = AUTO, deterministic = False)\n    val_dataset = val_dataset.map(lambda x, y: cut_off(x, y), num_parallel_calls = AUTO, deterministic = False)\n    \n    val_dataset = val_dataset.batch(DataModule.OVERALL_BATCH_SIZE, drop_remainder = True)\n    val_dataset = val_dataset.prefetch(AUTO)\n    \n\n    return train_dataset, val_dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:23.823983Z","iopub.execute_input":"2021-05-22T23:10:23.824484Z","iopub.status.idle":"2021-05-22T23:10:23.842986Z","shell.execute_reply.started":"2021-05-22T23:10:23.824453Z","shell.execute_reply":"2021-05-22T23:10:23.841409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class eff_net_v2_model(keras.Model):\n    def __init__(self, model_name, enc_dim):\n        super().__init__()\n        self.eff_net_v2_model = effnetv2_model.EffNetV2Model(model_name=model_name, name = 'eff_net_v2_model')\n        #self.proj = keras.layers.Conv2D(enc_dim, 1, use_bias = False, kernel_initializer = 'he_uniform', activation = 'relu') # Proj Layer, as the EffNetV2's don't have them and would benefit greatly from them.\n        #self.bn = keras.layers.BatchNormalization()\n    def call(self, x, training, features_only = True):\n        features = self.eff_net_v2_model(x, training = training, features_only = features_only) # Scale features 0 \n        #return self.bn(self.proj(features, training = training), training = training)\n        return features","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:23.846475Z","iopub.execute_input":"2021-05-22T23:10:23.846853Z","iopub.status.idle":"2021-05-22T23:10:23.861861Z","shell.execute_reply.started":"2021-05-22T23:10:23.846821Z","shell.execute_reply":"2021-05-22T23:10:23.860456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_efficientnetv2_backbone(model_name, weight_path, enc_dim, include_top=False, input_shape=(192,384,3), pooling=None, weights=None):\n    # Load Complex Weights.\n    model = eff_net_v2_model(model_name, enc_dim) \n    model.build((None, *DataModule.IMG_SHAPE))\n    info = model.eff_net_v2_model._layers[-1]\n    model.eff_net_v2_model._layers.pop(-2)\n    model.eff_net_v2_model._layers += [info]\n    if weight_path is not None:\n        checkpoint = tf.train.Checkpoint(net = model)\n        status = checkpoint.restore(tf.train.latest_checkpoint(weight_path))\n        print(\"Loaded Encoder Weights\")\n    #status.assert_existing_objects_matched() # Just Check that everything worked.\n    \n    return model\n    \nclass ModelConfig:\n    # ENCODER_CONFIG\n    enc_dim = 640 # Controls the Dimension of the Enc Dim\n    enc_dec_drop = 0.0 # Dropout Between Encoder and Decoder\n    transformer_drop = 0.2\n    final_drop = 0.5\n    num_encoder_layers = 4\n    num_decoder_layers = 6 # 6 layers is the max(Official Transformer uses 6.) - Becomes Exponentially Slow.\n    num_att_heads = 12 # Standard Transformer Config.\n    # CHANGE TO EFFICIENTNET V2. - Medium or Large Model(XL is too many params)\n    model_name = 'efficientnetv2-l' # EfficientNetV2 Small(Will scale to Large) - Mostly for model capacity, not for weights(NS is better.)\n    base_dir = 'efficientnet-v2-weights'\n    GCS_PATH = KaggleDatasets().get_gcs_path(base_dir) if TPU else '../input/efficientnet-v2-weights'\n    print(GCS_PATH)\n    weight_path = f'{GCS_PATH}/efficientnetv2-l-21k/efficientnetv2-l-21k/'\n    BB_FN = get_efficientnetv2_backbone\n    PREPROCESSING_FN = tf.keras.applications.efficientnet.preprocess_input\n    FREEZE_ENCODER = False\n    \n    tmp_model = BB_FN(model_name, weight_path,enc_dim, include_top=False, input_shape=DataModule.IMG_SHAPE)\n    IMG_EMB_DIM = tmp_model(tf.ones((DataModule.BATCH_SIZE_DEBUG, *DataModule.IMG_SHAPE)), features_only = True).shape[1:]\n    IMG_EMB_DIM = (IMG_EMB_DIM[0]*IMG_EMB_DIM[1], IMG_EMB_DIM[2])\n    \n    BaseSavePath = './save/'\n    PrevModelPath = None#'../input/halftrainedtransformer/save/'\n    try:\n        os.mkdir(BaseSavePath)\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:23.864317Z","iopub.execute_input":"2021-05-22T23:10:23.864658Z","iopub.status.idle":"2021-05-22T23:10:52.19925Z","shell.execute_reply.started":"2021-05-22T23:10:23.864626Z","shell.execute_reply":"2021-05-22T23:10:52.198379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoder Model","metadata":{}},{"cell_type":"code","source":"print(\"\\n... ENCODER MODEL CREATION STARTING ...\\n\")\n\n# SAMPLE IMAGES\ntrain_ds, val_ds = load_dataset()\nval_ds = val_ds.unbatch().batch(DataModule.BATCH_SIZE_DEBUG)\nSAMPLE_IMGS, SAMPLE_LBLS = next(iter(val_ds))\n    \nclass CNNEncoder(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        \n        self.image_embedding_dim = ModelConfig.IMG_EMB_DIM\n        self.preprocessing_fn = ModelConfig.PREPROCESSING_FN\n        self.model_name = ModelConfig.model_name\n        self.weight_path = ModelConfig.weight_path\n        self.enc_dim = ModelConfig.enc_dim\n        self.backbone_fn = ModelConfig.BB_FN\n        self.img_shape = DataModule.IMG_SHAPE\n        self.encoder_backbone = self.backbone_fn(self.model_name, self.weight_path, self.enc_dim, include_top=False, weights=None, input_shape=self.img_shape)\n        self.dropout = ModelConfig.enc_dec_drop\n        self.spat_drop = keras.layers.SpatialDropout2D(rate = self.dropout)\n        self.reshape = tf.keras.layers.Reshape(self.image_embedding_dim, name='image_embedding')\n    def call(self, x, training):\n        \"\"\" TODO\n        \n        Args:\n            TODO        \n        \n        Returns:\n            TODO\n        \"\"\"\n        x = self.preprocessing_fn(x)\n        x = self.encoder_backbone(x, training=training)\n        x = self.spat_drop(x, training = training)\n        x = self.reshape(x, training=training)\n        return x\n    \n    \n# Example enoder output\nwith tf.device('/CPU:0'):\n    encoder = CNNEncoder()\n    img_embedding_batch = encoder(tf.cast(SAMPLE_IMGS, tf.float32))\nprint(f'\\n... Encoder Output Shape  :  (batch_size, embedding_length, embedding_depth)  :  {img_embedding_batch.shape} ...\\n')\n\nprint(\"\\n... ENCODER MODEL CREATION FINISHED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:10:52.200703Z","iopub.execute_input":"2021-05-22T23:10:52.201239Z","iopub.status.idle":"2021-05-22T23:11:04.566229Z","shell.execute_reply.started":"2021-05-22T23:10:52.201205Z","shell.execute_reply":"2021-05-22T23:11:04.564799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vit Transformer Head Encoder:\n- i.E Hybrid CNN -> Vit Model(\n- EffNetV2 L -> 4 Layer Vit","metadata":{}},{"cell_type":"code","source":"class EncoderMultiHeadAttention(keras.layers.Layer):\n    def __init__(self, encoder_dim, num_heads, drop_prob = 0.1):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.num_heads = num_heads\n        self.drop_prob = drop_prob\n        \n        self.layer_norm1 = keras.layers.LayerNormalization(epsilon = 1e-6)\n        self.MAH = keras.layers.MultiHeadAttention(self.num_heads, self.encoder_dim // self.num_heads)\n        self.layer_norm2 = keras.layers.LayerNormalization(epsilon = 1e-6)\n        self.Linear = keras.layers.Dense(self.encoder_dim)\n    def __call__(self, x, training):\n        norm = self.layer_norm1(x, training = training)\n        mah = self.MAH(key = norm, value = norm, query = norm, training = training) + x\n        \n        norm2 = self.layer_norm2(mah, training = training)\n        linear = self.Linear(norm2, training = training) + mah\n        return linear\nclass TransformerEncoder(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.num_layers = ModelConfig.num_encoder_layers\n        self.num_heads = ModelConfig.num_att_heads\n        self.enc_dim = ModelConfig.IMG_EMB_DIM[-1]# Features from the CNN encoder\n        self.drop_prob = ModelConfig.enc_dec_drop\n        self.length_input = ModelConfig.IMG_EMB_DIM[0] # Length of the input\n\n        self.initializer = keras.initializers.glorot_uniform(seed = 42)\n        self.pos_enc = tf.Variable(self.initializer(shape = (1, self.length_input, self.enc_dim)), trainable = True)\n        self.encoders = [\n            EncoderMultiHeadAttention(self.enc_dim, self.num_heads, drop_prob = self.drop_prob) for i in range(self.num_layers)\n        ]\n    def call(self, x, training):\n        x = x + tf.cast(self.pos_enc, DataModule.TARGET_DTYPE) # Should Broadcast across Batch\n        for encoder in self.encoders:\n            x = encoder(x, training = training)\n        return x\nclass Encoder(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.CNN = CNNEncoder()\n        self.transformer = TransformerEncoder()\n    def call(self, x, training):\n        cnn = self.CNN(x, training = training)\n        return self.transformer(cnn, training = training)\nwith tf.device(\"CPU: 0\"):\n    encoder = Encoder()\n    img_embedding_batch = encoder(tf.cast(SAMPLE_IMGS, tf.float32))\n    print(img_embedding_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:11:04.567899Z","iopub.execute_input":"2021-05-22T23:11:04.568349Z","iopub.status.idle":"2021-05-22T23:11:14.92156Z","shell.execute_reply.started":"2021-05-22T23:11:04.568307Z","shell.execute_reply":"2021-05-22T23:11:14.919578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformer Decoder","metadata":{}},{"cell_type":"code","source":"class DecoderMultiHeadAttention(keras.layers.Layer):\n    def __init__(self, encoder_features, decoder_features, num_heads, drop_prob = 0.1):\n        super().__init__()\n        self.encoder_features = encoder_features\n        self.decoder_features = decoder_features\n        self.drop_prob = drop_prob\n        self.num_heads = num_heads\n        \n        self.MAH = keras.layers.MultiHeadAttention(self.num_heads, self.encoder_features // self.num_heads, value_dim = self.decoder_features // self.num_heads)\n        self.Dropout = keras.layers.Dropout(rate = self.drop_prob)\n        self.layernorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    def __call__(self, encoder_features, decoder_features, attention_mask, training):\n        query = decoder_features # (B, T, Dim) \n        key = encoder_features\n        value = encoder_features # (B, S, Dim) \n\n        attended = self.MAH(query = query, key = key, value = value, attention_mask = attention_mask,  training = training)\n        return self.layernorm(self.Dropout(attended, training = training) + decoder_features, training = training)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:11:14.922913Z","iopub.execute_input":"2021-05-22T23:11:14.923282Z","iopub.status.idle":"2021-05-22T23:11:14.933173Z","shell.execute_reply.started":"2021-05-22T23:11:14.923252Z","shell.execute_reply":"2021-05-22T23:11:14.932252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, encoder_features, decoder_features, num_heads, feedforward_dim, drop_prob = 0.1):\n        super().__init__()\n        self.encoder_features = encoder_features\n        self.decoder_features = decoder_features\n        self.feedforward_dim = feedforward_dim\n        \n        self.num_heads = num_heads\n        self.drop_prob = drop_prob\n        \n        self.MAH1 = DecoderMultiHeadAttention(self.decoder_features, self.decoder_features, self.num_heads, drop_prob = self.drop_prob)\n        self.MAH2 = DecoderMultiHeadAttention(self.encoder_features, self.decoder_features, self.num_heads, drop_prob = self.drop_prob)\n        \n        self.FFN = keras.Sequential([\n            keras.layers.Dense(self.feedforward_dim, activation = 'relu'),\n            keras.layers.Dense(self.decoder_features)\n        ])\n        self.Dropout = keras.layers.Dropout(rate = self.drop_prob)\n        self.layernorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    def __call__(self, encoder_features, decoder_features, attention_mask, padding_mask, training):\n        attention1 = self.MAH1(decoder_features, decoder_features, attention_mask, training = training)\n        \n        attention2 = self.MAH2(encoder_features, attention1, padding_mask, training = training)\n        \n        FFN = self.layernorm(self.Dropout(self.FFN(attention2, training = training), training = training) + attention2, training = training)\n        \n        return FFN\nclass FullTransformerDecoder(keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        # Extract Settings from Config\n        self.encoder_dim = ModelConfig.IMG_EMB_DIM[-1]\n        self.decoder_dim = DataModule.DECODER_DIM\n        self.feedforward_dim = DataModule.FEEDFORWARD_DIM\n        self.drop_prob = ModelConfig.transformer_drop\n        self.num_layers = ModelConfig.num_decoder_layers\n        self.num_heads = ModelConfig.num_att_heads\n        \n        self.DecoderLayer = [\n            TransformerDecoder(self.encoder_dim, self.decoder_dim, self.num_heads, self.feedforward_dim, drop_prob = self.drop_prob) for i in range(self.num_layers)\n        ]\n    \n    def __call__(self, encoder_features, decoder_features, attention_mask, padding_mask, training):\n        cur_decoded = decoder_features\n        for decoder in self.DecoderLayer:\n            cur_decoded = decoder(encoder_features, cur_decoded, attention_mask, padding_mask, training = training)\n        return cur_decoded\nclass Decoder(keras.Model):\n    # Full Decoder Model\n    def positional_embeddings(self, max_len, dim):\n        pos_enc = np.zeros((1, max_len, dim), dtype = np.float32)\n        for L in range(max_len):\n            for i in range(0, dim + 2, 2):\n                if i >= dim:\n                    break\n                pos_enc[:, L, i] = math.sin(L / 10000 ** (i / dim))\n                if i + 1 >= dim:\n                    break\n                pos_enc[:, L, i + 1] = math.cos(L / 10000 ** ((i + 1) / dim))\n\n        return tf.cast(tf.identity(pos_enc), DataModule.TARGET_DTYPE)\n    \n    def __init__(self):\n        super().__init__()\n        # Extract Features from Config \n        self.vocab_len = DataModule.VOCAB_LEN\n        self.decoder_dim = DataModule.DECODER_DIM\n        self.final_drop = ModelConfig.final_drop\n        self.max_len = DataModule.MAX_LEN\n        \n        self.embedding = keras.layers.Embedding(self.vocab_len, self.decoder_dim)\n        self.decoder_transformer = FullTransformerDecoder()\n        \n        self.Dropout = keras.layers.Dropout(rate = self.final_drop)\n        self.Linear = keras.layers.Dense(self.vocab_len)\n        self.layer = keras.layers.Layer(dtype = tf.float32)\n        \n        self.pos_enc = self.positional_embeddings(self.max_len, self.decoder_dim)\n    \n    def mask_pad(self, input_tokens, dtype):\n        # From Padidng Tokens, generates a Padding Mask(To Mimic Inference Time)\n        # Input_tokens: Tensor(B, L)\n        pad_tok = DataModule.PAD_TOKEN\n        is_pad = tf.equal(input_tokens, pad_tok) # (B, L)\n        return tf.cast(1 - tf.cast(tf.expand_dims(is_pad, axis = -1), dtype = tf.uint8),dtype) # (B, L, 1)\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n    def call(self, encoder_features, decoder_token, training):\n        # Extract Embeddings\n        # Encoder features: Tensor(B, L, C)\n        # Decoder Token: tensor(B, L, 1)      \n        B, L = decoder_token.shape\n        C = self.decoder_dim\n        _, Enc_L, _ = encoder_features.shape\n        \n        attention_mask = self.causal_attention_mask(B, L, L, tf.uint8) # (B, L, L)\n        padding_mask = self.mask_pad(decoder_token, tf.uint8) # (B, L, 1) \n        \n        # Add to the Attention Mask and Also Add Pad Mask\n        attention_mask = attention_mask * padding_mask\n        decoder_embeddings = self.embedding(decoder_token, training = training) # (B, L, C)\n        # Stretch the Padding Mask across encoder dims\n        padding_mask = tf.repeat(padding_mask, Enc_L, axis = -1)\n        # Stretch Positional Encodings Across the Batch\n        pos_enc = tf.repeat(self.pos_enc, B, axis = 0)[:, :L, :]\n        # Add Decoder Pos Enc\n        decoder_embeddings = decoder_embeddings + pos_enc\n        # Convert Masks to Bool\n        \n        attention_mask = tf.cast(attention_mask, tf.bool)\n        padding_mask = tf.cast(padding_mask, tf.bool) \n    \n        # Decode\n        decoded = self.decoder_transformer(encoder_features, decoder_embeddings, attention_mask, padding_mask, training = training) # (B, L, C)\n        # Get Element wise Predictions\n        dropped = self.Dropout(decoded, training = training)\n        return self.layer(self.Linear(dropped, training = training)) # (B, NumClasses)\n# Create Sample Decoder\nwith tf.device(\"CPU:0\"):\n    decoder = Decoder()\n    input_val = tf.ones((DataModule.BATCH_SIZE_DEBUG, DataModule.MAX_LEN), dtype = tf.uint8) * DataModule.PAD_TOKEN\n    import time\n    cur_time = time.time()\n    pred_output = decoder(img_embedding_batch, input_val, training = False)\n    print(pred_output.shape)\n    print(time.time() - cur_time)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:11:14.934525Z","iopub.execute_input":"2021-05-22T23:11:14.934998Z","iopub.status.idle":"2021-05-22T23:11:16.81453Z","shell.execute_reply.started":"2021-05-22T23:11:14.934953Z","shell.execute_reply":"2021-05-22T23:11:16.813446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Accumulation Adam","metadata":{}},{"cell_type":"code","source":"class GradAccAdam():\n    # Just a Wrapper to Accumulate Gradients and Send them to Adam\n    def __init__(self, encoder, decoder, learning_rate, grad_acc_steps):\n        self.learning_rate = learning_rate\n        self.grad_acc_steps = grad_acc_steps\n        \n        self.weight_decay = TrainingConfig.weight_decay\n        self.optimizer = tfa.optimizers.AdamW(learning_rate = self.learning_rate, weight_decay = self.weight_decay)\n        \n        self.PrevModelPath = ModelConfig.PrevModelPath\n        if self.PrevModelPath:\n            self.opt_weights = np.load(f'{self.PrevModelPath}optimizer_last.npy', allow_pickle = True)\n        \n            trainable_weights = encoder.trainable_weights + decoder.trainable_weights\n            \n            zero_grads = [tf.zeros_like(w) for w in trainable_weights]\n            @tf.function\n            def f():\n                self.optimizer.apply_gradients(zip(zero_grads, trainable_weights))\n            strategy.run(f)\n            self.optimizer.set_weights(self.opt_weights)\n            print(\"Loaded Weights\")\n        \n        self.gradients = None\n        self.cur_grad_acc = 0\n    def apply_gradients(self, gradients, variables):\n        if self.gradients is None:\n            self.gradients = [g / tf.constant(float(self.grad_acc_steps)) for g in gradients]\n            self.cur_grad_acc += 1\n        else:\n            for i in range(len(gradients)):\n                self.gradients[i] += gradients[i] / tf.constant(float(self.grad_acc_steps))\n            self.cur_grad_acc += 1\n        if self.cur_grad_acc == self.grad_acc_steps:\n            self.optimizer.apply_gradients(zip(self.gradients, variables))\n            self.gradients = None\n            self.cur_grad_acc = 0","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:11:16.815945Z","iopub.execute_input":"2021-05-22T23:11:16.816284Z","iopub.status.idle":"2021-05-22T23:11:16.829771Z","shell.execute_reply.started":"2021-05-22T23:11:16.816251Z","shell.execute_reply":"2021-05-22T23:11:16.827923Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lr Scheduler\n","metadata":{}},{"cell_type":"code","source":"class ParamScheduler:\n    def __init__(self, start, end, num_iter):\n        self.start = start\n        self.end = end\n        self.num_iter = num_iter\n        self.idx = -1\n        \n        \n    def step(self):\n        self.idx+=1\n        return self.func(self.start, self.end, self.idx/self.num_iter)\n    \n    def reset(self):\n        self.idx=-1\n        \n    def is_complete(self):\n        return self.idx >= self.num_iter\n\nclass CosineScheduler(ParamScheduler):\n    def func(self, start_val, end_val, pct):\n        cos_out = np.cos(np.pi * pct) + 1\n        return end_val + (start_val - end_val)/2 * cos_out\nclass ConstantScheduler(ParamScheduler):\n    def __init__(self, init_lr, num_steps):\n        self.init_lr = init_lr\n        self.num_steps = num_steps\n        self.steps = -1\n    def step(self):\n        self.steps += 1\n        return self.init_lr\n    def reset(self):\n        self.steps = -1\n    def is_complete(self):\n        return self.steps >= self.num_steps\nclass OneCycleScheduler(keras.callbacks.Callback):\n    \n    def __init__(self, init_lr, max_lr, min_lr, warm_steps, peak_steps, total_steps):\n        momentums=(0.95,0.85)\n        start_div=25.\n        pct_start=warm_steps\n        pct_climax = peak_steps# Stay at the peak for 0.1 of training.\n        verbose=True\n        sched=CosineScheduler\n        end_div=None\n        self.pct_climax = pct_climax\n        self.max_lr, self.momentums, self.start_div, self.pct_start, self.verbose, self.sched, self.end_div = max_lr, momentums, start_div, pct_start, verbose, sched, end_div\n        if self.end_div is None:\n            self.end_div = start_div * 1e4\n        self.logs = {}\n        self.min_lr = min_lr\n        self.init_lr = init_lr\n  \n        self.start_lr = self.max_lr/self.start_div\n        self.end_lr = self.max_lr/self.end_div \n        self.num_iter = int(total_steps * 1.2) # Pad the Steps a bit to make sure no overflow.\n        self.num_iter_1 = int(self.pct_start*self.num_iter)\n        self.num_iter_2 = int(self.pct_climax * self.num_iter)\n        self.num_iter_3 = self.num_iter - self.num_iter_1 - self.num_iter_2\n        \n        self.lr_scheds = (self.sched(self.start_lr, self.max_lr, self.num_iter_1), ConstantScheduler(self.max_lr, self.num_iter_2), self.sched(self.max_lr, self.end_lr, self.num_iter_3))\n        self.sched_idx = 0 \n        \n    def optimizer_params_step(self):\n        next_lr = self.lr_scheds[self.sched_idx].step()\n        next_lr = tf.maximum(next_lr, self.min_lr)\n        next_lr = tf.cast(next_lr, tf.float32)\n        # update optimizer params\n        optimizer.optimizer.learning_rate.assign(next_lr)\n        \n    def step(self):\n        self.optimizer_params_step()\n        if self.lr_scheds[self.sched_idx].is_complete():\n            self.sched_idx += 1","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:11:16.831448Z","iopub.execute_input":"2021-05-22T23:11:16.831831Z","iopub.status.idle":"2021-05-22T23:11:16.853944Z","shell.execute_reply.started":"2021-05-22T23:11:16.831797Z","shell.execute_reply":"2021-05-22T23:11:16.852906Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Config","metadata":{}},{"cell_type":"code","source":"class Config():\n    def __init__(self,):\n        self.lr_config = {}\n    \n    def initialize_lr_config(self, total_steps, init_lr, warm_steps, max_lr, min_lr, peak_steps):\n        self.lr_config = dict(\n            total_steps=total_steps, \n            init_lr=init_lr, \n            warm_steps=warm_steps, \n            max_lr = max_lr, \n            min_lr=min_lr,\n            peak_steps = peak_steps\n        )\n        \ntraining_config = Config()\n\ntraining_config.initialize_lr_config(total_steps=TrainingConfig.TOTAL_STEPS, \n                                     init_lr =TrainingConfig.WARM_START_LR, \n                                     warm_steps = TrainingConfig.WARM_STEPS,\n                                     peak_steps = TrainingConfig.PEAK_STEPS,\n                                     max_lr=TrainingConfig.PEAK_START_LR, \n                                     min_lr=TrainingConfig.FINAL_LR)\n\nprint(f\"TRAINING LEARNING RATE CONFIG:\\n\\t--> {training_config.lr_config}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:11:16.855302Z","iopub.execute_input":"2021-05-22T23:11:16.855739Z","iopub.status.idle":"2021-05-22T23:11:16.871046Z","shell.execute_reply.started":"2021-05-22T23:11:16.8557Z","shell.execute_reply":"2021-05-22T23:11:16.870352Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_for_training():\n    \"\"\" Declare required objects under TPU session scope and return ready for training\n    \n    Args:\n        lr_config (dict): Keyword arguments mapped to desired values for lr schedule function\n        encoder_config (dict): Keyword arguments mapped to desired values for encoder model instantiation\n        decoder_config (dict): Keyword arguments mapped to desired values for decoder model instantiation    \n        encoder_wts (str, optional): Path to pretrained model weights for encoder\n        decoder_wts (str, optional): Path to pretrained model weights for decoder\n        verbose (bool, optional): Whether or not to print model information and plot lr schedule\n        \n    Returns:\n        loss_fn - TBD\n        metrics - TBD\n        optimizer - TBD\n        lr_scheduler - TBD\n        encoder - TBD\n        decoder - TBD\n        \n    \"\"\"\n    \n\n    # Everything must be declared within the scope when leveraging the TPU strategy\n    #     - This will still function properly if scope is set to another type of accelerator\n    with strategy.scope():\n        \n        print(\"\\t--> CREATING LOSS FUNCTION ...\")\n        # Declare the loss object\n        #     - Sparse categorical cross entropy loss is used as root loss\n        label_smoothing = TrainingConfig.label_smoothing\n        loss_object = tf.keras.losses.CategoricalCrossentropy(\n            from_logits=True, reduction=tf.keras.losses.Reduction.NONE, label_smoothing = label_smoothing\n        )\n        \n        def loss_fn(real, pred):\n            # Convert to uint8\n            # Real: Tensor(B, L, C) \n            # PRed: Tensor(B, L, C) \n            \n            num_classes = DataModule.VOCAB_LEN\n            mask = tf.math.not_equal(real, 0)\n            loss_ = loss_object(tf.one_hot(real, num_classes), pred)\n            loss_ *= tf.cast(mask, dtype=loss_.dtype)\n\n            # https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\n            loss_ = tf.nn.compute_average_loss(loss_, global_batch_size= DataModule.REPLICA_BATCH_SIZE)\n            return loss_\n        \n        \n        print(\"\\t--> CREATING METRICS ...\")\n        metrics = {\n            'train_loss': tf.keras.metrics.Mean(),\n            'train_acc': tf.keras.metrics.SparseCategoricalAccuracy(),\n            'val_loss': tf.keras.metrics.Mean(),\n            'val_acc': tf.keras.metrics.SparseCategoricalAccuracy(),\n            'val_lsd': tf.keras.metrics.Mean(), \n        }\n        \n        \n        # Instiate an optimizer\n        print(\"\\t--> CREATING ENCODER MODEL ARCHITECTURE ...\")\n        encoder = Encoder()\n        initialization_batch = encoder(\n            tf.ones(((DataModule.REPLICA_BATCH_SIZE,)+DataModule.IMG_SHAPE), dtype=DataModule.TARGET_DTYPE), \n            training=False,\n        )\n        print(\"\\t--> CREATING DECODER MODEL ARCHITECTURE...\")\n        decoder = Decoder()\n        pred_output = decoder(\n            initialization_batch,\n            tf.identity(np.random.randint(0, DataModule.VOCAB_LEN, size = (DataModule.REPLICA_BATCH_SIZE, DataModule.MAX_LEN), dtype = np.uint8)), \n            training=False\n        )\n    \n        print(\"\\t--> CREATING OPTIMIZER ...\")\n        optimizer = GradAccAdam(encoder, decoder, learning_rate = TrainingConfig.WARM_START_LR, grad_acc_steps = TrainingConfig.GRAD_ACCUMULATION)\n        print(\"\\t--> CREATING LEARNING RATE SCHEDULER ...\")\n        # Declare the learning rate schedule (try this as actual lr schedule and list...)\n        lr_scheduler = OneCycleScheduler(**training_config.lr_config)\n        \n        # Instantiate the encoder model \n        \n        \n        PrevModelPath = ModelConfig.PrevModelPath\n        if PrevModelPath is not None:\n            encoder.load_weights(f\"{PrevModelPath}enc_last.h5\")\n            if ModelConfig.FREEZE_ENCODER:\n                encoder.trainable = False\n            print(\"Loaded Encoder\")\n        # Instantiate the decoder model\n        \n        \n        if PrevModelPath is not None:\n            decoder.load_weights(f\"{PrevModelPath}dec_last.h5\")\n            print(\"Loaded Decoder\")\n       \n  \n    return loss_fn, metrics, optimizer, lr_scheduler, encoder, decoder\n    \n    \nprint(\"\\n... GENERATING THE FOLLOWING:\")\n# Instantiate our required training components in the correct scope\nloss_fn, metrics, optimizer, lr_scheduler, encoder, decoder = \\\n    prepare_for_training()\n\nprint(\"\\n... TRAINING PREPERATION FINISHED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:13:02.497312Z","iopub.execute_input":"2021-05-22T23:13:02.497719Z","iopub.status.idle":"2021-05-22T23:14:36.85773Z","shell.execute_reply.started":"2021-05-22T23:13:02.497686Z","shell.execute_reply":"2021-05-22T23:14:36.855992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-\"*100) # Just for printing, Summed Loss Should be 14 with BS 64\n\nloss = loss_fn(SAMPLE_LBLS[:, :], pred_output)\nprint(f\"\\n\\n... AVERAGE LOSS ACROSS BATCH:\\n\\t--> {loss}\")\nprint(\"-\"*100) # Just for printing","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:14:36.859576Z","iopub.execute_input":"2021-05-22T23:14:36.859944Z","iopub.status.idle":"2021-05-22T23:14:41.252125Z","shell.execute_reply.started":"2021-05-22T23:14:36.859911Z","shell.execute_reply":"2021-05-22T23:14:41.251019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Training","metadata":{}},{"cell_type":"code","source":"def train_step(_image_batch, _inchi_batch):\n    \"\"\" Forward pass (calculate gradients)\n    \n    Args:\n        _image_batch (): TBD\n        _inchi_batch (): TBD\n    \n    Returns:\n        tbd\n    \"\"\"\n    \n    batch_loss = tf.constant(0.0, tf.float32)   \n    with tf.GradientTape() as tape:\n        # image_batch_embedding has shape --> (REPLICA_BATCH_SIZE, IMG_EMB_DIM)\n        image_batch_embedding = encoder(_image_batch, training=True)\n        # Image_Batch_Embedding: Tensor(B, L, C) \n        input_vals = _inchi_batch[:, :-1] # (B, L - 1) \n        GT = _inchi_batch[:, 1:] # (B, L - 1) \n        # Predict\n        pred = decoder(image_batch_embedding, input_vals, training = True) # (B, L - 1, C)\n        # Compute Loss\n        batch_loss = loss_fn(GT, pred) # (1)\n        # Compute Accuracy\n        metrics[\"train_acc\"].update_state(GT, pred, \n                                              sample_weight=tf.where(tf.not_equal(GT, DataModule.PAD_TOKEN), 1.0, 0.0))\n    # backpropagation using variables, gradients and loss\n    #    - split this into two seperate optimizers/lrs/etc in the future\n    #    - we use the batch loss accumulation to update gradients\n    gradients = tape.gradient(batch_loss, encoder.trainable_variables + decoder.trainable_variables)\n    # Clip Grads?\n    gradients = [tf.clip_by_norm(g, TrainingConfig.clip_grad) for g in gradients]\n    batch_loss = batch_loss/(DataModule.MAX_LEN-1)\n    \n    metrics[\"train_loss\"].update_state(batch_loss)\n    \n    optimizer.apply_gradients(gradients, encoder.trainable_variables+decoder.trainable_variables)\n@tf.function\ndef dist_train_step(iterator, num_steps):\n    for i in tf.range(num_steps):\n        strategy.run(train_step, args=next(iterator))\ndef val_step(_image_batch, _inchi_batch):\n    \"\"\" Forward pass (calculate gradients)\n    \n    Args:\n        image_batch (): TBD\n        inchi_batch (): TBD\n    \n    Returns:\n        tbd\n    \"\"\"\n    \n    # Initialize batch_loss\n    batch_loss = tf.constant(0.0, tf.float32)   \n    \n    # image_batch_embedding has shape --> (REPLICA_BATCH_SIZE, IMG_EMB_DIM)\n    image_batch_embedding = encoder(_image_batch, training=False)\n\n    predictions_seq_batch = tf.ones((DataModule.REPLICA_BATCH_SIZE, 1), dtype=tf.uint8) # (B, L)\n\n    # Teacher forcing - feeding the target as the next input\n    for c_idx in range(1, DataModule.MAX_LEN):\n        gt_batch = _inchi_batch[:, c_idx]\n        \n        # passing enc_output to the decoder\n        prediction_batch = \\\n            decoder(image_batch_embedding, predictions_seq_batch, training=False) # (B, L)\n\n        # Update Loss Accumulator\n        pred = prediction_batch[:, -1]\n        batch_loss += loss_fn(gt_batch, pred)\n        \n        # Update Accuracy Metric\n        metrics[\"val_acc\"].update_state(gt_batch, pred,\n                                        sample_weight=tf.where(tf.not_equal(gt_batch, DataModule.PAD_TOKEN), 1.0, 0.0))\n\n        # no teacher forcing, predicted char is next LSTMCell input\n        decoder_input_batch = tf.expand_dims(tf.cast(tf.math.argmax(keras.activations.softmax(pred, axis = -1), axis=1, output_type=tf.int32), tf.uint8), axis=1)\n        predictions_seq_batch = tf.concat([predictions_seq_batch, decoder_input_batch], axis=1)\n        \n    # Normalize loss across all characters    \n    batch_loss = batch_loss/(DataModule.MAX_LEN-1)\n    \n    # Update Levenshtein Distance Metric & Loss Metric\n    metrics[\"val_loss\"].update_state(batch_loss)\n    \n    return predictions_seq_batch    \n\n    \n@tf.function\ndef dist_val_step(_val_image_batch, _val_inchi_batch):\n    predictions_seq_batch_per_replica = strategy.run(val_step, args=(_val_image_batch, _val_inchi_batch))\n    predictions_seq_batch_accum = strategy.gather(predictions_seq_batch_per_replica, axis=0)\n    _val_inchi_batch_accum = strategy.gather(_val_inchi_batch, axis=0)\n    \n    return predictions_seq_batch_accum, _val_inchi_batch_accum","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:08:51.559228Z","iopub.status.idle":"2021-05-22T23:08:51.559653Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize LR","metadata":{}},{"cell_type":"code","source":"# Only Uncomment on Testing. This code ruins the scheduler\n#import matplotlib.pyplot as plt\n#import tqdm.notebook as tqdm\n#lrs = []\n\n#for i in tqdm.tqdm(range(TrainingConfig.NUM_EPOCHS * DataModule.TRAIN_STEPS)):\n#    lrs += [optimizer.optimizer.learning_rate.numpy()]\n#    lr_scheduler.step()\n#plt.plot(lrs)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:08:51.56056Z","iopub.status.idle":"2021-05-22T23:08:51.560963Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stat Logger","metadata":{}},{"cell_type":"code","source":"class StatLogger():\n    def __init__(self, verbose_frequency=100):\n        self.epochs = 0\n        self.steps = 0\n      \n        self.best_loss = float('inf')\n        self.best_lsd = float('inf')\n        self.best_acc = 0\n\n    def print_train(self, metrics):\n      # Train Metrics are train_accuracy\n      train_acc = round(metrics['train_acc'].result().numpy().item(), 3)\n      train_loss = round(metrics['train_loss'].result().numpy().item(), 3)\n      # Get Cur LR\n      cur_lr = optimizer.optimizer.learning_rate.numpy().item()\n      print(f'E: {self.epochs}, S: {self.steps}, BL: {self.best_loss}, BLSD: {self.best_lsd}, BA: {self.best_acc}, TA: {train_acc}, TL: {train_loss}, LR: {cur_lr}')\n      self.steps += TrainingConfig.PRINT_EVERY\n\n      # Reset Metrics\n      metrics['train_acc'].reset_states()\n      metrics['train_loss'].reset_states()\n\n    def print_val(self, metrics):\n      \n      val_acc = round(metrics['val_acc'].result().numpy().item(), 3)\n      val_loss = round(metrics['val_loss'].result().numpy().item(), 3)\n      val_lsd = round(metrics['val_lsd'].result().numpy().item(), 3)\n        \n      basePath = ModelConfig.BaseSavePath\n      if val_lsd < self.best_lsd:\n        self.best_lsd = val_lsd\n        encoder.save_weights(f\"{basePath}enc_lsd.h5\")\n        decoder.save_weights(f\"{basePath}dec_lsd.h5\")\n      if val_acc > self.best_acc:\n        self.best_acc = val_acc\n        encoder.save_weights(f\"{basePath}enc_acc.h5\")\n        decoder.save_weights(f'{basePath}dec_acc.h5')\n      if val_loss < self.best_loss:\n        self.best_loss = val_loss\n        encoder.save_weights(f'{basePath}enc_loss.h5')\n        decoder.save_weights(f'{basePath}dec_loss.h5')\n      \n      print(f\"E: {self.epochs}, BA: {self.best_acc}, BL: {self.best_loss}, BLSD: {self.best_lsd}, VA: {val_acc} VL:{val_loss} VLSD: {val_lsd}\")\n      self.epochs += 1\n      self.steps = 0\n      # Reset All metrics\n      for metric in metrics:\n        metrics[metric].reset_states()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:08:51.562065Z","iopub.status.idle":"2021-05-22T23:08:51.562531Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sparse tensors are required to compute the Levenshtein distance\ndef dense_to_sparse(dense):\n    \"\"\" Convert a dense tensor to a sparse tensor \n    \n    Args:\n        dense (Tensor): TBD\n        \n    Returns:\n        A sparse tensor    \n    \"\"\"\n    indices = tf.where(tf.ones_like(dense))\n    values = tf.reshape(dense, (DataModule.MAX_LEN* DataModule.OVERALL_BATCH_SIZE,))\n    sparse = tf.SparseTensor(indices, values, dense.shape)\n    return sparse\n\ndef levenshtein_distance(preds, lbls):\n    \"\"\" Computes the Levenshtein distance between the predictions and labels \n    \n    Args:\n        preds (tensor): Batch of predictions\n        lbls (tensor): Batch of labels\n        \n    Returns:\n        The mean Levenshtein distance calculated across the batch\n    \"\"\"\n    preds = tf.where(tf.not_equal(lbls, DataModule.END_TOKEN) & tf.not_equal(lbls, DataModule.PAD_TOKEN), preds, 0)\n    lbls = tf.where(tf.not_equal(lbls, DataModule.END_TOKEN), lbls, 0)\n    \n    preds = tf.cast(preds, tf.uint8)\n    lbls = tf.cast(lbls, tf.uint8)\n    \n    preds_sparse = dense_to_sparse(preds)\n    lbls_sparse = dense_to_sparse(lbls)\n\n    batch_distance = tf.edit_distance(preds_sparse, lbls_sparse, normalize=False)\n    mean_distance = tf.math.reduce_mean(batch_distance)\n    \n    return mean_distance","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:08:51.563324Z","iopub.status.idle":"2021-05-22T23:08:51.563746Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"# Instantiate our tool for logging\nstat_logger = StatLogger()\n# Load Distributed dataset\nprint_every = TrainingConfig.PRINT_EVERY\nfor epoch in range(TrainingConfig.NUM_EPOCHS):\n    print(\"epoch: \")\n    \n    train_ds, val_ds = load_dataset()\n    train_dist_ds = iter(strategy.experimental_distribute_dataset(train_ds))\n    val_dist_ds = iter(strategy.experimental_distribute_dataset(val_ds))\n\n    for i in tqdm(range((TrainingConfig.TRAIN_STEPS // print_every) + 1)):\n        dist_train_step(train_dist_ds, print_every)\n        # -------Update LR 100 Times-----------(To make up for the print every steps)(Can't do it inside a tf.function)\n        for j in range(print_every):\n            lr_scheduler.step()\n        stat_logger.print_train(metrics)\n\n    print(\"\\n... VALIDATION DATASET STATISTICS ... \\n\")\n    for images, inchi in val_dist_ds:\n        preds, lbls = dist_val_step(images, inchi)\n        metrics[\"val_lsd\"].update_state(levenshtein_distance(preds, lbls))\n    stat_logger.print_val(metrics)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:08:51.564552Z","iopub.status.idle":"2021-05-22T23:08:51.564957Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Final Weights of the Model\nbasePath = ModelConfig.BaseSavePath\nwith strategy.scope():\n    encoder.save_weights(f\"{basePath}enc_last.h5\") # Save Last Encoder Weights\n    decoder.save_weights(f\"{basePath}dec_last.h5\") # Save Last Decoder Weights\n    np.save(f\"{basePath}optimizer_last.npy\", optimizer.optimizer.get_weights())","metadata":{"execution":{"iopub.status.busy":"2021-05-22T23:08:51.565818Z","iopub.status.idle":"2021-05-22T23:08:51.566218Z"},"trusted":true},"execution_count":null,"outputs":[]}]}