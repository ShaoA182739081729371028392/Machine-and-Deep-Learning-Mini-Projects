{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nprint(\"\\n... PIP INSTALLS COMPLETE ...\\n\")\n\nprint(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow_addons as tfa\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np\n\n# Built In Import\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom glob import glob\nimport random\nimport math\nfrom tqdm.notebook import tqdm\nimport os\n\nAUTO = tf.data.experimental.AUTOTUNE\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_it_all()\n\nimport sys\nsys.path.append(\"/kaggle/input/experimental-efficientnetv2/automl\")\nsys.path.append(\"/kaggle/input/experimental-efficientnetv2/automl/brain_automl\")\nsys.path.append(\"/kaggle/input/experimental-efficientnetv2/automl/brain_automl/efficientnetv2\")\n\n# Google brain Imports\n\n# EfficientNet Module Imports\nimport brain_automl\nfrom brain_automl import efficientnetv2\nfrom efficientnetv2 import effnetv2_model\nfrom efficientnetv2 import effnetv2_configs\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T03:30:58.717209Z","iopub.execute_input":"2021-05-29T03:30:58.71787Z","iopub.status.idle":"2021-05-29T03:31:04.879266Z","shell.execute_reply.started":"2021-05-29T03:30:58.717773Z","shell.execute_reply":"2021-05-29T03:31:04.878371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!conda install -y -c rdkit rdkit # For Normalization","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:04.880679Z","iopub.execute_input":"2021-05-29T03:31:04.881192Z","iopub.status.idle":"2021-05-29T03:31:04.885207Z","shell.execute_reply.started":"2021-05-29T03:31:04.881159Z","shell.execute_reply":"2021-05-29T03:31:04.884173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n    tf.keras.mixed_precision.set_global_policy('mixed_bfloat16' if TPU else 'float32')\n    tf.config.optimizer.set_jit(True)\nexcept:\n    TPU = None\n    strategy = tf.distribute.get_strategy() \nN_REPLICAS = strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:04.886908Z","iopub.execute_input":"2021-05-29T03:31:04.887597Z","iopub.status.idle":"2021-05-29T03:31:04.903682Z","shell.execute_reply.started":"2021-05-29T03:31:04.887563Z","shell.execute_reply":"2021-05-29T03:31:04.902453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataModule:\n    base_dir = 'bmsdatatest'\n    DATA_DIR = KaggleDatasets().get_gcs_path(base_dir)\n    if not TPU:\n        DATA_DIR = f\"/kaggle/input/{base_dir}\"\n    TARGET_DTYPE = tf.bfloat16 if TPU else tf.float32\n    TOKEN_LIST = [\"<PAD>\", \"InChI=1S/\", \"<END>\", \"/c\", \"/h\", \"/m\", \"/t\", \"/b\", \"/s\", \"/i\"] +\\\n             ['Si', 'Br', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', 'C', 'H', 'B', ] +\\\n             [str(i) for i in range(167,-1,-1)] +\\\n             [\"\\+\", \"\\(\", \"\\)\", \"\\-\", \",\", \"D\", \"T\"]\n    # Get rid of Backslashs\n    for token_idx in range(len(TOKEN_LIST)):\n        TOKEN_LIST[token_idx] = TOKEN_LIST[token_idx].strip('\\\\')\n        \n    # The start/end/pad tokens will be removed from the string when computing the Levenshtein distance\n    # We want them as tf.constant's so they will operate properly within the @tf.function context\n    VOCAB_LEN = len(TOKEN_LIST)\n    START_TOKEN = tf.constant(TOKEN_LIST.index(\"InChI=1S/\"), dtype=tf.uint8)\n    END_TOKEN = tf.constant(TOKEN_LIST.index(\"<END>\"), dtype=tf.uint8)\n    PAD_TOKEN = tf.constant(TOKEN_LIST.index(\"<PAD>\"), dtype=tf.uint8)\n\n    # Prefixes and Their Respective Ordering/Format\n    #      -- ORDERING --> {c}{h/None}{b/None}{t/None}{m/None}{s/None}{i/None}{h/None}{t/None}{m/None}\n    PREFIX_ORDERING = \"chbtmsihtm\"\n    print(f\"\\n... PREFIX ORDERING IS {PREFIX_ORDERING} ...\")\n\n    # Paths to Respective Image Directories\n    TEST_DIR = os.path.join(DATA_DIR, \"test_records\")\n\n    # Get the Full Paths to The Individual TFRecord Files\n    TEST_TFREC_PATHS = sorted(\n        tf.io.gfile.glob(os.path.join(TEST_DIR, \"*.tfrec\")), \n        key=lambda x: int(x.rsplit(\"_\", 2)[1]))\n\n   \n    # Paths to relevant CSV files containing training and submission information\n    TRAIN_CSV_PATH = os.path.join(\"/kaggle/input\", \"bms-molecular-translation\", \"train_labels.csv\")\n    SS_CSV_PATH    = os.path.join(\"/kaggle/input\", \"bms-molecular-translation\", \"sample_submission.csv\")\n    # When debug is true we use a smaller batch size and smaller model\n    DEBUG=False \n    \n    train_df = pd.read_csv(TRAIN_CSV_PATH)\n    ss_df    = pd.read_csv(SS_CSV_PATH)\n\n    # --- Distribution Information ---\n    N_EX    = len(train_df)\n    N_TEST  = len(ss_df)\n    N_VAL   = 100_000 # 80000 # Fixed from dataset creation information\n    N_TRAIN = N_EX-N_VAL\n    \n    BATCH_SIZE_DEBUG   = 2\n    REPLICA_BATCH_SIZE = 16# Larger BS at Infer.\n    if DEBUG:\n        REPLICA_BATCH_SIZE = BATCH_SIZE_DEBUG\n    OVERALL_BATCH_SIZE = REPLICA_BATCH_SIZE*N_REPLICAS\n\n\n    # --- Input Image Information ---\n    IMAGE_SIZE = 384\n    IMG_SHAPE = (192,384,3)\n    \n    # --- Autocalculate Training/Validation Information ---\n    START_TF = 0\n    END_TF = 21#len(TEST_TFREC_PATHS)\n    COMPLETED_TF = 0\n    NUM_TF = (END_TF - START_TF) # 16107 Examples in the last TFRec.\n    ALL_TF = False # True on Second Half\n    if ALL_TF:\n        # Compute Number of TF's already completed\n        COMPLETED_TF = START_TF\n        TEST_STEPS = len(ss_df) - COMPLETED_TF * 40000\n    else:\n        TEST_STEPS = NUM_TF * 40000\n    TEST_BATCHES = (TEST_STEPS // OVERALL_BATCH_SIZE) + 1\n    REQUIRED_DATASET_PAD = OVERALL_BATCH_SIZE-TEST_STEPS%OVERALL_BATCH_SIZE # Pad to Avoid Dropping Examples.\n    # --- Modelling Information --\n    FEEDFORWARD_DIM = 2048 # Increase to 256\n    DECODER_DIM   = 512 # increase to 1024 later \n    \n    MAX_LEN = 300 # Max Length Allowed at Inferenced\n    seed = 7\n    \n    mean_train = np.array([0.485, 0.456, 0.406])#np.array([0.9871, 0.9871, 0.9871])\n    mean_train = np.expand_dims(np.expand_dims(mean_train, axis = 0), axis = 0)\n    std_train = np.array([0.229, 0.224, 0.225])#np.array([0.0888,0.0888,0.0888])\n    std_train = np.expand_dims(np.expand_dims(std_train, axis = 0), axis = 0)\n    stats = (mean_train, std_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:04.905449Z","iopub.execute_input":"2021-05-29T03:31:04.906097Z","iopub.status.idle":"2021-05-29T03:31:15.070474Z","shell.execute_reply.started":"2021-05-29T03:31:04.906058Z","shell.execute_reply":"2021-05-29T03:31:15.069414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingConfig:\n    NUM_EPOCHS = 16\n    PRINT_EVERY = 100\n    \n \n    label_smoothing = 0.05\n    weight_decay = 1e-6\n    \n    clip_grad = 20.\n    \n    TEST_STEPS = DataModule.TEST_STEPS\n    TOTAL_STEPS = TEST_STEPS * NUM_EPOCHS\n    WARM_STEPS = 0.1\n    PEAK_STEPS = 0.2\n    WARM_START_LR = 1e-6\n    PEAK_START_LR = 5e-4\n    FINAL_LR = 1e-5\n    \n    GRAD_ACCUMULATION = 1","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:15.071752Z","iopub.execute_input":"2021-05-29T03:31:15.072199Z","iopub.status.idle":"2021-05-29T03:31:15.077729Z","shell.execute_reply.started":"2021-05-29T03:31:15.072157Z","shell.execute_reply":"2021-05-29T03:31:15.076505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_ex(x, y):\n    # Removes Examples that are too long.\n    num_pad = tf.equal(y, DataModule.PAD_TOKEN)\n    first_token = tf.argmax(num_pad)\n    if first_token > DataModule.MAX_LEN:\n        return False\n    return True\ndef cut_off(x, y):\n    return x, y[:DataModule.MAX_LEN]\ndef load_image(one_sample):\n    \n    feature_dict = {\n        'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=''),\n        'image_id': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value='')\n    }\n    features = tf.io.parse_single_example(one_sample, features=feature_dict)\n\n    image = tf.io.decode_png(features['image'], channels = 3)\n    image_id = features['image_id']\n    image = tf.reshape(image, DataModule.IMG_SHAPE)\n    image = tf.cast(image, tf.float32) / 255.0\n    # Normalize\n    return tf.cast(image, DataModule.TARGET_DTYPE), image_id","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:15.079409Z","iopub.execute_input":"2021-05-29T03:31:15.079816Z","iopub.status.idle":"2021-05-29T03:31:15.096495Z","shell.execute_reply.started":"2021-05-29T03:31:15.079772Z","shell.execute_reply":"2021-05-29T03:31:15.095275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset():\n    # Loads the Test Dataset in\n    options = tf.data.Options()\n    options.experimental_deterministic = False\n    \n    extra_padding = DataModule.REQUIRED_DATASET_PAD\n    \n    \n    START = DataModule.START_TF\n    END = DataModule.END_TF\n    test_dataset = tf.data.TFRecordDataset(DataModule.TEST_TFREC_PATHS[START:END], num_parallel_reads = AUTO)\n    test_dataset = test_dataset.with_options(options)\n    \n    test_dataset = test_dataset.map(lambda x: load_image(x), num_parallel_calls = AUTO, deterministic = False)\n    \n    if extra_padding!=0:\n        pad_dataset = tf.data.Dataset.from_tensor_slices((\n            tf.zeros((extra_padding, *DataModule.IMG_SHAPE), dtype=DataModule.TARGET_DTYPE),       # Fake Images\n            tf.constant([\"000000000000\",]*extra_padding, dtype=tf.string))   # Fake IDs\n        )\n        test_dataset = test_dataset.concatenate(pad_dataset)\n    \n    test_dataset = test_dataset.batch(DataModule.OVERALL_BATCH_SIZE)\n    test_dataset = test_dataset.prefetch(AUTO)\n    return test_dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:15.097749Z","iopub.execute_input":"2021-05-29T03:31:15.098073Z","iopub.status.idle":"2021-05-29T03:31:15.108553Z","shell.execute_reply.started":"2021-05-29T03:31:15.098045Z","shell.execute_reply":"2021-05-29T03:31:15.107759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nModel Config","metadata":{}},{"cell_type":"code","source":"class eff_net_v2_model(keras.Model):\n    def __init__(self, model_name, enc_dim):\n        super().__init__()\n        self.eff_net_v2_model = effnetv2_model.EffNetV2Model(model_name=model_name, name = 'eff_net_v2_model')\n        #self.proj = keras.layers.Conv2D(enc_dim, 1, use_bias = False, kernel_initializer = 'he_uniform', activation = 'relu') # Proj Layer, as the EffNetV2's don't have them and would benefit greatly from them.\n        #self.bn = keras.layers.BatchNormalization()\n    def call(self, x, training, features_only = True):\n        features = self.eff_net_v2_model(x, training = training, features_only = features_only) # Scale features 0 \n        #return self.bn(self.proj(features, training = training), training = training)\n        return features","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:15.109642Z","iopub.execute_input":"2021-05-29T03:31:15.110221Z","iopub.status.idle":"2021-05-29T03:31:15.125815Z","shell.execute_reply.started":"2021-05-29T03:31:15.11018Z","shell.execute_reply":"2021-05-29T03:31:15.125098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_efficientnetv2_backbone(model_name, weight_path, enc_dim, include_top=False, input_shape=(192,384,3), pooling=None, weights=None):\n    # Load Complex Weights.\n    model = eff_net_v2_model(model_name, enc_dim) \n    model.build((None, *DataModule.IMG_SHAPE))\n    info = model.eff_net_v2_model._layers[-1]\n    model.eff_net_v2_model._layers.pop(-2)\n    model.eff_net_v2_model._layers += [info]\n    if weight_path is not None:\n        checkpoint = tf.train.Checkpoint(net = model)\n        status = checkpoint.restore(tf.train.latest_checkpoint(weight_path))\n        print(\"Loaded Encoder Weights\")\n    #status.assert_existing_objects_matched() # Just Check that everything worked.\n    \n    return model\n    \nclass ModelConfig:\n    # ENCODER_CONFIG\n    enc_dim = 640 # Controls the Dimension of the Enc Dim\n    enc_dec_drop = 0.0 # Dropout Between Encoder and Decoder\n    transformer_drop = 0.2\n    final_drop = 0.5\n    num_encoder_layers = 4\n    num_decoder_layers = 6 # 6 layers is the max(Official Transformer uses 6.) - Becomes Exponentially Slow.\n    num_att_heads = 12 # Standard Transformer Config.\n    # CHANGE TO EFFICIENTNET V2. - Medium or Large Model(XL is too many params)\n    model_name = 'efficientnetv2-l' # EfficientNetV2 Small(Will scale to Large) - Mostly for model capacity, not for weights(NS is better.)\n    base_dir = 'efficientnet-v2-weights'\n    GCS_PATH = KaggleDatasets().get_gcs_path(base_dir) if TPU else '../input/efficientnet-v2-weights'\n    print(GCS_PATH)\n    weight_path = f'{GCS_PATH}/efficientnetv2-l-21k/efficientnetv2-l-21k/'\n    BB_FN = get_efficientnetv2_backbone\n    PREPROCESSING_FN = tf.keras.applications.efficientnet.preprocess_input\n    FREEZE_ENCODER = False\n    \n    tmp_model = BB_FN(model_name, weight_path,enc_dim, include_top=False, input_shape=DataModule.IMG_SHAPE)\n    IMG_EMB_DIM = tmp_model(tf.ones((DataModule.BATCH_SIZE_DEBUG, *DataModule.IMG_SHAPE)), features_only = True).shape[1:]\n    IMG_EMB_DIM = (IMG_EMB_DIM[0]*IMG_EMB_DIM[1], IMG_EMB_DIM[2])\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:15.128768Z","iopub.execute_input":"2021-05-29T03:31:15.129215Z","iopub.status.idle":"2021-05-29T03:31:37.758537Z","shell.execute_reply.started":"2021-05-29T03:31:15.12917Z","shell.execute_reply":"2021-05-29T03:31:37.75786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n... ENCODER MODEL CREATION STARTING ...\\n\")\n    \nclass CNNEncoder(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        \n        self.image_embedding_dim = ModelConfig.IMG_EMB_DIM\n        self.preprocessing_fn = ModelConfig.PREPROCESSING_FN\n        self.model_name = ModelConfig.model_name\n        self.weight_path = ModelConfig.weight_path\n        self.enc_dim = ModelConfig.enc_dim\n        self.backbone_fn = ModelConfig.BB_FN\n        self.img_shape = DataModule.IMG_SHAPE\n        self.encoder_backbone = self.backbone_fn(self.model_name, self.weight_path, self.enc_dim, include_top=False, weights=None, input_shape=self.img_shape)\n        self.dropout = ModelConfig.enc_dec_drop\n        self.spat_drop = keras.layers.SpatialDropout2D(rate = self.dropout)\n        self.reshape = tf.keras.layers.Reshape(self.image_embedding_dim, name='image_embedding')\n    def call(self, x, training):\n        \"\"\" TODO\n        \n        Args:\n            TODO        \n        \n        Returns:\n            TODO\n        \"\"\"\n        x = self.preprocessing_fn(x)\n        x = self.encoder_backbone(x, training=training)\n        x = self.spat_drop(x, training = training)\n        x = self.reshape(x, training=training)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:37.759973Z","iopub.execute_input":"2021-05-29T03:31:37.760444Z","iopub.status.idle":"2021-05-29T03:31:37.76911Z","shell.execute_reply.started":"2021-05-29T03:31:37.760413Z","shell.execute_reply":"2021-05-29T03:31:37.767998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderMultiHeadAttention(keras.layers.Layer):\n    def __init__(self, encoder_dim, num_heads, drop_prob = 0.1):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.num_heads = num_heads\n        self.drop_prob = drop_prob\n        \n        self.layer_norm1 = keras.layers.LayerNormalization(epsilon = 1e-6)\n        self.MAH = keras.layers.MultiHeadAttention(self.num_heads, self.encoder_dim // self.num_heads)\n        self.layer_norm2 = keras.layers.LayerNormalization(epsilon = 1e-6)\n        self.Linear = keras.layers.Dense(self.encoder_dim)\n    def __call__(self, x, training):\n        norm = self.layer_norm1(x, training = training)\n        mah = self.MAH(key = norm, value = norm, query = norm, training = training) + x\n        \n        norm2 = self.layer_norm2(mah, training = training)\n        linear = self.Linear(norm2, training = training) + mah\n        return linear\nclass TransformerEncoder(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.num_layers = ModelConfig.num_encoder_layers\n        self.num_heads = ModelConfig.num_att_heads\n        self.enc_dim = ModelConfig.IMG_EMB_DIM[-1]# Features from the CNN encoder\n        self.drop_prob = ModelConfig.enc_dec_drop\n        self.length_input = ModelConfig.IMG_EMB_DIM[0] # Length of the input\n\n        self.initializer = keras.initializers.glorot_uniform(seed = 42)\n        self.pos_enc = tf.Variable(self.initializer(shape = (1, self.length_input, self.enc_dim)), trainable = True)\n        self.encoders = [\n            EncoderMultiHeadAttention(self.enc_dim, self.num_heads, drop_prob = self.drop_prob) for i in range(self.num_layers)\n        ]\n    def call(self, x, training):\n        x = x + tf.cast(self.pos_enc, DataModule.TARGET_DTYPE) # Should Broadcast across Batch\n        for encoder in self.encoders:\n            x = encoder(x, training = training)\n        return x\nclass Encoder(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.CNN = CNNEncoder()\n        self.transformer = TransformerEncoder()\n    def call(self, x, training):\n        cnn = self.CNN(x, training = training)\n        return self.transformer(cnn, training = training)\nwith tf.device(\"CPU: 0\"):\n    encoder = Encoder()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:37.770331Z","iopub.execute_input":"2021-05-29T03:31:37.77058Z","iopub.status.idle":"2021-05-29T03:31:49.920362Z","shell.execute_reply.started":"2021-05-29T03:31:37.770556Z","shell.execute_reply":"2021-05-29T03:31:49.919317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Decoder","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(keras.layers.Layer):\n    def __init__(self, encoder_features, decoder_features, num_heads, drop_prob = 0.1):\n        super().__init__()\n        self.encoder_features = encoder_features\n        self.decoder_features = decoder_features\n        self.drop_prob = drop_prob\n        self.num_heads = num_heads\n        \n        self.MAH = keras.layers.MultiHeadAttention(self.num_heads, self.encoder_features // self.num_heads, value_dim = self.decoder_features // self.num_heads)\n        self.Dropout = keras.layers.Dropout(rate = self.drop_prob)\n        self.layernorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    def __call__(self, encoder_features, decoder_features, attention_mask, training):\n        query = decoder_features # (B, T, Dim) \n        key = encoder_features\n        value = encoder_features # (B, S, Dim) \n\n        attended = self.MAH(query = query, key = key, value = value, attention_mask = attention_mask,  training = training)\n        return self.layernorm(self.Dropout(attended, training = training) + decoder_features, training = training)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:49.921624Z","iopub.execute_input":"2021-05-29T03:31:49.922266Z","iopub.status.idle":"2021-05-29T03:31:49.931034Z","shell.execute_reply.started":"2021-05-29T03:31:49.922224Z","shell.execute_reply":"2021-05-29T03:31:49.930076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n    def __init__(self, encoder_features, decoder_features, num_heads, feedforward_dim, drop_prob = 0.1):\n        super().__init__()\n        self.encoder_features = encoder_features\n        self.decoder_features = decoder_features\n        self.feedforward_dim = feedforward_dim\n        \n        self.num_heads = num_heads\n        self.drop_prob = drop_prob\n        \n        self.MAH1 = MultiHeadAttention(self.decoder_features, self.decoder_features, self.num_heads, drop_prob = self.drop_prob)\n        self.MAH2 = MultiHeadAttention(self.encoder_features, self.decoder_features, self.num_heads, drop_prob = self.drop_prob)\n        \n        self.FFN = keras.Sequential([\n            keras.layers.Dense(self.feedforward_dim, activation = 'relu'),\n            keras.layers.Dense(self.decoder_features)\n        ])\n        self.Dropout = keras.layers.Dropout(rate = self.drop_prob)\n        self.layernorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    def __call__(self, encoder_features, decoder_features, attention_mask, padding_mask, training):\n        attention1 = self.MAH1(decoder_features, decoder_features, attention_mask, training = training)\n        \n        attention2 = self.MAH2(encoder_features, attention1, padding_mask, training = training)\n        \n        FFN = self.layernorm(self.Dropout(self.FFN(attention2, training = training), training = training) + attention2, training = training)\n        \n        return FFN\nclass FullTransformerDecoder(keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        # Extract Settings from Config\n        self.encoder_dim = ModelConfig.IMG_EMB_DIM[-1]\n        self.decoder_dim = DataModule.DECODER_DIM\n        self.feedforward_dim = DataModule.FEEDFORWARD_DIM\n        self.drop_prob = ModelConfig.transformer_drop\n        self.num_layers = ModelConfig.num_decoder_layers\n        self.num_heads = ModelConfig.num_att_heads\n        \n        self.DecoderLayer = [\n            TransformerDecoder(self.encoder_dim, self.decoder_dim, self.num_heads, self.feedforward_dim, drop_prob = self.drop_prob) for i in range(self.num_layers)\n        ]\n    \n    def __call__(self, encoder_features, decoder_features, attention_mask, padding_mask, training):\n        cur_decoded = decoder_features\n        for decoder in self.DecoderLayer:\n            cur_decoded = decoder(encoder_features, cur_decoded, attention_mask, padding_mask, training = training)\n        return cur_decoded\nclass Decoder(keras.Model):\n    # Full Decoder Model\n    def positional_embeddings(self, max_len, dim):\n        pos_enc = np.zeros((1, max_len, dim), dtype = np.float32)\n        for L in range(max_len):\n            for i in range(0, dim + 2, 2):\n                if i >= dim:\n                    break\n                pos_enc[:, L, i] = math.sin(L / 10000 ** (i / dim))\n                if i + 1 >= dim:\n                    break\n                pos_enc[:, L, i + 1] = math.cos(L / 10000 ** ((i + 1) / dim))\n\n        return tf.cast(tf.identity(pos_enc), DataModule.TARGET_DTYPE)\n    \n    def __init__(self):\n        super().__init__()\n        # Extract Features from Config \n        self.vocab_len = DataModule.VOCAB_LEN\n        self.decoder_dim = DataModule.DECODER_DIM\n        self.final_drop = ModelConfig.final_drop\n        self.max_len = DataModule.MAX_LEN\n        \n        self.embedding = keras.layers.Embedding(self.vocab_len, self.decoder_dim)\n        self.decoder_transformer = FullTransformerDecoder()\n        \n        self.Dropout = keras.layers.Dropout(rate = self.final_drop)\n        self.Linear = keras.layers.Dense(self.vocab_len)\n        self.layer = keras.layers.Layer(dtype = tf.float32)\n        \n        self.pos_enc = self.positional_embeddings(self.max_len, self.decoder_dim)\n    \n    def mask_pad(self, input_tokens, dtype):\n        # From Padidng Tokens, generates a Padding Mask(To Mimic Inference Time)\n        # Input_tokens: Tensor(B, L)\n        pad_tok = DataModule.PAD_TOKEN\n        is_pad = tf.equal(input_tokens, pad_tok) # (B, L)\n        return tf.cast(1 - tf.cast(tf.expand_dims(is_pad, axis = -1), dtype = tf.uint8),dtype) # (B, L, 1)\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n    def call(self, encoder_features, decoder_token, training):\n        # Extract Embeddings\n        # Encoder features: Tensor(B, L, C)\n        # Decoder Token: tensor(B, L, 1)      \n        B, L = decoder_token.shape\n        C = self.decoder_dim\n        _, Enc_L, _ = encoder_features.shape\n        \n        attention_mask = self.causal_attention_mask(B, L, L, tf.uint8) # (B, L, L)\n        padding_mask = self.mask_pad(decoder_token, tf.uint8) # (B, L, 1) \n        \n        # Add to the Attention Mask and Also Add Pad Mask\n        attention_mask = attention_mask * padding_mask\n        decoder_embeddings = self.embedding(decoder_token, training = training) # (B, L, C)\n        # Stretch the Padding Mask across encoder dims\n        padding_mask = tf.repeat(padding_mask, Enc_L, axis = -1)\n        # Stretch Positional Encodings Across the Batch\n        pos_enc = tf.repeat(self.pos_enc, B, axis = 0)[:, :L, :]\n        # Add Decoder Pos Enc\n        decoder_embeddings = decoder_embeddings + pos_enc\n        # Convert Masks to Bool\n        \n        attention_mask = tf.cast(attention_mask, tf.bool)\n        padding_mask = tf.cast(padding_mask, tf.bool) \n    \n        # Decode\n        decoded = self.decoder_transformer(encoder_features, decoder_embeddings, attention_mask, padding_mask, training = training) # (B, L, C)\n        # Get Element wise Predictions\n        dropped = self.Dropout(decoded, training = training)\n        return self.layer(self.Linear(dropped, training = training)) # (B, NumClasses)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:49.932413Z","iopub.execute_input":"2021-05-29T03:31:49.932726Z","iopub.status.idle":"2021-05-29T03:31:49.957295Z","shell.execute_reply.started":"2021-05-29T03:31:49.932692Z","shell.execute_reply":"2021-05-29T03:31:49.9562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pretrained Config","metadata":{}},{"cell_type":"code","source":"class PretrainedConfig:\n    encoder_checkpoints = ['../input/cptrmodel/save/enc_lsd.h5']\n    decoder_checkpoints = ['../input/cptrmodel/save/dec_lsd.h5']","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:49.95882Z","iopub.execute_input":"2021-05-29T03:31:49.959268Z","iopub.status.idle":"2021-05-29T03:31:49.973177Z","shell.execute_reply.started":"2021-05-29T03:31:49.959232Z","shell.execute_reply":"2021-05-29T03:31:49.972302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_for_training(verbose=0):\n    \"\"\" Declare required objects under TPU session scope and return ready for training\n    \n    Args:\n        lr_config (dict): Keyword arguments mapped to desired values for lr schedule function\n        encoder_config (dict): Keyword arguments mapped to desired values for encoder model instantiation\n        decoder_config (dict): Keyword arguments mapped to desired values for decoder model instantiation    \n        encoder_wts (str, optional): Path to pretrained model weights for encoder\n        decoder_wts (str, optional): Path to pretrained model weights for decoder\n        verbose (bool, optional): Whether or not to print model information and plot lr schedule\n        \n    Returns:\n        loss_fn - TBD\n        metrics - TBD\n        optimizer - TBD\n        lr_scheduler - TBD\n        encoder - TBD\n        decoder - TBD\n        \n    \"\"\"\n    \n\n    # Everything must be declared within the scope when leveraging the TPU strategy\n    #     - This will still function properly if scope is set to another type of accelerator\n    with strategy.scope():\n        \n        encoder_checkpoints = PretrainedConfig.encoder_checkpoints\n        decoder_checkpoints = PretrainedConfig.decoder_checkpoints\n        \n        assert len(encoder_checkpoints) == len(decoder_checkpoints)\n        models = []\n        for i in range(len(encoder_checkpoints) if len(encoder_checkpoints) != 0 else 1):\n            \n            # Instantiate the encoder model \n            print(\"\\t--> CREATING ENCODER MODEL ARCHITECTURE ...\")\n            encoder = Encoder()\n            initialization_batch = encoder(\n                tf.ones(((DataModule.REPLICA_BATCH_SIZE,)+DataModule.IMG_SHAPE), dtype=DataModule.TARGET_DTYPE), \n                training=False,\n            )\n        \n\n            # Instantiate the decoder model\n            print(\"\\t--> CREATING DECODER MODEL ARCHITECTURE...\")\n            decoder = Decoder()\n            pred_output = decoder(\n                initialization_batch,\n                tf.identity(np.random.randint(0, DataModule.VOCAB_LEN, (DataModule.REPLICA_BATCH_SIZE, DataModule.MAX_LEN), dtype = np.uint8)),\n                training=False,\n            )\n            models += [(encoder, decoder)]\n        for i in range(len(encoder_checkpoints)):\n            encoder_path = encoder_checkpoints[i]\n            decoder_path = decoder_checkpoints[i]\n            \n            models[i][0].load_weights(encoder_path)\n            models[i][1].load_weights(decoder_path)\n\n  \n    return models\n    \n    \nprint(\"\\n... GENERATING THE FOLLOWING:\")\n# Instantiate our required training components in the correct scope\nmodels = prepare_for_training(verbose=0)\n\nprint(\"\\n... TRAINING PREPERATION FINISHED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:31:49.974699Z","iopub.execute_input":"2021-05-29T03:31:49.975306Z","iopub.status.idle":"2021-05-29T03:32:23.328067Z","shell.execute_reply.started":"2021-05-29T03:31:49.975266Z","shell.execute_reply":"2021-05-29T03:32:23.327087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_encoder(encoder, beam_size):\n    # Encoder: tensor(B, L, C) \n    # Decoder: tensor(B, T)\n    # Repeats the encoder values and decoder values beam_size times to make them match the tokens\n    new_encoder = []\n    for encoder_values in encoder:\n        value = tf.repeat(encoder_values, beam_size, axis = 0)\n        new_encoder += [value]\n    return new_encoder # (BK, L, C)\n\ndef batch_decoder(decoder, beam_size):\n    new_decoder = tf.repeat(decoder, beam_size, axis = 0)\n    \n    return new_decoder # (BK, T)\ndef repeat_logits(logits, beam_size):\n    # Logits: \n    new_logits = tf.repeat(logits, beam_size, axis = 0)\n    return new_logits\ndef test_step(images):\n    # BATCHED BEAM SEARCH PREDICTIONS(semi Batched Beam Search) --------TRANSFORMER ENSEMBLE.\n    \n    B, H, W, C = images.shape\n    def predict(models, encoded_images, predictions_seq_batch, training = False):\n        pred = []\n        \n        for decoder_idx in range(len(models)):\n            pred += [models[decoder_idx][1](encoded_images[decoder_idx], predictions_seq_batch, training = training)[:, -1]]\n        # Average the predictions\n        pred = tf.stack(pred, axis = -1)\n        pred = tf.reduce_mean(pred, axis = -1)\n        # Softmax\n        pred = keras.activations.softmax(pred) # (B, C)\n        return pred\n    classes = DataModule.VOCAB_LEN\n    BEAM_SIZE = 16 # No Free Will said that min Beam Size 16 is needed for any results.\n    training = False\n    MAX_LEN = DataModule.MAX_LEN\n    # Initial Work Before: Encoding the Images\n     # image_batch_embedding has shape --> (REPLICA_BATCH_SIZE, IMG_EMB_DIM)\n    training = False\n    encoded_images = []\n    for encoder_idx in range(len(models)):\n        model_pred = models[encoder_idx][0](images, training = training)\n        _, L, C = model_pred.shape\n        encoded_images += [model_pred]\n    # For Simplicity: Let's assume a <START><END> never exists(it doesn't in this dataset)\n    predictions_seq_batch = tf.ones((DataModule.REPLICA_BATCH_SIZE, 1), dtype=tf.uint8)\n    \n    preds = predict(models, encoded_images, predictions_seq_batch, training = training)\n    # Top k\n    top_k = tf.math.top_k(preds, k = BEAM_SIZE)\n    indices = tf.cast(tf.reshape(top_k.indices, (-1, 1)), tf.uint8)\n    cur_logits = tf.reshape(top_k.values, (-1, 1))\n    \n    encoded_images = batch_encoder(encoded_images, BEAM_SIZE)\n    predictions_seq_batch = tf.ones((DataModule.REPLICA_BATCH_SIZE * BEAM_SIZE, 1), dtype=tf.uint8)\n    predictions_seq_batch = tf.concat([predictions_seq_batch, indices], axis = -1)\n    \n    final_predictions = [tf.ones((MAX_LEN), dtype = tf.uint8) for i in range(B)]\n    final_scores = [tf.ones((1), dtype = tf.float32) * 999. for i in range(B)]\n    beams_completed = np.ones((B, BEAM_SIZE))\n   \n\n    def pad_tensor(value): \n        # pads an output sentence to max len\n        L = value.shape[0]\n        pad_len = MAX_LEN - L\n        padded = tf.ones((pad_len), dtype = value.dtype) * tf.constant(DataModule.PAD_TOKEN, dtype = value.dtype)\n    \n        return tf.concat([value, padded], axis = 0)\n    def batch_wise_index(values, indices):\n        # Batch wise Indexing. Slow, but it works.\n        B, L, C = values.shape\n        _, L_prime = indices.shape\n        all_batches = tf.zeros((1, C), dtype = values.dtype)\n        for b in range(B):\n            all_lengths = tf.zeros((1, C), dtype = values.dtype)\n            index = indices[b]\n            \n            for i in range(index.shape[0]):\n                idx = index[i]\n                all_lengths = tf.concat([all_lengths, tf.expand_dims(values[b, idx], axis = 0)], axis = 0)\n            all_lengths = all_lengths[1:]\n            all_batches = tf.concat([all_batches, all_lengths], axis = 0)\n        return all_batches[1:]\n        \n   # Teacher forcing - feeding the target as the next input\n    for IDX in range(1, MAX_LEN - 1):\n        if IDX % 10 == 0:\n            print(IDX)\n       \n        # Decoded logits shouls already by (BK, T)\n        pred = predict(models, encoded_images, predictions_seq_batch, training = training) # (BK, C) \n\n        # Repeat the Tokens K times\n        repeated_tokens = batch_decoder(predictions_seq_batch, BEAM_SIZE) # (BK^2, T)\n        repeated_logits = repeat_logits(cur_logits, BEAM_SIZE) # (BK^2)\n        # Take the Top K Predictions\n        top_k_1 = tf.math.top_k(pred, k = BEAM_SIZE) # (BK, K)\n        indices = tf.cast(top_k_1.indices, tf.uint8) # (BK, K) - top K Classes\n        logits = top_k_1.values # (BK,K)\n        # Mask the logits\n        mask = tf.reshape(beams_completed, (-1, 1)) # (BK)\n        logits = logits * tf.cast(mask, logits.dtype) # (BK, K)\n        \n        indices = tf.reshape(indices, (-1, 1)) # (BK^2, 1)\n        logits = tf.reshape(logits, (-1, BEAM_SIZE)) # (BK, K)\n        # Append the Indices and Add the logits to get all predictions\n        logits = logits + cur_logits\n        repeated_logits = tf.reshape(logits, (B, -1)) # (B, K^2)\n        # Append\n        repeated_tokens = tf.concat([repeated_tokens, indices], axis = -1) # (BK^2, T + 1) \n        # Take the top K best Predictions\n        top_k_2 = tf.math.top_k(repeated_logits, k = BEAM_SIZE) \n        extraction_indices = tf.reshape(top_k_2.indices, (B, BEAM_SIZE)) # (B, K)\n        indices = tf.reshape(indices, (B, -1)) # (B, K ** 2)\n        \n        extracted_tokens = tf.gather(indices, extraction_indices, batch_dims = 1) # (B(K))\n        extracted_tokens = tf.squeeze(tf.reshape(extracted_tokens, (-1, 1)))\n\n        # Extracte the top K best Sentences\n        sentence_scores =tf.squeeze(tf.reshape(top_k_2.values, (-1, 1)))# (BK)\n        repeated_tokens = tf.reshape(repeated_tokens, (B, BEAM_SIZE ** 2, -1))  #(B, K ** 2, L)\n        extracted_sentences = batch_wise_index(repeated_tokens, extraction_indices)\n   \n        \n        # Reshape to separate out batch size and K \n        extracted_tokens = tf.reshape(extracted_tokens, (B, BEAM_SIZE)) # (B, K)\n        sentence_scores = tf.reshape(sentence_scores, (B, BEAM_SIZE)) # (B, K)\n        extracted_sentences = tf.reshape(extracted_sentences, (B, BEAM_SIZE, -1)) # (B, K, T + 1)\n        # Check for equality to <END>\n        new_mask = np.ones((B, BEAM_SIZE))\n        for b in range(B):\n            all_completed_sentences = tf.zeros((1, len(extracted_sentences[0, 0])), dtype = extracted_sentences[0, 0].dtype)\n            all_completed_scores = tf.zeros((1, 1), dtype = sentence_scores[0, 0].dtype)\n            \n            for beam_idx in range(BEAM_SIZE):\n                \n                if extracted_tokens[b, beam_idx] == tf.constant(DataModule.END_TOKEN): # (B, K) \n                    all_completed_sentences = tf.concat([all_completed_sentences, tf.expand_dims(extracted_sentences[b, beam_idx], axis = 0)], axis = 0)\n                    all_completed_scores = tf.concat([all_completed_scores, tf.expand_dims(tf.expand_dims(sentence_scores[b, beam_idx], axis = 0), axis = 0)], axis = 0)\n                    new_mask[b, beam_idx] = 0\n                else:\n                    pass\n            # ISSUE WITH NESTED LOOPS, ONLY HAPPENS WHEN LOOPS ARE NESTED.        \n            all_completed_sentences = all_completed_sentences[1:]\n            all_completed_scores = all_completed_scores[1:]\n            # Find Completed sentences \n            completed_sentences = all_completed_sentences \n            completed_scores = all_completed_scores # (N)\n            \n            # Hash all of the scores of completed indices\n            for i in range(len(completed_scores)):\n            \n                if tf.reduce_sum(tf.cast(tf.equal(completed_sentences[i], DataModule.END_TOKEN), tf.int32)) == 1 and tf.reduce_sum(tf.cast(tf.equal(completed_sentences[i, -1], DataModule.END_TOKEN), tf.int32)) == 1:\n                    \n                    padded_sent = pad_tensor(completed_sentences[i])\n                    scores = completed_scores[i] / (len(completed_sentences[i]) - 1)\n                    if scores < final_scores[b]:\n                        final_scores[b] = scores\n                        final_predictions[b] = padded_sent\n                    \n                else:\n                    pass\n            \n        \n        # Iterate through batches and remove completed sentences\n        beams_completed = new_mask\n        cur_logits = tf.reshape(sentence_scores, (-1, 1)) # (BK, 1)\n        predictions_seq_batch = tf.reshape(extracted_sentences, (B * BEAM_SIZE, -1))\n        \n    # add all remaining to the cache\n    predictions_seq_batch = tf.reshape(predictions_seq_batch, (B, BEAM_SIZE, -1))\n    cur_logits = tf.reshape(cur_logits, (B, BEAM_SIZE))\n   \n    for b in range(predictions_seq_batch.shape[0]):\n        for i in range(len(predictions_seq_batch[b])):\n            padded_sent = pad_tensor(predictions_seq_batch[b, i])\n            scores = cur_logits[b, i] / (MAX_LEN - 1)\n        \n            if scores < final_scores[b]:\n                final_scores[b] = scores\n                final_predictions[b] = padded_sent\n    \n    return final_predictions\n\n@tf.function\ndef dist_test_step(images, ids):\n    pred = strategy.run(test_step, args = (images, ))\n    predictions = strategy.gather(pred, axis=0)\n    pred_ids = strategy.gather(ids, axis=0)\n    return predictions, pred_ids","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:32:23.329244Z","iopub.execute_input":"2021-05-29T03:32:23.329713Z","iopub.status.idle":"2021-05-29T03:32:23.363467Z","shell.execute_reply.started":"2021-05-29T03:32:23.329685Z","shell.execute_reply":"2021-05-29T03:32:23.362445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def arr_2_inchi(arr):\n    \"\"\" Basic integer array to inchi string conversion \"\"\"\n    inchi_str = ''\n    for i in arr:\n        c = DataModule.TOKEN_LIST[i]\n        if c==\"<END>\":\n            break\n        inchi_str += c\n    return inchi_str\n\ndef decode(sentences):\n    all_sentences = []\n    B, L = sentences.shape\n    for i in range(B):\n        sentence = sentences[i]\n        inchi_string = ''\n        for j in sentence:\n            token = DataModule.TOKEN_LIST[j]\n          \n            if token == '<END>':\n                break\n            inchi_string += token\n        all_sentences += [inchi_string]\n    return all_sentences\ndef post_process(all_pred_arr, all_pred_ids):\n    # List of Sentences\n    max_len = DataModule.MAX_LEN\n    \n    image_id = [x[0].decode() for x in tqdm(all_pred_ids[1:-DataModule.REQUIRED_DATASET_PAD].numpy(), total=DataModule.TEST_STEPS)]\n    inchi = [arr_2_inchi(pred_arr) for pred_arr in tqdm(all_pred_arr[1:-DataModule.REQUIRED_DATASET_PAD].numpy(), total=DataModule.TEST_STEPS)]\n    pred_df = pd.DataFrame({\n    \"image_id\": image_id, \n    \"InChI\": inchi\n    })\n\n   \n    # create prediction DF\n    pred_df = pred_df.sort_values(by=\"image_id\").reset_index(drop=True)\n    pred_df.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T03:32:23.36457Z","iopub.execute_input":"2021-05-29T03:32:23.364888Z","iopub.status.idle":"2021-05-29T03:32:23.383637Z","shell.execute_reply.started":"2021-05-29T03:32:23.36486Z","shell.execute_reply":"2021-05-29T03:32:23.382365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = load_dataset()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference Loop \ntest_ds = load_dataset()\ntest_ds = iter(strategy.experimental_distribute_dataset(test_ds))\nall_pred_sentences = tf.ones((1, DataModule.MAX_LEN), dtype = tf.uint8)\nall_pred_ids = tf.zeros((1, 1), dtype = tf.string)\ncount = 0\ncur_val = 10000\nfor images, ids in test_ds:\n    sentences, ids = dist_test_step(images, ids)\n    all_pred_sentences = tf.concat([all_pred_sentences, sentences], axis = 0)\n    all_pred_ids = tf.concat([all_pred_ids, tf.expand_dims(ids, axis = -1)], axis = 0)\n    count += DataModule.OVERALL_BATCH_SIZE\n    if count > cur_val:\n        print(cur_val)\n        cur_val += 10000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"post_process(all_pred_sentences, all_pred_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalize Predictions(Post Processing)","metadata":{}},{"cell_type":"code","source":"'''\n%%writefile normalize_inchis.py\n\nfrom tqdm import tqdm\nfrom rdkit import Chem\nfrom rdkit import RDLogger\nRDLogger.DisableLog('rdApp.*')\nfrom pathlib import Path\n\ndef normalize_inchi(inchi):\n    try:\n        mol = Chem.MolFromInchi(inchi)\n    except:\n        pass\n    if mol is None:\n        return inchi\n    else:\n        try: return Chem.MolToInchi(mol)\n        except: return inchi\n        \nsubmission_name = '../input/bmsmt-ds-model/submission_3.06.csv'\nnorm_path = Path('submission_norm.csv')\n\n# Do the job\nN = norm_path.read_text().count('\\n') if norm_path.exists() else 0\nprint(N, 'number of predictions already normalized')\n\nr = open(submission_name, 'r')\nwrite_mode = 'w' if N == 0 else 'a'\nw = open(str(norm_path), write_mode, buffering=1)\n\nfor _ in range(N):\n    r.readline()\nline = r.readline()  # this line is the header or is where it died last time\nw.write(line)\n\npbar = tqdm()\nwhile True:\n    line = r.readline()\n    if not line:\n        break  # done\n    image_id = line.split(',')[0]\n    inchi = ','.join(line[:-1].split(',')[1:]).replace('\"','')\n    inchi_norm = normalize_inchi(inchi)\n    w.write(f'{image_id},\"{inchi_norm}\"\\n')\n    pbar.update(1)\n\nr.close()\nw.close()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !while [ 1 ]; do python normalize_inchis.py && break; done","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post Process Submission.csv Files.","metadata":{}},{"cell_type":"markdown","source":"# 1 Hour Startup Time, 20 Minutes to actually infer lol.","metadata":{}}]}