{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport PIL\n\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport timm\n\n\nimport matplotlib.pyplot as plt\n\nimport tqdm.notebook as tqdm\nimport copy\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Paths\nmobile_net_path = '../input/mobilenetv3trained/BestValLoss.pth'\neff_net_fold_0_path = \"../input/efficientnetfolds/fold0/fold_0_F.pth\"\neff_net_fold_1_path = \"../input/efficientnetfolds/fold1/fold_0_F.pth\"\neff_net_fold_2_path = \"../input/efficientnetfolds/fold2/fold_0_F.pth\" # Trained Model Paths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPERPARAMETERS\ndataframe_path = \"../input/plant-pathology-2021-fgvc8/train.csv\"\ntest_df_path = '../input/plant-pathology-2021-fgvc8/sample_submission.csv'\ntest_df = pd.read_csv(test_df_path)\ntest_df = test_df.set_index('image')\nBASE_PATH = \"../input/plant-pathology-2021-fgvc8/test_images/\"\nTEST_BATCH_SIZE = 32\nINPUT_WIDTH = 320\nIMAGE_SIZE = 320\nINPUT_HEIGHT = int(INPUT_WIDTH *1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transforms = A.Compose([\n    A.Resize(INPUT_WIDTH, INPUT_HEIGHT),\n    A.Normalize(),\n    ToTensorV2()\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataFrame = pd.read_csv(dataframe_path)\ndataFrame = dataFrame.set_index(\"image\")\ndef process_dataframe(dataframe):\n    '''\n    dataFrame: pandas dataframe containing image ids and labels\n    '''\n    classes = []\n    count = 0\n    for row in dataframe.iterrows():\n        labels = str.split(row[1][0])\n        classes += labels\n    classes = sorted(list(set(classes)))\n    classes.remove('healthy')\n    return classes # Multi Class Classification Over 5 Classes\nCLASSES = process_dataframe(dataFrame)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_base(df, base_path):\n    '''\n    Adds base paths to every entry in the df, so no more need for base_path\n    '''\n    pred = {'image': [], 'labels': []}\n    for row in df.iterrows():\n        img = base_path + row[0]\n        \n        labels = row[1][0]\n        pred['image'] += [img]\n        pred['labels'] += [labels]\n    df = pd.DataFrame(pred)\n    df = df.set_index('image')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_test_df = add_base(test_df, BASE_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process and Load Test Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestPlantDataset(torch.utils.data.Dataset):\n    def __init__(self, test_df, transforms):\n        self.test_df = test_df\n        self.indices = self.test_df.index.values\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.indices)\n    def __getitem__(self, idx):\n        index = self.indices[idx]\n        # Extract index from file_path\n        image_id =\"\"\n        for idx in range(len(index) - 1, -1, -1):\n            if index[idx] == \"/\":\n                image_id = index[idx + 1:]\n                break\n        # Load image in \n        image = cv2.imread(index)\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        image = self.transforms(image = image)['image']\n        return image, image_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Datasets for Initial Pred\ntest_dataset = TestPlantDataset(processed_test_df, test_transforms)\n# Load in Dataloader\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = TEST_BATCH_SIZE, shuffle = False, worker_init_fn = seed_worker)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Blocks"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StridedConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups) \n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SqueezeExcite(nn.Module):\n    def __init__(self, in_dim, inner_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        \n        self.Squeeze = nn.Linear(self.in_dim, self.inner_dim)\n        self.Excite = nn.Linear(self.inner_dim, self.in_dim)\n        self.act1 = nn.SiLU(inplace = True) \n    def forward(self, x):\n        max_pool, _ = torch.max(x, dim = -1) \n        max_pool, _ = torch.max(max_pool, dim = -1)\n        \n        squeezed = self.act1(self.Squeeze(max_pool))\n        excite = torch.sigmoid(self.Excite(squeezed)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BottleNeck(nn.Module):\n    def __init__(self, in_features, inner_features, device, stochastic_depth = 0.2):\n        super().__init__()\n        self.stochastic_depth = stochastic_depth\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.device = device\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1)\n        self.Expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n        self.SE = SqueezeExcite(self.in_features, self.in_features // 16)\n       \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        if self.training and random.random() <= self.stochastic_depth:\n            return x\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        excited = self.SE(expand)\n        return self.gamma * excited + x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InvertedBottleNeck(nn.Module):\n    def __init__(self, in_dim, inner_dim, device, stochastic_depth = 0.2):\n        super().__init__()\n        self.stochastic_depth = stochastic_depth\n        self.device = device\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        \n        self.expand = ConvBlock(self.in_dim, self.inner_dim, 1, 0, 1)\n        self.depthwise = ConvBlock(self.inner_dim, self.inner_dim, 3, 1, self.inner_dim)\n        self.SE = SqueezeExcite(self.inner_dim, self.inner_dim // 16)\n        self.squeeze = ConvBlock(self.inner_dim, self.in_dim, 1, 0, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        if self.training and random.random() <= self.stochastic_depth:\n            return x \n        expanded = self.expand(x)\n        depthwise = self.depthwise(expanded)\n        excited = self.SE(depthwise)\n        squeezed = self.squeeze(excited)\n        return self.gamma * squeezed + x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNNDownSampler(nn.Module):\n    '''\n    Uses Convolution and Resizing to Downsample images\n    '''\n    def __init__(self, out_size, device):\n        super().__init__()\n        self.device = device\n        self.out_size = out_size\n        self.initialCNN = nn.Sequential(*[\n            ConvBlock(3, 16, 1, 0, 1), ConvBlock(16, 16, 1, 0, 1)])\n        \n        self.post_processing = nn.Sequential(*[\n            ConvBlock(16, 16, 1, 0, 1) for i in range(2) \n        ])\n        self.proj = ConvBlock(16, 3, 1, 0, 1)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        resized = F.interpolate(x, size = (self.out_size, self.out_size), mode = 'bilinear')\n        initial_processing = self.initialCNN(x)\n        # Resize\n        resize_processed = F.interpolate(initial_processing, size = (self.out_size, self.out_size), mode = 'bilinear')\n        # Further Processing\n        further_processed = self.post_processing(resize_processed)\n        proj = self.proj(further_processed)\n        return proj * self.gamma + resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MobileNetv3(nn.Module):\n    '''\n    BaseLine MobileNet v3 Based Model\n    '''\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def __init__(self, out_size, num_classes, device, dropout = 0.2, stochastic_depth = 0.2):\n        super().__init__()\n        self.drop_prob = dropout\n        self.stochastic_depth = stochastic_depth\n        self.model_name = 'mobilenetv3_large_100'\n        self.model = timm.create_model(self.model_name, pretrained = False)\n        \n        self.out_size = out_size\n        self.num_classes = num_classes\n        self.device = device\n        self.downsampler = CNNDownSampler(self.out_size, self.device) \n        \n        self.conv1 = self.model.conv_stem\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        \n        self.block0 = self.model.blocks[0] # (16)\n        self.block1 = self.model.blocks[1] # (24)\n        self.block2 = self.model.blocks[2] \n        self.block3 = self.model.blocks[3]\n        self.block4 = self.model.blocks[4]\n        self.block5 = self.model.blocks[5]\n        self.block6 = self.model.blocks[6] # We won't be using this block, it's very expensive\n        \n        # Freeze Initial Layers\n        #self.freeze(self.conv1)\n        #self.freeze(self.bn1)\n        #self.freeze(self.block0)\n        #self.freeze(self.block1)\n        #self.freeze(self.block2)\n        #self.freeze(self.block3)\n        #self.freeze(self.block4)\n        #self.freeze(self.block5)\n        \n        # Custom Layers\n        self.Attention1 = SqueezeExcite(40, 16)\n        self.Attention2 = SqueezeExcite(112, 32)\n        self.Attention3 = SqueezeExcite(160, 48)\n        self.layer4 = nn.Sequential(*[\n            ConvBlock(160, 320, 1, 0, 1)\n        ] + [\n            BottleNeck(320, 64, self.device, stochastic_depth = self.stochastic_depth) for i in range(5)\n        ])\n        \n        self.Attention4 = SqueezeExcite(320, 96)\n        self.layer5 = nn.Sequential(*[\n            StridedConvBlock(320, 512, 1, 0, 1, 2)\n        ] + [\n            BottleNeck(512, 128, self.device, stochastic_depth = self.stochastic_depth) for i in range(3)\n        ])\n        \n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(512, self.num_classes)\n    def forward(self, x):\n        downsampled = self.downsampler(x)\n        features0 = self.bn1(self.act1(self.conv1(downsampled)))\n        block0 = self.block0(features0) # (16, 160)\n        block1 = self.block1(block0) # (24, 80)\n        block2 = self.block2(block1) # (40, 40)\n        block2 = self.Attention1(block2)\n        \n        block3 = self.block3(block2) # (80, 20)\n        block4 = self.block4(block3) # (112, 20)\n        block4 = self.Attention2(block4)\n        \n        block5 = self.block5(block4) # (160, 10)\n        block5 = self.Attention3(block5)\n        \n        layer4 = self.layer4(block5) \n        layer4 = self.Attention4(layer4) # (320, 10)\n        \n        layer5 = self.layer5(layer4) # (512, 5) \n        \n        avg_pool = torch.squeeze(self.global_avg(layer5))\n        avg_pool = self.dropout(avg_pool)\n        return self.Linear(avg_pool)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomLoss(nn.Module):\n    '''\n    Separates BCE loss into 0s and 1 to weight them evenly(1s are sparse)\n    '''\n    def __init__(self):\n        super().__init__()\n        self.criterion = nn.BCEWithLogitsLoss()\n    def forward(self, pred, y_true):\n        one_bools = y_true == 1\n        zero_bools = y_true == 0\n        pred_one = pred[one_bools]\n        pred_zeros = pred[zero_bools]\n        \n        one_loss = self.criterion(pred_one, torch.ones_like(pred_one, device = pred_one.device))\n        zero_loss = self.criterion(pred_zeros, torch.zeros_like(pred_zeros, device = pred_zeros.device))\n        \n        return one_loss + zero_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MobileSolverQTPi(nn.Module):\n    def __init__(self, num_classes, device):\n        super().__init__()\n        self.out_size = IMAGE_SIZE\n        self.num_classes = num_classes\n        self.device = device\n        self.dropout = 0.5\n        self.stochastic_depth = 0.0\n        self.model = MobileNetv3(self.out_size, self.num_classes, self.device, dropout = self.dropout, stochastic_depth = self.stochastic_depth)\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            return torch.sigmoid(self.model(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedEffNetQT(nn.Module):\n    '''\n    EfficientNet B4 Variant of the Same Model\n    '''\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = True\n    def __init__(self, out_size, num_classes, device, dropout = 0.2, stochastic_depth = 0.2):\n        super().__init__()\n        self.model_name = \"tf_efficientnet_b4_ns\"\n        self.model = timm.create_model(self.model_name, pretrained = False)\n        \n        self.out_size = out_size\n        self.num_classes = num_classes\n        self.device = device\n        self.drop_prob = dropout\n        self.stochastic_depth = stochastic_depth\n        \n        self.downsampler = CNNDownSampler(self.out_size, self.device)\n        \n        self.conv1 = self.model.conv_stem\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n    \n        self.block0 = self.model.blocks[0]\n        self.block1 = self.model.blocks[1]\n        self.block2 = self.model.blocks[2]\n        self.block3 = self.model.blocks[3]\n        self.block4 = self.model.blocks[4]\n        self.block5 = self.model.blocks[5]\n        self.block6 = self.model.blocks[6]\n       \n        # Freeze Values(Uncomment if you replace self.downsampler with nn.Identity)\n        #self.freeze(self.conv1)\n        #self.freeze(self.bn1)\n        #self.freeze(self.block0)\n        #self.freeze(self.block1)\n        #self.freeze(self.block2)\n        \n        self.Attention1 = SqueezeExcite(56, 16)\n        self.Attention2 = SqueezeExcite(160, 32)\n        self.Attention3 = SqueezeExcite(448, 128)\n        \n        self.layer5 = nn.Sequential(*[\n            StridedConvBlock(448, 768, 1, 0, 1, 2)\n        ] + [\n           InvertedBottleNeck(768, 1536, self.device, stochastic_depth = self.stochastic_depth) for i in range(3)\n        ])\n        self.proj = ConvBlock(768, 2048, 1, 0, 1)\n        \n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(2048, self.num_classes)\n    def forward(self, x):\n        downsampled = self.downsampler(x)\n        features0 = self.bn1(self.act1(self.conv1(downsampled))) # (B, 48, 160, 160)\n        \n        block0 = self.block0(features0)\n        block1 = self.block1(block0)\n        block2 = self.block2(block1) # (B, 56, 40, 40)\n        # Attention1\n        block2 = self.Attention1(block2)\n        \n        block3 = self.block3(block2)\n        block4 = self.block4(block3) # (B, 160, 20, 20)\n        # Attention2\n        block4 = self.Attention2(block4)\n        \n        block5 = self.block5(block4)\n        block6 = self.block6(block5)\n        # attention 3\n        block6 = self.Attention3(block6) # (B, 448, 10, 10)\n        \n        layer5 = self.layer5(block6)\n        layer5 = self.proj(layer5)\n        global_avg = torch.squeeze(self.global_avg(layer5))\n        dropped = self.dropout(global_avg)\n        return self.Linear(dropped)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedResNetQT(nn.Module):\n    '''\n    Modified ResNet200D\n    '''\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = True\n    def __init__(self, out_size, num_classes, device, dropout = 0.2, stochastic_depth = 0.2):\n        super().__init__()\n        self.device = device\n        self.num_classes = num_classes\n        self.dropout = dropout \n        self.out_size = out_size\n        self.device = device\n        self.stochastic_depth = stochastic_depth\n        self.model_name = 'resnet200d'\n        self.model = timm.create_model(self.model_name, pretrained = False)\n        \n        self.downsampler = CNNDownSampler(self.out_size, self.device) \n        \n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.pool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4 # we won't use this layer.\n        \n        # Freeze Initial Layers\n        #self.freeze(self.conv1)\n        #self.freeze(self.bn1)\n        #self.freeze(self.layer1)\n        #self.freeze(self.layer2)\n        \n        # Custom Layer\n        self.Attention2 = SqueezeExcite(512, 128)\n        self.Attention3 = SqueezeExcite(1024, 256)\n        self.Attention4 = SqueezeExcite(2048, 512)\n        \n        self.layer4 = nn.Sequential(*[\n            StridedConvBlock(1024, 1536, 1, 0, 1, 2)\n        ] + [\n            BottleNeck(1536, 512, self.device, stochastic_depth = self.device) for i in range(5)\n        ])\n        self.layer5 = nn.Sequential(*[\n            StridedConvBlock(1536, 2048, 1, 0, 1)\n        ] + [\n            BottleNeck(2048, 512, self.device, stochastic_depth = self.device) for i in range(2)\n        ])\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(2048, self.num_classes)\n    def forward(self, x):\n        '''\n        x: Tensor(B, 3, H, W)\n        '''\n        downsampled = self.downsampler(x) # (B, 3, out_size, out_size)\n        features0 = self.pool(self.bn1(self.act1(self.conv1(downsampled)))) # (B, 64, 160, 160)\n        \n        layer1 = self.layer1(features0)\n        layer2 = self.layer2(layer1)\n        # Attention2\n        layer2 = self.Attention2(layer2)\n        \n        layer3 = self.layer3(layer2)\n        # Attention 3\n        layer3 = self.Attention3(layer3)\n        \n        layer4 = self.layer4(layer4)\n        layer4 = self.Attention4(layer4)\n        \n        layer5 = self.layer5(layer4)\n        \n        global_avg = torch.squeeze(self.global_avg(layer5))\n        dropped = self.dropout(global_avg)\n        return self.Linear(dropped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiFoldQTPi(nn.Module):\n    def __init__(self, num_classes, device):\n        super().__init__()\n        self.num_classes = num_classes\n        self.device = device\n        self.image_size = 320\n        self.stochastic_depth = 0.0\n        self.dropout = 0.2\n        \n        self.model = ModifiedEffNetQT(self.image_size, self.num_classes, self.device, stochastic_depth = self.stochastic_depth, dropout = self.dropout)\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            return torch.sigmoid(self.model(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load All Models into one for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model():\n    '''\n    Loads all model\n    '''\n    mobile = MobileSolverQTPi(len(CLASSES), device)\n    if mobile_net_path != \"\":\n        mobile.load_state_dict(torch.load(mobile_net_path, map_location = device))\n    mobile.to(device)\n    \n    eff0 = MultiFoldQTPi(len(CLASSES), device)\n    if eff_net_fold_0_path !=\"\":\n        eff0.load_state_dict(torch.load(eff_net_fold_0_path, map_location = device))\n    eff0.to(device)\n    \n    eff1 = MultiFoldQTPi(len(CLASSES), device)\n    if eff_net_fold_1_path != \"\":\n        eff1.load_state_dict(torch.load(eff_net_fold_1_path, map_location = device))\n    eff1.to(device)\n    \n    eff2 = MultiFoldQTPi(len(CLASSES), device)\n    if eff_net_fold_2_path != \"\":\n        eff2.load_state_dict(torch.load(eff_net_fold_2_path, map_location = device))\n    eff2.to(device)\n    return mobile, eff0, eff1, eff2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tester(nn.Module):\n    '''\n    Combines all 4 models for inference\n    '''\n    def __init__(self, mobile, eff0, eff1, eff2):\n        super().__init__()\n        self.mobile_weight = 0.25\n        self.eff0_weight = 1\n        self.eff1_weight = 1\n        self.eff2_weight = 1\n        \n        self.mobile = mobile\n        self.eff0 = eff0\n        self.eff1 = eff1\n        self.eff2 = eff2\n        \n        self.divide = self.mobile_weight + self.eff0_weight + self.eff1_weight + self.eff2_weight\n        \n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            image, image_vert, image_hor, image_double = self.tta(x)\n            pred0 = self.pred_one_model(self.mobile, image, image_vert, image_hor, image_double) * self.mobile_weight\n            pred1 = self.pred_one_model(self.eff0, image, image_vert, image_hor, image_double) * self.eff0_weight\n            pred2 = self.pred_one_model(self.eff1, image, image_vert, image_hor, image_double) * self.eff1_weight\n            pred3 = self.pred_one_model(self.eff2, image, image_vert, image_hor, image_double) * self.eff2_weight\n            \n            return (pred0 + pred1 + pred2 + pred3) / self.divide\n            \n    def threshold(self, pred):\n        bools = pred >= 0.5\n        pred[:, :] = 0\n        pred[bools] = 1\n        return pred\n    def pred_one_model(self, model, image, image_vert, image_hor, image_double):\n        pred = model(image)\n        pred1 = model(image_vert)\n        pred2 = model(image_hor)\n        pred3 = model(image_double)\n        return (pred + pred1 + pred2 + pred3) / 4 \n    def tta(self, image):\n        image_vert = image.flip(-2)\n        image_hor = image.flip(-1)\n        image_double = image.flip(-1).flip(-2)\n        return image, image_vert, image_hor, image_double\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PlantInference(nn.Module):\n    def __init__(self, CLASSES):\n        super().__init__()\n        self.classes = CLASSES\n        self.num_classes= len(self.classes) \n        \n        self.idx2class = {}\n        self.class2idx = {}\n        for idx in range(self.num_classes):\n            self.idx2class[idx] = self.classes[idx]\n            self.class2idx[self.classes[idx]] = idx\n    def decode(self, idx):\n        return self.idx2class[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(data):\n    df = pd.DataFrame(data)\n    df = df.set_index('image')\n    df.to_csv(\"./submission.csv\", index_label = 'image')\ndef create_submission(pred):\n    '''\n    pred: Tensor(C) \n    '''\n    inference_config = PlantInference(CLASSES)\n    C = pred.shape[0]\n    string = ''\n    for i in range(C):\n        val = pred[i].item()\n        if val == 1:\n            if string == '':\n                string += inference_config.decode(i)\n            else:\n                string += f' {inference_config.decode(i)}'\n    if string == '':\n        return 'healthy'\n    return string\n        \n    \ndef make_submission(model, dataloader):\n    data = {'image': [], 'labels': []}\n    for images, ids in tqdm.tqdm(dataloader):\n        images = images.to(device) \n        ids = list(ids)\n        pred = model(images) \n        threshold = model.threshold(pred) \n        B, C = threshold.shape\n        for b in range(B):\n            one_val = threshold[b]\n            data['labels'] += [create_submission(one_val)]\n        data['image'] += ids\n    submit(data)\n    return data\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_submission(Tester(*load_model()), test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}