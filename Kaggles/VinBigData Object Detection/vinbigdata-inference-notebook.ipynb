{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# Import Necessary Dependencies\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport numpy as np \nimport pandas as pd \nimport copy\nimport math\nimport random\n\nimport tqdm.notebook as tqdm\n\n!pip install timm\nimport timm\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Process Data for Test-Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_2_class = {\n    0:'Aortic enlargement',\n    1:'Atelectasis',\n    2:'Calcification',\n    3:'Cardiomegaly',\n    4:'Consolidation',\n    5: 'ILD',\n    6: 'Infiltration',\n    7: 'Lung Opacity',\n    8: 'Nodule/Mass', \n    9: 'Other lesion',\n    10: 'Pleural effusion',\n    11: 'Pleural thickening',\n    12: 'Pneumothorax',\n    13: 'Pulmonary fibrosis',\n    14: 'No Finding'\n}\nNUM_OBJ_CLASSES = 14\nclass_2_idx = {}\nfor idx in idx_2_class:\n    class_2_idx[idx_2_class[idx]] = idx","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#HYPER PARAMETERS for Test Time\nBASE_PATH = \"../input/vinbigdata-original-image-dataset/vinbigdata/test/\"\nDATA_PATH = \"../input/vinbigdata-original-image-dataset/vinbigdata/test.csv\"\nCLASS_PATH = \"../input/efficientnetb0/BestLoss.pth\"\nANCHOR_PATH = \"../input/yolo-model/AnchorBoxes.pth\" # Trained Anchor Sizes\nOBJ_PATH = \"../input/yolo-model/FinalModel.pth\" # Trained OBJ Detector \nINPUT_SIZE = 2048\nGRID_SIZE_1 = 4\nGRID_SIZE_2 = 16\nGRID_SIZE_3 = 64\nGRID_SIZES = [GRID_SIZE_1, GRID_SIZE_2, GRID_SIZE_3]\n# Defone\nNUM_GRIDS = len(GRID_SIZES)\nANCHOR_BOXES = torch.load(ANCHOR_PATH)\nNUM_ANCHORS = 5 # 5 Anchors per scale\nTEST_BATCH_SIZE = 24","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Augmentations\ntest_transforms = A.Compose([\n    A.Resize(INPUT_SIZE, INPUT_SIZE),\n    A.Normalize(),\n    ToTensorV2()\n])","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(DATA_PATH)\ndf = df.image_id","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, base_path, dataframe, transforms):\n        super().__init__()\n        self.base_path = base_path\n        self.dataframe = dataframe\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.dataframe)\n    def __getitem__(self, idx):\n        image_id = self.dataframe[idx]\n        image_path = self.base_path + str(image_id) + \".jpg\"\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        \n        image = self.transforms(image = image)['image']\n        return image, image_id","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = TestDataset(BASE_PATH, df, test_transforms)\nTestDataloader = torch.utils.data.DataLoader(dataset, batch_size = TEST_BATCH_SIZE, worker_init_fn = seed_worker)","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load in Both Models(class Head and OBJ Head)"},{"metadata":{},"cell_type":"markdown","source":"Classification Head"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size, padding = padding, groups = groups)\n        self.SILU = nn.SiLU(inplace = True)\n        self.bn = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn(self.SILU(self.conv(x)))","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvSqueezeExcite(nn.Module):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.Squeeze = nn.Conv2d(self.in_features, self.inner_features, kernel_size = 1)\n        self.Excited = nn.Conv2d(self.inner_features, self.in_features, kernel_size = 1)\n        self.SILU = nn.SiLU(inplace = True)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        squeezed = self.SILU(self.Squeeze(x))\n        excited = torch.sigmoid(self.Excited(squeezed))\n        return excited * x","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DownSampleConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, stride, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size, padding = padding, stride = stride, groups = groups)\n        self.SiLU = nn.SiLU(inplace = True)\n        self.bn = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn(self.SiLU(self.conv(x)))","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RegularSE(nn.Module):\n    '''\n    Normal Squeeze and Excitation Block \n    '''\n    def __init__(self, in_features, squeezed_features):\n        super().__init__()\n        self.in_features = in_features\n        self.squeezed_features = squeezed_features\n        self.Squeeze = nn.Linear(self.in_features, self.squeezed_features)\n        self.act1 = nn.SiLU(inplace = True)\n        self.Expand = nn.Linear(self.squeezed_features, self.in_features)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        # Max Pool over last 2 dims\n        max_pooled, _ = torch.max(x, dim = -1)\n        max_pooled, _ = torch.max(max_pooled, dim = -1) # (B, C)\n        # Squeeze and Excitation Network\n        squeezed = self.act1(self.Squeeze(max_pooled))\n        excited = torch.sigmoid(self.Expand(squeezed)).unsqueeze(-1).unsqueeze(-1)\n        return excited * x\n        ","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BottleNeck(nn.Module):\n    '''\n    Squeeze Excite Residual Block as Proposed in ResNet.\n    '''\n    def __init__(self, input_size, inner_size, device):\n        super().__init__()\n        self.device = device\n        self.input_size = input_size\n        self.inner_size = inner_size\n        self.Squeeze = ConvBlock(self.input_size, self.inner_size, 1, 0, 1)\n        self.Process = ConvBlock(self.inner_size, self.inner_size, 3, 1, 1)\n        self.Expand = ConvBlock(self.inner_size, self.input_size, 1, 0, 1)\n        self.SE = RegularSE(self.input_size, self.input_size // 16)\n        self.gamma = nn.Parameter(torch.zeros(1, device = self.device))\n    def forward(self, x):\n        squeezed = self.Squeeze(x)\n        processed = self.Process(squeezed)\n        expand = self.Expand(processed)\n        excited = self.SE(expand)\n        return self.gamma * excited + x","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DownConvolutionBlock(nn.Module):\n    '''\n    Uses very cheap operations to process and downconvolve the massive image\n    '''\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        self.in_features = 3\n        self.avgPool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = 2)\n        self.downConv = DownSampleConvBlock(3, 5, 3, 1, 2, 1) # (2048 -> 1024)\n        \n        self.downConv2 = DownSampleConvBlock(8, 16, 3, 1, 2, 1) # (1024 -> 512)\n        \n        self.downConv3 = DownSampleConvBlock(24, 64, 3, 1, 2, 1) # (512 -> 256)\n        self.process3 = nn.Sequential(*[\n            BottleNeck(64, 16, self.device) for i in range(3) # A little bit of processing\n        ])\n        \n        self.proj = nn.Sequential(*[\n            ConvBlock(64, 32, 1, 0, 1),\n            ConvBlock(32, 3, 1, 0, 1)])\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n        \n    def forward(self, x):\n        '''\n        Initial DownConvolution\n        x: Tensor(B, 3, 2048, 2048)\n        '''\n        B, _, _, _ = x.shape\n        interpolated = F.interpolate(x, (256, 256), mode = 'bilinear')\n        avgPool = self.avgPool(x) # (B, 3, 1024, 1024)\n        downConv = self.downConv(x) # (B, 5, 1024, 1024)\n        # Concatenate Features\n        concatted = torch.cat([downConv, avgPool], dim = 1) # (B, 8, 1024, 1024)\n        # DownConv again \n        avgPool2 = self.avgPool(concatted) # (B, 8, 512, 512)\n        downConv2 = self.downConv2(concatted) # (B, 32, 512, 512)\n        concatted2 = torch.cat([downConv2, avgPool2], dim = 1) # (B, 40, 512, 512)\n        # Conv Stride a Few times\n        conv3 = self.process3(self.downConv3(concatted2)) # (B, 64, 256, 256)  \n        proj = self.proj(conv3) # (B, 64, 128, 128)\n        return proj * self.gamma + interpolated","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InvertedResidualBlock(nn.Module):\n    def __init__(self, in_features, inner_features, device):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.device = device\n        self.expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n        self.depthwise = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n        self.SE = RegularSE(self.inner_features, self.inner_features // 16)\n        self.squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        expanded = self.expand(x)\n        depthwise = self.depthwise(expanded)\n        se = self.SE(depthwise)\n        squeezed = self.squeeze(se)\n        return self.gamma * squeezed + x","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransposedConvBlock(nn.Module):\n    '''\n    Same as ConvBlock, but now allows for transposed convolution \n    '''\n    def __init__(self, in_features, out_features, kernel_size, output_padding, stride):\n        super().__init__()\n        self.convT = nn.ConvTranspose2d(in_features, out_features, kernel_size = kernel_size, output_padding = output_padding, stride = stride)\n        self.act1 = nn.SiLU(inplace = True)\n        self.bn1 = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn1(self.act1(self.convT(x)))","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedEfficientNetStudent(nn.Module):\n    '''\n    Student uses Down Convolutional Block to quickly downsample super large images.\n    '''\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = True\n    def __init__(self, num_classes, device, model_name = 'efficientnet_b3_pruned', drop_prob = 0.0, classification = True):\n        super().__init__()\n        self.classification = classification\n        self.num_classes = num_classes \n        self.device = device\n        self.model_name = model_name\n        self.drop_prob = drop_prob\n        self.model = timm.create_model(self.model_name, pretrained = False)\n        \n        # Extract Layers\n        self.downsampled = DownConvolutionBlock(self.device)\n        self.conv1 = self.model.conv_stem\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        \n        self.block0 = self.model.blocks[0]\n        self.block1 = self.model.blocks[1]\n        self.block2 = self.model.blocks[2]\n        self.block3 = self.model.blocks[3]\n        self.block4 = self.model.blocks[4]\n        self.block5 = self.model.blocks[5]\n        self.block6 = self.model.blocks[6]\n        \n        # Custom Layers\n        self.Attention1 = RegularSE(12, 4)\n        self.Attention2 = RegularSE(40, 16)\n        self.Attention3 = RegularSE(120, 32)\n        self.Attention4 = RegularSE(384, 64)\n        # Freeze Initial Layers\n        self.freeze(self.downsampled)\n        self.freeze(self.conv1)\n        self.freeze(self.bn1)\n        \n        self.freeze(self.block0)\n        self.freeze(self.block1)\n        self.freeze(self.block2)\n        self.freeze(self.block3)\n        self.freeze(self.block4)\n        self.freeze(self.Attention1)\n        self.freeze(self.Attention2)\n        self.freeze(self.Attention3)\n        \n        \n        self.layer4 = nn.Sequential(*[\n            DownSampleConvBlock(384, 384, 5, 2, 2, 384), # (B, 320, 4, 4)\n            ConvBlock(384, 768, 1, 0, 1)] + # (B, 768, 4, 4)\n        [\n            InvertedResidualBlock(768, 1536, self.device) for i in range(5)\n         \n        ]\n        )\n        self.conv2 = ConvBlock(768, 1536, 1, 0, 1)\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(1536, self.num_classes)\n    def forward(self, x):\n        '''\n        x: Tensor(B, 3, 320, 320)\n        '''\n        downsampled = self.downsampled(x) # (B, 3, 256, 256)\n        conv1 = self.bn1(self.act1(self.conv1(downsampled))) # (B, 32, 256, 256)\n        # Extract Features\n        block0 = self.block0(conv1)\n        block1 = self.block1(block0) # (B, 24, 64, 64) \n        attention1 = self.Attention1(block1)\n        \n        block2 = self.block2(attention1) # (B, 40, 32, 32)\n        attention2 = self.Attention2(block2)\n        \n        block3 = self.block3(attention2)\n        block4 = self.block4(block3) # (B, 112, 16, 16)\n        attention3= self.Attention3(block4)\n        \n        block5 = self.block5(attention3)\n        block6 = self.block6(block5) # (B, 320, 8, 8)\n        attention4 = self.Attention4(block6)\n        # Custom Layer 4\n        layer4 = self.layer4(attention4) # (B, 512, 4, 4)\n        if self.classification:\n            conv2 = self.conv2(layer4)\n            avg = torch.squeeze(self.dropout(self.global_avg(conv2)))\n            return self.Linear(avg)\n        return attention1, attention3, layer4","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassificationHead(nn.Module):\n    def __init__(self, device, file_path):\n        super().__init__()\n        self.file_path = file_path\n        self.num_classes = 1 # Binary Classification\n        self.device = device\n        self.model = ModifiedEfficientNetStudent(self.num_classes, self.device) \n        self.load_state_dict(torch.load(self.file_path, map_location = self.device))\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            pred = torch.sigmoid(self.model(x)) \n            # Round to 1 or 0\n            ones = pred >= 0.5\n            pred[:, :] = 0\n            pred[ones] = 1\n            return pred","execution_count":64,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OBJ Head"},{"metadata":{"trusted":true},"cell_type":"code","source":"class YOLOv3Config(nn.Module):\n    '''\n    Key functions needed for YOLOv3\n    \n    Note that since most of the operations involve Bounding Boxes, which varies in batches, all of this is not batched.\n    '''\n    def __init__(self, input_size, NUM_ANCHORS, NUM_GRIDS, device):\n        super().__init__()\n        self.input_size = input_size\n        self.NUM_ANCHORS = NUM_ANCHORS\n        self.NUM_GRIDS = NUM_GRIDS\n        self.device = device\n    def forward(self, GRID_SIZES, ANCHOR_BOXES):\n        '''\n        input: (image, GRID_SIZES, anchor_boxes)\n        Grid_SIZES: list of Grid Sizes\n        High Level function that generates a grid and anchor boxes for you.\n        '''\n        grids = []\n        anchors = []\n        for GRID_SIZE in GRID_SIZES:\n            # Generate Anchor Boxes and Grid Cells \n            grid = self.generate_grid(GRID_SIZE) # (S, S, 4)\n            anchor_boxes = self.generate_anchors(grid, ANCHOR_BOXES, GRID_SIZE) # (NUM_ANCHORS, S, S, 4) \n            grids += [grid]\n            anchors += [anchor_boxes]\n        return grids, anchors\n    def forward_GT(self, GRIDS_SIZES, ANCHOR_BOXES, GTS):\n        '''\n        GTS: (N, 5)\n        '''\n        grids, anchors = self(GRIDS_SIZES, ANCHOR_BOXES)\n        scales = []\n        for i in range(len(grids)):\n            grid = grids[i]\n            anchor = anchors[i]\n            indices, GT = self._assign_bbox(GTS, anchor)\n            # Convert the GT's back to cxcywh model\n            GT_classes = GT[:, -1].unsqueeze(-1)\n            GT = self._VOC2YOLO(GT)\n            GT = torch.cat([GT, GT_classes], dim = -1)\n            \n            if len(list(indices.shape)) == 1:\n                # Empty bounding boxes\n                scales += [(grid, anchor, None, None, GT)]\n            else:\n                GT_deltas = self._find_bbox_scale(GT, indices, anchor)\n                scales += [(grid, anchor, indices, GT_deltas, GT)]\n        return scales\n    def _lookup_anchors(self, anchors, anchor_idx):\n        '''\n        Helps to lookup the anchors needed given anchor indices\n        anchors(NUM_ANCHORS, GRID_SIZE, GRID_SIZE, 4)\n        anchor_idx: Tensor(N, 3)\n        '''\n        x_coords = anchor_idx[:, 0]\n        y_coords = anchor_idx[:, 1]\n        type_idx = anchor_idx[:, 2]\n        return anchors[type_idx, x_coords, y_coords, :]\n        \n    def _find_bbox_scale(self, GT_BBOX, anchor_box_idx, anchor_boxes):\n        '''\n        Determines what cx, cy, w, and h are the GT to map from anchor box to Ground Truth \n        GT_BBOX: (N, 5)\n        anchor_box_idx: Tensor(N, 3), where indices are expressed as (x, y, type)\n        anchor_boxes: Tensor(NUM_ANCHORS, S, S, 4)\n        '''\n        GT_classes = GT_BBOX[:, -1]\n        GT_BBOX = GT_BBOX[:, :-1]\n        Anchors_Needed = self._lookup_anchors(anchor_boxes, anchor_box_idx)\n        '''\n        Recall the change parameter\n        x prime = x + delta x\n        y prime = y + delta y\n        w prime = w * e^delta w\n        h prime = h * e^delta h\n        Thus,\n        delta x= x prime - x\n        delta y = y prime - y\n        delta w = ln(wprime / w)\n        delta h = ln(hprime / h) \n        '''\n        delta_x = GT_BBOX[:, 0] - Anchors_Needed[:, 0]\n        delta_y = GT_BBOX[:, 1] - Anchors_Needed[:, 1]\n        delta_w = torch.log(GT_BBOX[:, 2] / Anchors_Needed[:, 2])\n        delta_h = torch.log(GT_BBOX[:, 3] / Anchors_Needed[:, 3])\n        GroundTruthDeltas = torch.cat([delta_x.unsqueeze(-1), delta_y.unsqueeze(-1), delta_w.unsqueeze(-1), delta_h.unsqueeze(-1), GT_classes.unsqueeze(-1)], dim = -1)\n        return GroundTruthDeltas\n    def _find_x_and_y(self, GT_BBOXES, anchors_boxes):\n        '''\n        Computes the x and y index needed in the anchor boxes\n        GT_BBOXES: (N, 5)\n        Anchor_Boxes: (NUM_ANCHORS, GRID_SIZE, GRID_SIZE, 4) \n        or: (NUM_ANCHORS, Columns, Rows, 4)\n        '''\n        _, GRID_SIZE, _, _ = anchors_boxes.shape\n        cell_size = self.input_size / GRID_SIZE # Size of each cell(cell_size, cell_size)\n        N, _ = GT_BBOXES.shape\n        indices = []\n        for n in range(N):\n            bbox = GT_BBOXES[n, :-1] # (4)\n            indices += [(bbox[0] // cell_size, bbox[1] // cell_size)]\n        return torch.tensor(indices, device = GT_BBOXES.device)\n    def convert_image(self, images):\n        images = images.to(torch.float32) / 255.0\n        return images\n    def _assign_bbox(self, GT_BBOXES, anchor_boxes):\n        '''\n        returns the index associated with Each BBoxes.\n        GT_BBOXES: (N, 5), the Ground Truth Bboxes\n        Anchor_Boxes: (NUM_ANCHORS, GRID_SIZE, GRID_SIZE, 4), a grid of any scale \n        '''\n        _, GRID_SIZE, _, _ = anchor_boxes.shape\n        x_y_coord = self._find_x_and_y(GT_BBOXES, anchor_boxes).to(torch.long) # (N, 2) (Column, Row)\n        columns = x_y_coord[:, 0]\n        rows = x_y_coord[:, 1] \n        \n        potential_anchors = anchor_boxes[:, columns, rows, :] # (NUM_ANCHORS, N, 4)\n        # GT_BBOXES: (N, 5)\n        GT_classes = GT_BBOXES[:, -1]\n        GT_BBOXES = GT_BBOXES[:, :-1] # (N, 4)\n        # Convert to VOC Coords\n        GT_BBOXES = torchvision.ops.box_convert(GT_BBOXES, in_fmt = 'cxcywh', out_fmt = 'xyxy')\n        potential_anchors = torchvision.ops.box_convert(potential_anchors, in_fmt = 'cxcywh', out_fmt = 'xyxy')\n        # Select Largest IOU\n        N, _ = GT_BBOXES.shape\n        selected_box_idx = []\n        kept_GT = []\n        kept_GT_class= []\n        for n in range(N):\n            GT = GT_BBOXES[n, :].unsqueeze(0) # (1, 4)\n            potential = potential_anchors[:, n, :] # (N, 4)\n            iou = torchvision.ops.box_iou(GT, potential)\n            max_iou, max_idx = torch.max(iou, dim = 1)\n            if max_iou.item() >= 0.2: # We say that 0.2 Threshold for GT_Boxes \n                selected_box_idx += [(columns[n], rows[n], max_idx.item())]\n                kept_GT += [GT]\n                kept_GT_class += [GT_classes[n].unsqueeze(0)]\n            else:\n                pass\n                #print(f\"WARNING: GT has no Adequate Bounding Box. IOU: {max_iou.item()}, GT: {GT}, cur_GR: {GRID_SIZE}\")\n        selected_box_idx = torch.tensor(selected_box_idx, device = anchor_boxes.device)\n        if len(kept_GT) == 0:\n            return selected_box_idx, torch.zeros((0, 5), device = self.device)\n        kept_GT = torch.cat(kept_GT)\n        kept_GT_class = torch.cat(kept_GT_class).unsqueeze(-1)\n        return selected_box_idx, torch.cat([kept_GT, kept_GT_class], dim = -1)\n    def scale_up(self, bboxes):\n        return bboxes * self.input_size\n    def scale_down(self, bboxes):\n        return bboxes / self.input_size\n    def apply_GT_deltas(self, bboxes, deltas):\n        '''\n        bboxes: Tensors(N, 5)\n        deltas: Tensors(N, 4)\n        '''\n        x_bbox = bboxes[:, 0]\n        y_bbox = bboxes[:, 1]\n        w_bbox = bboxes[:, 2]\n        h_bbox = bboxes[:, 3]\n        \n        delta_x = deltas[:, 0]\n        delta_y = deltas[:, 1]\n        delta_w = deltas[:, 2]\n        delta_h = deltas[:, 3]\n        \n        x_prime = x_bbox + delta_x\n        y_prime = y_bbox + delta_y\n        w_prime = w_bbox * torch.exp(delta_w)\n        h_prime = h_bbox * torch.exp(delta_h)\n        \n        new_bbox = torch.cat([x_prime.unsqueeze(-1), y_prime.unsqueeze(-1), w_prime.unsqueeze(-1), h_prime.unsqueeze(-1), bboxes[:,-1].unsqueeze(-1)], axis = 1)\n        return new_bbox\n    \n    def visualize_bbox(self, image, bboxes):\n        '''\n        Visualizes bounding boxes on top of an image\n        image: Tensor(3, H, W)\n        bboxes: (N, 5), in YOLO coordinates\n        '''\n        VOC = self._YOLO2XYXY(bboxes) # (N, 4)\n        fig, ax = plt.subplots()\n        N, _ = VOC.shape\n        # Display the image\n        ax.imshow(image.transpose(0, 1).transpose(1, 2))\n        for n in range(N):\n            bbox = VOC[n, :]\n            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth = 1, alpha = 1, fill = False)\n            ax.add_patch(rect);\n        plt.show();\n    \n    def visualize_grid(self, image, grid):\n        '''\n        Visualizes one image and its grid.\n        Plots Grid as dots, and not bbox \n        image: Tensor(3, H, W)\n        grid: Tensor(N, 4), all bounding boxes\n        '''\n        N, _ = grid.shape \n        plt.imshow(image.transpose(0, 1).transpose(1, 2))\n        for n in range(N):\n            bbox = grid[n, :] \n            plt.scatter([bbox[0]], [bbox[1]])\n        plt.show();\n    def generate_anchors(self, grid, anchor_boxes, GRID_SIZE):\n        '''\n        Generates anchor boxes, in the form of (cx, cy, w, h) \n        grid: (S, S, 4), set of grid centers\n        anchor_boxes: List of Anchor sizes, all a tuple(w, h)\n        grid_size: The size of each grid cell(The scale to use)\n        '''\n        # Compute the grid size\n        grid_size = self.input_size / GRID_SIZE\n        anchors = []\n        C, R, _ = grid.shape\n        for width in anchor_boxes:\n            anchor_set = []\n            for c in range(C):\n                anchor_rows = []\n                for r in range(R):\n                    anchor = copy.deepcopy(grid[c, r, :]) # (4)\n                    anchor[2] = width * grid_size \n                    anchor[3] = grid_size\n                    anchor_rows += [anchor]\n                anchor_set += [torch.stack(anchor_rows)]\n            anchors += [torch.stack(anchor_set)]\n        return torch.stack(anchors)\n    def _batch_tensors(self, tensor, batch_size):\n        '''\n        Repeats a tensor across the batch dimension\n        tensor(...), outputs: Tensor(B, ...)\n        '''\n        repeated = torch.repeat_interleave(tensor, batch_size, axis = 0)\n        return repeated\n    def generate_grid(self, grid_size):\n        '''\n        Creates a Grid for the object detector(grid cells)\n        Returns a grid of size(S, S, 2) # (cx, cy, w, h) for every grid cell.\n        '''\n        grid_box_width = (1 / grid_size) * self.input_size\n        num_grid_width = self.input_size // grid_size\n        grid_box_height = (1 / grid_size) * self.input_size\n        num_grid_height = self.input_size // grid_size\n        \n        grid_centers = []\n        for w in range(0, grid_size):\n            for h in range(0, grid_size):\n                cx = w * grid_box_width + grid_box_width / 2\n                cy = h * grid_box_height + grid_box_height / 2\n                grid_centers += [(cx, cy, grid_box_width, grid_box_height)]\n        grid_centers = torch.tensor(grid_centers, device = self.device) \n        grid_centers = grid_centers.view(grid_size, grid_size, -1) # (Column, Rows, 4)\n        return grid_centers\n    def _convert_bbox(self, bbox):\n        '''\n        Inflates a bounding box with 4 values(N, 4) to 5 where all classes are just 0(Helpful for testing)\n        '''\n        N, _ = bbox.shape\n        vals = torch.zeros((N, 5))\n        vals[:, :-1] = bbox\n        return vals;\n    def _YOLO2XYXY(self, YOLO):\n        '''\n        YOLO: Tensor(N, 5)\n        '''\n        YOLO = YOLO[:, :-1] # (N, 4)\n        XYXY = torchvision.ops.box_convert(YOLO, in_fmt = 'cxcywh', out_fmt = 'xywh')\n        return XYXY\n    def _VOC2YOLO(self, VOC):\n        '''\n        Helper Function to Convert a VOC based Bounding Box into a YOLO based\n        VOC: Tensor(N, 5)\n        '''\n        VOC = VOC[:, :-1] # (N, 4) \n        YOLO = torchvision.ops.box_convert(VOC, in_fmt = 'xyxy', out_fmt = 'cxcywh')\n        return YOLO\n    def _YOLO2VOC(self, YOLO):\n        '''\n        Converts a YOLO based bounding box to a VOC based bounding box.\n        YOLO: Tensor(N, 5)\n        '''\n        YOLO = YOLO[:, :-1] # (N, 4)\n        VOC = torchvision.ops.box_convert(YOLO, in_fmt = 'cxcywh', out_fmt = 'xyxy')\n        return VOC","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class YOLOv3LossConfig(nn.Module):\n    '''\n    Helper Class that configurates the loss\n    '''\n    def __init__(self, input_size, anchor_sizes, grid_sizes, num_obj_classes, device):\n        super().__init__()\n        self.input_size = input_size\n        self.anchor_sizes = anchor_sizes\n        self.grid_sizes = grid_sizes\n        self.num_obj_classes = num_obj_classes \n        self.device = device       \n        \n        self.num_anchors = len(self.anchor_sizes)\n        self.num_grids = len(self.grid_sizes)\n        \n        self.Yconfig = YOLOv3Config(self.input_size, self.num_anchors, self.num_grids, self.device)\n        # Variety of Loss Functions\n        self.obj_crit = nn.BCEWithLogitsLoss()\n        self.MSELoss = nn.MSELoss()\n        self.cls_crit = nn.CrossEntropyLoss()\n        # Weights\n        self.obj_loss = 0.1\n        self.mse_loss = 2\n        self.cls_loss = 1 \n    def _not_selected(self, bounding_boxes, indices):\n        '''\n        Bounding_Boxes: (NUM_ANCHORS, S_Scale, S_Scale, 18)\n        indices: (N, 3) \n        '''\n        _, S_scale, _, _ = bounding_boxes.shape\n        x_coords = indices[:, 0]\n        y_coords = indices[:, 1]\n        anchor_idx = indices[:, 2]\n        selected_vals = [(x_coords[i].item(), y_coords[i].item(), anchor_idx[i].item()) for i in range(x_coords.shape[0])] \n        all_x_coords = [i for i in range(S_scale)] \n        all_y_coords = [i for i in range(S_scale)]\n        all_anchor_idx = [i for i in range(self.num_anchors)]\n        all_possible_combinations = list(itertools.product(all_x_coords, all_y_coords, all_anchor_idx))\n        \n        not_selected_combinations = torch.tensor([combo for combo in all_possible_combinations if combo not in selected_vals], device = self.device)\n        return not_selected_combinations\n        \n        \n    def _loss(self, bbox_scale, GT_scale):\n        '''\n        Computes the Loss for a given BBox Scale\n        bbox_scale; Tensor(NUM_ANCHORS(5 + C), S_scale, S_scale)\n        '''\n        _, S_Scale, _ = bbox_scale.shape\n        grid, anchor, index, GT_delta, GT_bbox= GT_scale\n        if index == None:\n            return 0\n        # Select the GT Anchor\n        selected_anchor = self.Yconfig._lookup_anchors(anchor, index)\n        # split out the bbox_scale into (3, 5 + C) \n        bbox_scale = bbox_scale.view(self.num_anchors, 5 + self.num_obj_classes, S_Scale, S_Scale);\n        bbox_scale = bbox_scale.transpose(1, 2).transpose(2, 3)\n        # Select BBoxes\n        selected_bboxes = self.Yconfig._lookup_anchors(bbox_scale, index)\n        not_selected_bboxes_inds = self._not_selected(bbox_scale, index)\n        not_selected_bboxes= self.Yconfig._lookup_anchors(bbox_scale, not_selected_bboxes_inds)\n        # Selected Bounding Box Losses\n        selected_N, selected_C = selected_bboxes.shape\n        not_selected_N, _ = not_selected_bboxes.shape\n        \n        # Extract CLS, OBJ, and Deltas\n        class_selected = selected_bboxes[:, selected_C - self.num_obj_classes:]\n        bbox_selected = selected_bboxes[:, :selected_C - self.num_obj_classes] # (cx, cy, w, h) shifts\n        deltas_selected = bbox_selected[:, :-1]\n        obj_selected = bbox_selected[:, -1]\n        \n        # Extract CLS and Deltas\n        deltas_GT = GT_delta[:, :-1]\n        class_GT = GT_delta[:, -1]\n        \n        obj_loss_selected = self.obj_crit(obj_selected, torch.ones_like(obj_selected, device = obj_selected.device)) * self.obj_loss\n        cls_loss_selected = self.cls_crit(class_selected, class_GT.to(torch.long)) * self.cls_loss\n        delta_loss_selected = self.MSELoss(deltas_selected, deltas_GT) * self.mse_loss\n        selected_loss = obj_loss_selected + cls_loss_selected + delta_loss_selected\n        \n        # Extract OBJ from not selected\n        obj_not_selected = not_selected_bboxes[:, 4]\n        obj_loss_not = self.obj_crit(obj_not_selected, torch.zeros_like(obj_not_selected, device = obj_not_selected.device)) * self.obj_loss\n        not_selected_loss= obj_loss_not\n        # Add Losses\n        loss = selected_loss + not_selected_loss\n        return loss\n        \n    def forward(self, bbox_small, bbox_medium, bbox_large, GT_bboxes):\n        '''\n        bbox_small: Tensor(anchors(5 + C), S_small, S_small)\n        bbox_medium: Tensor(anchors(5 + C), S_medium, S_medium)\n        bbox_large: Tensor(anchors(5 + C), S_large, S_large)\n        GT_bboxes: Tensor(N, 5)\n        \n        Note: This operation cannot be batched.\n        '''\n        N, _, _ = bbox_small.shape\n        # Compute Grids, and Necessary fucntions\n        GT_large, GT_medium, GT_small = self.Yconfig.forward_GT(self.grid_sizes, self.anchor_sizes, GT_bboxes)\n        # Compute Loss small\n        loss_small = self._loss(bbox_small, GT_small)\n        # Compute Loss medium\n        loss_medium = self._loss(bbox_medium, GT_medium)\n        # Compute Loss Large\n        loss_large = self._loss(bbox_large, GT_large)\n        # Add Losses(This will be super large to backprop through)\n        total_loss = (loss_small + loss_medium + loss_large) / N\n        return total_loss","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class YOLOv3Inference(nn.Module):\n    '''\n    Helper Class For Inference with YOLO\n    '''\n    def __init__(self, input_size, anchor_boxes, grid_sizes, num_obj_classes, device, obj_thresh = 0.0, iou_thresh = 0.5):\n        super().__init__()\n        self.device = device\n        self.obj_thresh = obj_thresh\n        self.iou_thresh = iou_thresh\n        self.input_size = input_size\n        self.anchor_boxes = anchor_boxes \n        self.grid_sizes = grid_sizes\n        self.num_obj_classes = num_obj_classes\n        \n        self.num_anchor_boxes = len(self.anchor_boxes)\n        self.num_grid_sizes = len(self.grid_sizes)\n        \n        self.Yconfig = YOLOv3Config(self.input_size, self.num_anchor_boxes, self.num_grid_sizes, self.device)\n    def process_size(self, anchor_size, bbox_shape):\n        '''\n        bbox_shape: Tensor(B, ANCHOR_BOXES(5 + C), S_size, S_size)\n        anchor_sizes: Tensor(ANCHOR_BOXES, S_size, S_size, 4)\n        '''\n        B, C, S_size, _ = bbox_shape.shape\n        bbox_shape = bbox_shape.view(B, self.num_anchor_boxes, 5 + self.num_obj_classes, S_size, S_size)\n        # Extract Out Classes\n        classes = F.softmax(bbox_shape[:, :, 5:, :, :]) # (B, NUM_ANCHORS, self.num_obj_classes, S_size, S_size)\n        # Argmax over classes\n        _, classes_ind = torch.max(classes, dim = 2) # (B, NUM_ANCHORS, S_size, S_size)\n        \n        bboxes = bbox_shape[:, :, :5, :, :] # (B, NUM_ANCHORS, 5, S_size, S_size)\n        \n        deltas = bboxes[:, :, :-1, :, :] # (B, NUM_ANCHORS, 4, S_size, S_size)\n       \n        # Extract Out (cx, cy, w, h) shifts \n        cx_delta = deltas[:, :, 0, :, :]\n        cy_delta = deltas[:, :, 1, :, :]\n        w_delta = deltas[:, :, 2, :, :]\n        h_delta = deltas[:, :, 3, :, :]\n        \n        obj_scores = torch.sigmoid(bboxes[:, :, -1, :, :]) # (B, NUM_ANCHORS, S_size, S_size)\n        \n        threshold = obj_scores >= self.obj_thresh # (B, NUM_ANCHORS, S_size, S_size)\n        kept_sorted_bboxes = []\n        for b in range(B):\n            threshold_batch = threshold[b] # (NUM_ANCHORS, S_size, S_size)\n            cx_batch = cx_delta[b]\n            cy_batch = cy_delta[b]\n            w_batch = w_delta[b]\n            h_batch = h_delta[b]\n            classes_batch = classes_ind[b]\n            obj_scores_batch = obj_scores[b] \n            anchor_selected = anchor_size[threshold_batch, :] # (N, 4)\n            # Inflate anchors\n            anchor_selected = self.Yconfig._convert_bbox(anchor_selected) # (N, 5) \n            anchor_selected = anchor_selected.to(self.device)\n            kept_cx = cx_batch[threshold_batch]\n            kept_cy = cy_batch[threshold_batch]\n            kept_w = w_batch[threshold_batch]\n            kept_h = h_batch[threshold_batch]\n            kept_deltas = torch.stack([kept_cx, kept_cy, kept_w, kept_h], dim = -1) # (N, 4)\n            \n            kept_bbox = self.Yconfig.apply_GT_deltas(anchor_selected, kept_deltas)[:, :-1] # (N, 4)\n            kept_classes = classes_batch[threshold_batch]\n            kept_obj_scores = obj_scores_batch[threshold_batch]\n            \n            # NMS Threshold Current Bounding Boxes\n            kept_idx = torchvision.ops.batched_nms(kept_bbox, kept_obj_scores, kept_classes, iou_threshold = self.iou_thresh)\n            kept_bbox = kept_bbox[kept_idx]\n            kept_classes = kept_classes[kept_idx].unsqueeze(-1)\n            kept_obj_scores = kept_obj_scores[kept_idx].unsqueeze(-1)\n            kept_bbox = torch.cat([kept_bbox, kept_obj_scores, kept_classes], dim = -1) # (N, 6)\n            kept_sorted_bboxes += [kept_bbox]\n        return torch.stack(kept_sorted_bboxes)\n\n    def forward(self, bbox_small, bbox_medium, bbox_large):\n        '''\n        grid_small: Tensor(B, NUM_ANCHORS(5 + C), S_small, S_small)\n        grid_medium: Tensor(B, NUM_ANCHORS(5 + C), S_medium, S_medium)\n        grid_large: Tensor(B, NUM_ANCHORS(5 + C), S_large, S_large)\n        '''\n        B, C, _, S_small = bbox_small.shape\n        _, _, _, S_medium = bbox_medium.shape\n        _, _, _, S_large = bbox_large.shape\n        grids, anchors = self.Yconfig(self.grid_sizes, self.anchor_boxes)\n        grid_small, grid_medium, grid_large = grids\n        anchor_large, anchor_medium, anchor_small = anchors\n        # anchor_small: Tensor(NUM_ANCHORS,S_small, S_small, 4), anchor_medium: Tensor(NUM_ANCHORS, S_medium, S_medium, 4), anchor_large: Tensor(NUM_ANCHORS, S_large, S_large, 4)\n        # grid_small: Tensor(S_small, S_small, 4), grid_medium: Tenso(S_medium, S_medium, 4), grid_large: Tensor(S_large, S_large, 4)\n        # Load onto Device\n        grid_small = grid_small.to(self.device)\n        grid_medium = grid_medium.to(self.device)\n        grid_large = grid_large.to(self.device)\n        \n        anchor_large = anchor_large.to(self.device)\n        anchor_medium = anchor_medium.to(self.device)\n        anchor_small = anchor_small.to(self.device) \n        \n        small_selected = self.process_size(anchor_small, bbox_small) # (B, selected_small, 6)\n        medium_selected = self.process_size(anchor_medium, bbox_medium) # (B, selected_medium, 6)\n        large_selected = self.process_size(anchor_large, bbox_large) # (B, selected_large, 6)\n        # NMS Threshold Bounding Boxes\n        kept_bboxes = []\n        for b in range(B):\n            small_bboxes = small_selected[b] # (selected_small, 6)\n            medium_bboxes = medium_selected[b] # (selected_medium, 6)\n            large_bboxes = large_selected[b] # (selected_large, 6)\n            # Concatenate\n            bboxes = torch.cat([small_bboxes, medium_bboxes, large_bboxes], dim = 0) # (s + m + l, 6)\n            # Extract Class and OBJ\n            bbox = bboxes[:, :4]\n            obj = bboxes[:, 4]\n            classes = bboxes[:, -1] \n            # NMS Thresh \n            kept_bboxes_ind = torchvision.ops.batched_nms(bbox, obj, classes, iou_threshold = self.iou_thresh)\n            kept_bboxes += [bboxes[kept_bboxes_ind]]\n        return kept_bboxes","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassificationAlpha(nn.Module):\n    def __init__(self, device, file_path = None):\n        super().__init__()\n        self.file_path = file_path\n        self.num_classes = 1\n        self.device = device\n        self.model = ModifiedEfficientNetStudent(self.num_classes, self.device, drop_prob = 0.1, classification = False)\n        if file_path != None:\n            self.load_state_dict(torch.load(self.file_path, map_location = self.device))\n    def forward(self, x):\n        return self.model(x)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ObjectDetectorQT(nn.Module):\n    def __init__(self, in_features, lower_in_features, inner_features, num_obj_classes, device, num_anchors = 3):\n        super().__init__()\n        self.inner_features = inner_features\n        self.lower_in_features = lower_in_features\n        self.in_features = in_features\n        self.num_classes = num_obj_classes\n        self.num_anchors = num_anchors\n        self.device = device\n        \n        # Projection Layers to map embeddings to the correct size\n        self.proj = TransposedConvBlock(self.lower_in_features, self.in_features, 1, 1, 2)\n        self.proj2 = ConvBlock(2 * self.in_features, self.inner_features, 1, 0, 1)\n        \n        self.ConvPipeLine = nn.Sequential(*[\n            BottleNeck(self.inner_features, self.inner_features // 4, self.device) for i in range(6)\n        ])\n        \n        self.predict = nn.Conv2d(self.inner_features, self.num_anchors * (5 + self.num_classes), kernel_size = 1)\n    def forward(self, x, lower_x):\n        '''\n        x: Tensor(B, C, H, W), \n        lower_x: Tensor(B, C_prime, H_prime, W_prime), features from the previous Object Detector Layer\n        '''\n        # project lower_x into x\n        proj_x = self.proj(lower_x) # (B, in_features, H, W)\n        # Concatenate the features\n        concatenated = torch.cat([x, proj_x], dim = 1) # (B, self.in_features * 2, H, W) \n        # Project into ConvPipeLine\n        proj_conv = self.proj2(concatenated) \n        # Run through convPipeLine for features\n        features = self.ConvPipeLine(proj_conv)\n        return self.predict(features), features","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ObjectDetectorAlpha(nn.Module):\n    def __init__(self, in_features, inner_features, num_obj_classes, device, num_anchors = 3):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_classes = num_obj_classes\n        self.num_anchors = num_anchors\n        self.device = device\n        \n        self.proj = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n        \n        self.ConvPipeLine = nn.Sequential(*[\n            BottleNeck(self.inner_features, self.inner_features // 4, self.device) for i in range(6)\n        ])\n        \n        self.predict = nn.Conv2d(self.inner_features, self.num_anchors * (5 + self.num_classes), kernel_size = 1)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W), \n        '''\n        proj_conv = self.proj(x) \n        # Extract Features\n        features = self.ConvPipeLine(proj_conv)\n        return self.predict(features), features","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class YOLOQT(nn.Module):\n    def __init__(self, num_obj_classes, num_anchors, device, file_path = None):\n        super().__init__()\n        self.num_anchors = num_anchors\n        self.num_obj_classes = num_obj_classes\n        self.device = device\n        self.file_path = file_path\n        \n        self.feature_extractor = ClassificationAlpha(self.device, file_path = self.file_path)\n        \n        # Object Detector Layers\n        \n        self.obj_detect_1 = ObjectDetectorAlpha(768, 256, self.num_obj_classes, self.device, num_anchors = self.num_anchors)\n        self.obj_detect_2 = ObjectDetectorQT(248, 128, 256, self.num_obj_classes, self.device, num_anchors = self.num_anchors)\n        self.obj_detect_3 = ObjectDetectorQT(76, 128, 128, self.num_obj_classes, self.device, num_anchors = self.num_anchors)\n        \n        # Upsample and Further Processing Layers\n        \n        self.upsample_3 = TransposedConvBlock(768, 256, 1, 1, 2)\n        self.process_3 = BottleNeck(256, 32, self.device)\n        self.upsample3_5 = TransposedConvBlock(256, 128, 1, 1, 2)\n        self.process3_5 = BottleNeck(128, 16, self.device) \n        self.sequential_3 = nn.Sequential(*[\n            BottleNeck(248, 64, self.device) for i in range(3)\n        ])\n        \n        self.upsample_2 = TransposedConvBlock(248, 128, 1, 1, 2)\n        self.process_2 = BottleNeck(128, 32, self.device)\n    \n        self.upsample_1 = TransposedConvBlock(128, 64, 1, 1, 2)\n        self.process_1 = BottleNeck(64, 16, self.device)\n        \n        \n        \n        self.upsample_small = TransposedConvBlock(256, 128, 1, 1, 2)\n        self.process_small = BottleNeck(128, 32, self.device)\n        self.upsample_middle = TransposedConvBlock(256, 128, 1, 1, 2)\n        self.process_middle = BottleNeck(128, 32, self.device)\n        self.sequential_1 = nn.Sequential(*[\n            BottleNeck(76, 24, self.device) for i in range(3)\n        ])\n        \n    def forward(self, x):\n        layer2, layer3, layer4 = self.feature_extractor(x)\n        # Layer2: Tensor(B, 24, 64, 64)\n        # Layer3: Tensor(B, 112, 16, 16)\n        # Layer4: Tensor(B, 768, 4, 4) \n        # Make Large Predictions\n        bbox_pred_large, features_large = self.obj_detect_1(layer4)\n        # features_large: Tensor(B, 256, 4, 4)\n        # Upsample and Process Layer 4\n        upsampled_3 = self.upsample_3(layer4) # (B, 256, 8, 8)\n        processed_3 = self.process_3(upsampled_3) \n        upsampled_3 = self.upsample3_5(processed_3) # (B, 128, 16, 16)\n        processed_3 = self.process3_5(upsampled_3) # (B, 128, 16, 16)\n        \n        upsample_layer3 = self.upsample_small(features_large) # (B, 128, 8, 8)\n        processed_layer3 = self.process_small(upsample_layer3) # (B, 128, 8, 8)\n        # Concatenate\n        concat_3 = torch.cat([layer3, processed_3], dim = 1) # (B, 264, 16, 16)\n        sequential_3 = self.sequential_3(concat_3) # (B, 264, 16, 16)\n        # Object Detector 3(Medium Predictions)\n        bbox_pred_medium, features_medium = self.obj_detect_2(sequential_3, processed_layer3) \n        # Features_medium: Tensor(B, 256, 16, 16) \n        # Upsample and Process Layer 3\n        upsampled_2 = self.upsample_2(sequential_3) \n        processed_2 = self.process_2(upsampled_2) # (B, 128, 32, 32)\n        upsampled_1 = self.upsample_1(processed_2)\n        processed_1 = self.process_1(upsampled_1) # (B, 64, 64, 64)\n        # Concatenate\n        concat_1 = torch.cat([layer2, processed_1], dim = 1) # (B, 96, 64, 64) \n        sequential_1 = self.sequential_1(concat_1) # (B, 96, 64, 64) \n        \n        # Process Features Medium\n        upsample_medium = self.upsample_middle(features_medium)\n        processed_medium = self.process_middle(upsample_medium) # (B, 128, 32, 32)\n        # Object Detector 2(Small Predictions)\n        bbox_pred_small, features_small = self.obj_detect_3(sequential_1, processed_medium) \n        \n        return bbox_pred_small, bbox_pred_medium, bbox_pred_large\n        # Bbox_Small: Tensor(B, anchors(5 + C), S_small, S_small), BBox_Medium: Tensor(B, anchors(5 + C), S_medium, S_medium), Bbox_Large: Tensor(B, anchors(5 + C), S_large, S_large)\n        ","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class YOLOSolver(nn.Module):\n    def __init__(self, input_size, anchor_boxes, grid_sizes, num_obj_classes, device, file_path = None):\n        super().__init__()\n        self.file_path = file_path\n        self.input_size = input_size\n        self.anchor_boxes = anchor_boxes\n        self.grid_sizes = grid_sizes\n        self.num_obj_classes = num_obj_classes\n        self.device = device\n        self.num_anchors = len(anchor_boxes)\n        self.num_grids = len(self.grid_sizes)\n        \n        self.Yconfig = YOLOv3Config(self.input_size, self.num_anchors, self.num_grids, self.device)\n        self.lossConfig = YOLOv3LossConfig(self.input_size, self.anchor_boxes, self.grid_sizes, self.num_obj_classes, self.device)\n        self.YInference = YOLOv3Inference(self.input_size, self.anchor_boxes, self.grid_sizes, self.num_obj_classes, self.device)\n        \n        self.model = YOLOQT(self.num_obj_classes, self.num_anchors, self.device, file_path = self.file_path)\n    def forward(self, x):\n        '''\n        Runs a Test Time Run through the Object Detector.\n        '''\n        self.eval()\n        with torch.no_grad():\n            grid_small, grid_medium, grid_large = self.model(self.Yconfig.convert_image(x))\n            # Extract Bounding Boxes out and Threshold their values\n            selected_bounding_boxes = self.YInference(grid_small, grid_medium, grid_large)\n            return selected_bounding_boxes","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FullModelPredAlpha(nn.Module):\n    def __init__(self, input_size, anchor_boxes, grid_sizes, num_classes, device, file_path_obj, file_path_cls):\n        super().__init__()\n        self.device = device\n        self.OBJDetect = YOLOSolver(input_size, anchor_boxes, grid_sizes, num_classes, device, file_path_cls)\n        self.OBJDetect.load_state_dict(torch.load(file_path_obj, map_location = self.device))\n        \n        self.Classification = ClassificationHead(device, file_path_cls)\n    def forward(self, x):\n        '''\n        Performs Inference on Model\n        '''\n        self.eval()\n        B, _, _, _ = x.shape\n        output = [None for i in range(B)]\n        with torch.no_grad():\n            classification = torch.squeeze(self.Classification(x)) # (B)\n            bools = classification == 1\n            pred_bboxes= self.OBJDetect(x)\n            for idx in range(B):\n                if bools[idx]:\n                    # Estimated Bboxes Predicted\n                    bboxes = pred_bboxes[idx]\n                    output[idx] = bboxes\n        return output\n            ","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nsolver = FullModelPredAlpha(INPUT_SIZE, ANCHOR_BOXES, GRID_SIZES, NUM_OBJ_CLASSES, device, OBJ_PATH, CLASS_PATH)\nsolver.to(device)","execution_count":88,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create and Submit CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_submission_str(class_pred, x_min, x_max, y_min, y_max, confidence):\n    '''\n    Each Submission String appears in the format of: {class_pred confidence xmin ymin xmax ymax}\n    '''\n    return f\"{class_pred} {confidence} {x_min} {y_min} {x_max} {y_max}\"","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_submission(TestDataloader):\n    '''\n    Predicts on the Test DataLoader\n    '''\n    predictions = {\"image_id\": [], \"PredictionString\": []} \n    for images, image_ids in tqdm.tqdm(TestDataloader):\n        images = images.to(device)\n        bboxes = solver(images)\n        predictions['image_id'] += list(image_ids)\n        for i in bboxes:\n            if i == None:\n                predictions['PredictionString'] += ['14 1 0 0 1 1']\n            else:\n                N, _ = i.shape\n                pred_string = ''\n                for n in range(N):\n                    bbox = i[n, :] # (6)\n                    x_min, y_min, x_max, y_max, obj, CLS = bbox\n                    if pred_string == \"\":\n                        pred_string += prepare_submission_str(CLS, x_min, x_max, y_min, y_max, obj)\n                    else:\n                        pred_string += f\" {prepare_submission_str(CLS, x_min, x_max, y_min, y_max, obj)}\"\n    df = pd.DataFrame(predictions) \n    df = df.set_index('image_id')\n    df.to_csv(\"./submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_submission(TestDataloader)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}