{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"scrolled":true},"cell_type":"code","source":"%%capture\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n!pip install ensemble-boxes\n!pip install livelossplot\n!pip install timm\n!pip install mapcalc\nimport livelossplot\nimport copy\nimport random\n\nimport math\nfrom mapcalc import calculate_map\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport random\n\nimport tqdm.notebook as tqdm\n\nimport PIL\nimport os\nimport collections\nimport itertools\n\nimport torchvision\nimport torchvision.transforms as transforms\n\n\nimport albumentations as al\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\n\nimport torch\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nimport timm\n\nfrom sklearn.model_selection import train_test_split\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data \n\nMultiStage Training Strategy:\n- \nTwo Problems inside of the Data:\n- Multiple No Findings(Solution: Train a 2 Part classifier, one training on classes or no classes, other on bbox, the ones with no findings are separated out from the rest of the dataset)\n- Multiple Radiographer Readings: We will use NMS to threshold out the bounding boxes so limited duplicates occur).\n\nNo problems:\n- If there is no finding, all radiographers all say no findings\n\n"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"idx_2_class = {\n    0:'Aortic enlargement',\n    1:'Atelectasis',\n    2:'Calcification',\n    3:'Cardiomegaly',\n    4:'Consolidation',\n    5: 'ILD',\n    6: 'Infiltration',\n    7: 'Lung Opacity',\n    8: 'Nodule/Mass', \n    9: 'Other lesion',\n    10: 'Pleural effusion',\n    11: 'Pleural thickening',\n    12: 'Pneumothorax',\n    13: 'Pulmonary fibrosis',\n    14: 'No Finding'\n}\nNUM_OBJ_CLASSES = 14\nclass_2_idx = {}\nfor idx in idx_2_class:\n    class_2_idx[idx_2_class[idx]] = idx","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# HYPER PARAMETERS DEFINED HERE\nBATCH_SIZE = 24\nTEST_BATCH_SIZE = 24\nLR = 1e-4","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# CONSTANTS DEFINED HERE\nTRAIN_PATH = \"../input/vinbigdata-original-image-dataset/vinbigdata/train/\"\ntrain_csv = '../input/vinbigdata-original-image-dataset/vinbigdata/train.csv'\ntest_csv = '../input/vinbigdata-original-image-dataset/vinbigdata/test.csv'\ntrain_state_dict_path = '../input/efficientnetb0/BestLoss.pth'\ntrain_pd = pd.read_csv(train_csv)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class TrainObjectDetectionDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, unique_ids, transforms, train_path, iou_threshold = 0.5):\n        self.dataframe = self.parse(dataframe)\n        self.transforms = transforms\n        self.iou_threshold = iou_threshold\n        self.train_path = train_path\n        \n        self.unique_ids = self.clean_unique_ids(unique_ids) \n    def grab_all_bboxes(self):\n        '''\n        Retrieves all Bounding boxes as a list, used in K-means clustering.\n        '''\n        width_height_pairs = []\n        for row in tqdm.tqdm(self.dataframe.iterrows()):\n            row = row[1]\n            width_height_pairs += [((row['x_max'] - row['x_min']) / (row['y_max'] - row['y_min']))]\n        return width_height_pairs\n        \n    def clean_unique_ids(self, unique_ids):\n        '''\n        Removes all unique_ids that have class 14\n        '''\n        valid_ids = []\n        for id in unique_ids:\n            if id in self.dataframe.image_id.values:\n                valid_ids += [id]\n        return valid_ids\n    def cleanse_files(self, unique_ids):\n        '''\n        Cleanse all invalid files from the self.unique_ids, if any exist\n        '''\n        valid_id = []\n        all_files = os.listdir(self.train_path)\n        for id in unique_ids:\n            if id + '.jpg' in all_files:\n                valid_id += [id]\n        return valid_id\n    def parse(self, dataframe):\n        '''\n        Removes all entries with the class id 14(No Findings)\n        '''\n        indices_needed = dataframe.class_id.values != 14\n        return dataframe.loc[indices_needed]\n    def __len__(self):\n        return len(self.unique_ids)\n    def __getitem__(self, idx):\n        '''\n        Finds all unique bounding boxes related to a given image\n        \n        Returns YOLO based(cxcywh) bounding boxes that have been NMS thresholded to help prevent duplicate bounding boxes.\n        '''\n        image_index_needed = self.unique_ids[idx]\n        file_path = TRAIN_PATH + image_index_needed + '.jpg'\n\n        \n        # Read in Image\n        image_loaded = cv2.imread(file_path)\n        image_loaded = cv2.cvtColor(image_loaded, cv2.COLOR_BGR2RGB)\n        \n        # Locate All Images\n        image_ids = self.dataframe.image_id.values == image_index_needed\n        rows = self.dataframe.loc[image_ids]\n        assert len(rows) > 0, collections.Counter(image_ids)\n        # Load In the Bounding Boxes\n        bboxes = []\n        classes = []\n        conf_scores = []\n        width = 0\n        height = 0\n        for row in rows.iterrows():\n            row = row[1]\n            width = row['width']\n            height = row['height']\n            bboxes += [(row['x_min'], row['y_min'], row['x_max'], row['y_max'])]\n            classes += [float(row['class_id'])]\n            conf_scores += [1.0]\n        \n        bboxes = torch.tensor(bboxes)\n        classes = torch.tensor(classes)\n        conf_scores = torch.tensor(conf_scores)\n        \n        # NMS Thresh\n        vals = torchvision.ops.batched_nms(boxes = bboxes, scores = conf_scores, idxs = classes, iou_threshold = self.iou_threshold)\n        \n        kept_bboxes = bboxes[vals]\n        kept_classes = torch.unsqueeze(classes[vals], -1)\n        kept_conf_scores = conf_scores[vals]\n        \n        # Augment Image and Bounding box using albumentations \n        augmentation = self.transforms(image = image_loaded, bboxes = kept_bboxes, classes = kept_classes)\n        aug_bboxes = torch.tensor(augmentation['bboxes'])\n        aug_image = torch.tensor(augmentation['image'])\n        aug_classes = torch.tensor(augmentation['classes']).unsqueeze(-1)\n\n        # Convert Pascal_VOC \n        YOLO_bboxes = torchvision.ops.box_convert(boxes = aug_bboxes, in_fmt = 'xyxy', out_fmt = 'cxcywh')\n        # concatenate classes \n        concat_bboxes = torch.cat([YOLO_bboxes, aug_classes], dim = -1)\n        \n        return aug_image, concat_bboxes.float()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Augmentations to use for object detection:\nIMAGE_SIZE = 2048\nTRAIN_AUGMENTATIONS_OBJ = al.Compose([\n    al.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, scale = (0.9, 0.9)),\n    al.HorizontalFlip(p=0.5),\n    al.HueSaturationValue(p=0.2, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n    al.ShiftScaleRotate(p=0.2, shift_limit=0.0025, scale_limit=0.01, rotate_limit=0),\n    \n    al.Cutout(p=0.2, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n    \n    al.OneOf([\n        al.MotionBlur(blur_limit=(3, 5)),\n        al.MedianBlur(blur_limit=5),\n        al.GaussianBlur(blur_limit=(3, 5)),\n        al.GaussNoise(var_limit=(5.0, 30.0)),\n    ], p=0.7),\n    \n    \n    al.RandomGamma(gamma_limit=(70, 130), p=0.3),\n    al.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75),\n    \n    al.Normalize(),\n    ToTensorV2()\n], bbox_params = al.BboxParams(format = 'pascal_voc', label_fields = ['classes']))\n\n# Augmentations at Test Time/Val Time\nTEST_AUGMENTATIONS_OBJ = al.Compose([\n    al.Resize(IMAGE_SIZE, IMAGE_SIZE),\n    al.Normalize(),\n    ToTensorV2()\n], bbox_params = al.BboxParams(format = 'pascal_voc', label_fields = ['classes']))\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def unique(list_of_vals):\n    '''\n    Slow, but reproducible computation of unique values in list.\n    '''\n    unique = []\n    for i in tqdm.tqdm(list_of_vals):\n        if i not in unique:\n            unique += [i]\n    return unique","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Generate Validation and Training Set\nunique_ids = unique(train_pd.image_id)\ntrain_ids, val_ids = train_test_split(unique_ids, train_size = 0.99, test_size = 0.01, random_state = 42)","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/67914 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9818f7c670a248ac9db09c7c19777752"}},"metadata":{}}]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Create Dataset\nTrainOBJDataset = TrainObjectDetectionDataset(train_pd, train_ids, TRAIN_AUGMENTATIONS_OBJ, TRAIN_PATH);\nValOBJDataset = TrainObjectDetectionDataset(train_pd, val_ids, TEST_AUGMENTATIONS_OBJ, TRAIN_PATH);","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def custom_collate(images):\n    '''\n    Collates outputs from the torch.utils.data.dataset for object detection(varying GT size)\n    '''\n    values = list(zip(*images))\n    images = values[0]\n    bboxes = values[1]\n    return torch.stack(images),bboxes ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Create Dataloaders\nTrainOBJDataloader = torch.utils.data.DataLoader(TrainOBJDataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = custom_collate, worker_init_fn = seed_worker)\nValOBJDataloader = torch.utils.data.DataLoader(ValOBJDataset, batch_size = TEST_BATCH_SIZE, collate_fn = custom_collate, worker_init_fn = seed_worker)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"YOLOv3 Model"},{"metadata":{},"cell_type":"markdown","source":"General Implementation Details:\n- EfficientNetB0 BackBone, Extracts some features from the images(pretrained on ImageNet), gard features from all layers \n- Upsample 3 Times and Predicted on 3 Scales \nNot an Exact Copy of YOLOv3, and actually modifies many things(Call it YOLOvAlpha)"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# CONSTANTS FOR YOLOv3\nINPUT_SIZE = 2048\nGRID_SIZE_1 = 4\nGRID_SIZE_2 = 16\nGRID_SIZE_3 = 64\nGRID_SIZES = [GRID_SIZE_1, GRID_SIZE_2, GRID_SIZE_3]\n# Defone\nNUM_GRIDS = len(GRID_SIZES)\nNUM_ANCHORS = 5 # 3 Anchors per scale","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Config Classes"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class AnchorConfig(nn.Module):\n    def __init__(self, num_anchors):\n        super().__init__()\n        self.num_anchors = num_anchors\n        self.kmeans = MiniBatchKMeans(n_clusters = self.num_anchors, batch_size = 200, max_iter = 1000, max_no_improvement = 50)\n    def train_k_means(self, bounding_boxes):\n        '''\n        Trains a K-Means classifier to find rough bounding box sizes.\n        Input: List of (width, height) values\n        '''\n        # Convert Bounding Boxes to np.Array\n        bbox = np.expand_dims(np.array(bounding_boxes), -1)\n        self.kmeans = self.kmeans.fit(bbox)\n    def _get_anchors(self):\n        '''\n        Represents the ratio between width and height on the bounding box\n        '''\n        return np.squeeze(self.kmeans.cluster_centers_).tolist()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"anchors = AnchorConfig(NUM_ANCHORS)\n\n# Grab Bounding Boxes and Fit on the Training Set\nbounding_boxes = TrainOBJDataset.grab_all_bboxes()\n\nanchors.train_k_means(bounding_boxes)\n\nANCHOR_BOXES = anchors._get_anchors()","execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"|          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b216fdd58282430fa0b26195c3b9827b"}},"metadata":{}}]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class YOLOv3Config(nn.Module):\n    '''\n    Key functions needed for YOLOv3\n    \n    Note that since most of the operations involve Bounding Boxes, which varies in batches, all of this is not batched.\n    '''\n    def __init__(self, input_size, NUM_ANCHORS, NUM_GRIDS, device):\n        super().__init__()\n        self.input_size = input_size\n        self.NUM_ANCHORS = NUM_ANCHORS\n        self.NUM_GRIDS = NUM_GRIDS\n        self.device = device\n    def forward(self, GRID_SIZES, ANCHOR_BOXES):\n        '''\n        input: (image, GRID_SIZES, anchor_boxes)\n        Grid_SIZES: list of Grid Sizes\n        High Level function that generates a grid and anchor boxes for you.\n        '''\n        grids = []\n        anchors = []\n        for GRID_SIZE in GRID_SIZES:\n            # Generate Anchor Boxes and Grid Cells \n            grid = self.generate_grid(GRID_SIZE) # (S, S, 4)\n            anchor_boxes = self.generate_anchors(grid, ANCHOR_BOXES, GRID_SIZE) # (NUM_ANCHORS, S, S, 4) \n            grids += [grid]\n            anchors += [anchor_boxes]\n        return grids, anchors\n    def forward_GT(self, GRIDS_SIZES, ANCHOR_BOXES, GTS):\n        '''\n        GTS: (N, 5)\n        '''\n        grids, anchors = self(GRIDS_SIZES, ANCHOR_BOXES)\n        scales = []\n        for i in range(len(grids)):\n            grid = grids[i]\n            anchor = anchors[i]\n            indices, GT = self._assign_bbox(GTS, anchor)\n            # Convert the GT's back to cxcywh model\n            GT_classes = GT[:, -1].unsqueeze(-1)\n            GT = self._VOC2YOLO(GT)\n            GT = torch.cat([GT, GT_classes], dim = -1)\n            \n            if len(list(indices.shape)) == 1:\n                # Empty bounding boxes\n                scales += [(grid, anchor, None, None, GT)]\n            else:\n                GT_deltas = self._find_bbox_scale(GT, indices, anchor)\n                scales += [(grid, anchor, indices, GT_deltas, GT)]\n        return scales\n    def _lookup_anchors(self, anchors, anchor_idx):\n        '''\n        Helps to lookup the anchors needed given anchor indices\n        anchors(NUM_ANCHORS, GRID_SIZE, GRID_SIZE, 4)\n        anchor_idx: Tensor(N, 3)\n        '''\n        x_coords = anchor_idx[:, 0]\n        y_coords = anchor_idx[:, 1]\n        type_idx = anchor_idx[:, 2]\n        return anchors[type_idx, x_coords, y_coords, :]\n        \n    def _find_bbox_scale(self, GT_BBOX, anchor_box_idx, anchor_boxes):\n        '''\n        Determines what cx, cy, w, and h are the GT to map from anchor box to Ground Truth \n        GT_BBOX: (N, 5)\n        anchor_box_idx: Tensor(N, 3), where indices are expressed as (x, y, type)\n        anchor_boxes: Tensor(NUM_ANCHORS, S, S, 4)\n        '''\n        GT_classes = GT_BBOX[:, -1]\n        GT_BBOX = GT_BBOX[:, :-1]\n        Anchors_Needed = self._lookup_anchors(anchor_boxes, anchor_box_idx)\n        '''\n        Recall the change parameter\n        x prime = x + delta x\n        y prime = y + delta y\n        w prime = w * e^delta w\n        h prime = h * e^delta h\n        Thus,\n        delta x= x prime - x\n        delta y = y prime - y\n        delta w = ln(wprime / w)\n        delta h = ln(hprime / h) \n        '''\n        delta_x = GT_BBOX[:, 0] - Anchors_Needed[:, 0]\n        delta_y = GT_BBOX[:, 1] - Anchors_Needed[:, 1]\n        delta_w = torch.log(GT_BBOX[:, 2] / Anchors_Needed[:, 2])\n        delta_h = torch.log(GT_BBOX[:, 3] / Anchors_Needed[:, 3])\n        GroundTruthDeltas = torch.cat([delta_x.unsqueeze(-1), delta_y.unsqueeze(-1), delta_w.unsqueeze(-1), delta_h.unsqueeze(-1), GT_classes.unsqueeze(-1)], dim = -1)\n        return GroundTruthDeltas\n    def _find_x_and_y(self, GT_BBOXES, anchors_boxes):\n        '''\n        Computes the x and y index needed in the anchor boxes\n        GT_BBOXES: (N, 5)\n        Anchor_Boxes: (NUM_ANCHORS, GRID_SIZE, GRID_SIZE, 4) \n        or: (NUM_ANCHORS, Columns, Rows, 4)\n        '''\n        _, GRID_SIZE, _, _ = anchors_boxes.shape\n        cell_size = self.input_size / GRID_SIZE # Size of each cell(cell_size, cell_size)\n        N, _ = GT_BBOXES.shape\n        indices = []\n        for n in range(N):\n            bbox = GT_BBOXES[n, :-1] # (4)\n            indices += [(bbox[0] // cell_size, bbox[1] // cell_size)]\n        return torch.tensor(indices, device = GT_BBOXES.device)\n    def convert_image(self, images):\n        images = images.to(torch.float32) / 255.0\n        return images\n    def _assign_bbox(self, GT_BBOXES, anchor_boxes):\n        '''\n        returns the index associated with Each BBoxes.\n        GT_BBOXES: (N, 5), the Ground Truth Bboxes\n        Anchor_Boxes: (NUM_ANCHORS, GRID_SIZE, GRID_SIZE, 4), a grid of any scale \n        '''\n        _, GRID_SIZE, _, _ = anchor_boxes.shape\n        x_y_coord = self._find_x_and_y(GT_BBOXES, anchor_boxes).to(torch.long) # (N, 2) (Column, Row)\n        columns = x_y_coord[:, 0]\n        rows = x_y_coord[:, 1] \n        \n        potential_anchors = anchor_boxes[:, columns, rows, :] # (NUM_ANCHORS, N, 4)\n        # GT_BBOXES: (N, 5)\n        GT_classes = GT_BBOXES[:, -1]\n        GT_BBOXES = GT_BBOXES[:, :-1] # (N, 4)\n        # Convert to VOC Coords\n        GT_BBOXES = torchvision.ops.box_convert(GT_BBOXES, in_fmt = 'cxcywh', out_fmt = 'xyxy')\n        potential_anchors = torchvision.ops.box_convert(potential_anchors, in_fmt = 'cxcywh', out_fmt = 'xyxy')\n        # Select Largest IOU\n        N, _ = GT_BBOXES.shape\n        selected_box_idx = []\n        kept_GT = []\n        kept_GT_class= []\n        for n in range(N):\n            GT = GT_BBOXES[n, :].unsqueeze(0) # (1, 4)\n            potential = potential_anchors[:, n, :] # (N, 4)\n            iou = torchvision.ops.box_iou(GT, potential)\n            max_iou, max_idx = torch.max(iou, dim = 1)\n            if max_iou.item() >= 0.2: # We say that 0.2 Threshold for GT_Boxes \n                selected_box_idx += [(columns[n], rows[n], max_idx.item())]\n                kept_GT += [GT]\n                kept_GT_class += [GT_classes[n].unsqueeze(0)]\n            else:\n                pass\n                #print(f\"WARNING: GT has no Adequate Bounding Box. IOU: {max_iou.item()}, GT: {GT}, cur_GR: {GRID_SIZE}\")\n        selected_box_idx = torch.tensor(selected_box_idx, device = anchor_boxes.device)\n        if len(kept_GT) == 0:\n            return selected_box_idx, torch.zeros((0, 5), device = self.device)\n        kept_GT = torch.cat(kept_GT)\n        kept_GT_class = torch.cat(kept_GT_class).unsqueeze(-1)\n        return selected_box_idx, torch.cat([kept_GT, kept_GT_class], dim = -1)\n    def scale_up(self, bboxes):\n        return bboxes * self.input_size\n    def scale_down(self, bboxes):\n        return bboxes / self.input_size\n    def apply_GT_deltas(self, bboxes, deltas):\n        '''\n        bboxes: Tensors(N, 5)\n        deltas: Tensors(N, 4)\n        '''\n        x_bbox = bboxes[:, 0]\n        y_bbox = bboxes[:, 1]\n        w_bbox = bboxes[:, 2]\n        h_bbox = bboxes[:, 3]\n        \n        delta_x = deltas[:, 0]\n        delta_y = deltas[:, 1]\n        delta_w = deltas[:, 2]\n        delta_h = deltas[:, 3]\n        \n        x_prime = x_bbox + delta_x\n        y_prime = y_bbox + delta_y\n        w_prime = w_bbox * torch.exp(delta_w)\n        h_prime = h_bbox * torch.exp(delta_h)\n        \n        new_bbox = torch.cat([x_prime.unsqueeze(-1), y_prime.unsqueeze(-1), w_prime.unsqueeze(-1), h_prime.unsqueeze(-1), bboxes[:,-1].unsqueeze(-1)], axis = 1)\n        return new_bbox\n    \n    def visualize_bbox(self, image, bboxes):\n        '''\n        Visualizes bounding boxes on top of an image\n        image: Tensor(3, H, W)\n        bboxes: (N, 5), in YOLO coordinates\n        '''\n        VOC = self._YOLO2XYXY(bboxes) # (N, 4)\n        fig, ax = plt.subplots()\n        N, _ = VOC.shape\n        # Display the image\n        ax.imshow(image.transpose(0, 1).transpose(1, 2))\n        for n in range(N):\n            bbox = VOC[n, :]\n            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth = 1, alpha = 1, fill = False)\n            ax.add_patch(rect);\n        plt.show();\n    \n    def visualize_grid(self, image, grid):\n        '''\n        Visualizes one image and its grid.\n        Plots Grid as dots, and not bbox \n        image: Tensor(3, H, W)\n        grid: Tensor(N, 4), all bounding boxes\n        '''\n        N, _ = grid.shape \n        plt.imshow(image.transpose(0, 1).transpose(1, 2))\n        for n in range(N):\n            bbox = grid[n, :] \n            plt.scatter([bbox[0]], [bbox[1]])\n        plt.show();\n    def generate_anchors(self, grid, anchor_boxes, GRID_SIZE):\n        '''\n        Generates anchor boxes, in the form of (cx, cy, w, h) \n        grid: (S, S, 4), set of grid centers\n        anchor_boxes: List of Anchor sizes, all a tuple(w, h)\n        grid_size: The size of each grid cell(The scale to use)\n        '''\n        # Compute the grid size\n        grid_size = self.input_size / GRID_SIZE\n        anchors = []\n        C, R, _ = grid.shape\n        for width in anchor_boxes:\n            anchor_set = []\n            for c in range(C):\n                anchor_rows = []\n                for r in range(R):\n                    anchor = copy.deepcopy(grid[c, r, :]) # (4)\n                    anchor[2] = width * grid_size \n                    anchor[3] = grid_size\n                    anchor_rows += [anchor]\n                anchor_set += [torch.stack(anchor_rows)]\n            anchors += [torch.stack(anchor_set)]\n        return torch.stack(anchors)\n    def _batch_tensors(self, tensor, batch_size):\n        '''\n        Repeats a tensor across the batch dimension\n        tensor(...), outputs: Tensor(B, ...)\n        '''\n        repeated = torch.repeat_interleave(tensor, batch_size, axis = 0)\n        return repeated\n    def generate_grid(self, grid_size):\n        '''\n        Creates a Grid for the object detector(grid cells)\n        Returns a grid of size(S, S, 2) # (cx, cy, w, h) for every grid cell.\n        '''\n        grid_box_width = (1 / grid_size) * self.input_size\n        num_grid_width = self.input_size // grid_size\n        grid_box_height = (1 / grid_size) * self.input_size\n        num_grid_height = self.input_size // grid_size\n        \n        grid_centers = []\n        for w in range(0, grid_size):\n            for h in range(0, grid_size):\n                cx = w * grid_box_width + grid_box_width / 2\n                cy = h * grid_box_height + grid_box_height / 2\n                grid_centers += [(cx, cy, grid_box_width, grid_box_height)]\n        grid_centers = torch.tensor(grid_centers, device = self.device) \n        grid_centers = grid_centers.view(grid_size, grid_size, -1) # (Column, Rows, 4)\n        return grid_centers\n    def _convert_bbox(self, bbox):\n        '''\n        Inflates a bounding box with 4 values(N, 4) to 5 where all classes are just 0(Helpful for testing)\n        '''\n        N, _ = bbox.shape\n        vals = torch.zeros((N, 5))\n        vals[:, :-1] = bbox\n        return vals;\n    def _YOLO2XYXY(self, YOLO):\n        '''\n        YOLO: Tensor(N, 5)\n        '''\n        YOLO = YOLO[:, :-1] # (N, 4)\n        XYXY = torchvision.ops.box_convert(YOLO, in_fmt = 'cxcywh', out_fmt = 'xywh')\n        return XYXY\n    def _VOC2YOLO(self, VOC):\n        '''\n        Helper Function to Convert a VOC based Bounding Box into a YOLO based\n        VOC: Tensor(N, 5)\n        '''\n        VOC = VOC[:, :-1] # (N, 4) \n        YOLO = torchvision.ops.box_convert(VOC, in_fmt = 'xyxy', out_fmt = 'cxcywh')\n        return YOLO\n    def _YOLO2VOC(self, YOLO):\n        '''\n        Converts a YOLO based bounding box to a VOC based bounding box.\n        YOLO: Tensor(N, 5)\n        '''\n        YOLO = YOLO[:, :-1] # (N, 4)\n        VOC = torchvision.ops.box_convert(YOLO, in_fmt = 'cxcywh', out_fmt = 'xyxy')\n        return VOC","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# YOLOv3 Model:"},{"metadata":{},"cell_type":"markdown","source":"Some Useful Blocks"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size, padding = padding, groups = groups)\n        self.SILU = nn.SiLU(inplace = True)\n        self.bn = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn(self.SILU(self.conv(x)))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class ConvSqueezeExcite(nn.Module):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.Squeeze = nn.Conv2d(self.in_features, self.inner_features, kernel_size = 1)\n        self.Excited = nn.Conv2d(self.inner_features, self.in_features, kernel_size = 1)\n        self.SILU = nn.SiLU(inplace = True)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        squeezed = self.SILU(self.Squeeze(x))\n        excited = torch.sigmoid(self.Excited(squeezed))\n        return excited * x","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class DownSampleConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, stride, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size, padding = padding, stride = stride, groups = groups)\n        self.SiLU = nn.SiLU(inplace = True)\n        self.bn = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn(self.SiLU(self.conv(x)))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class RegularSE(nn.Module):\n    '''\n    Normal Squeeze and Excitation Block \n    '''\n    def __init__(self, in_features, squeezed_features):\n        super().__init__()\n        self.in_features = in_features\n        self.squeezed_features = squeezed_features\n        self.Squeeze = nn.Linear(self.in_features, self.squeezed_features)\n        self.act1 = nn.SiLU(inplace = True)\n        self.Expand = nn.Linear(self.squeezed_features, self.in_features)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        # Max Pool over last 2 dims\n        max_pooled, _ = torch.max(x, dim = -1)\n        max_pooled, _ = torch.max(max_pooled, dim = -1) # (B, C)\n        # Squeeze and Excitation Network\n        squeezed = self.act1(self.Squeeze(max_pooled))\n        excited = torch.sigmoid(self.Expand(squeezed)).unsqueeze(-1).unsqueeze(-1)\n        return excited * x\n        ","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class BottleNeck(nn.Module):\n    '''\n    Squeeze Excite Residual Block as Proposed in ResNet.\n    '''\n    def __init__(self, input_size, inner_size, device):\n        super().__init__()\n        self.device = device\n        self.input_size = input_size\n        self.inner_size = inner_size\n        self.Squeeze = ConvBlock(self.input_size, self.inner_size, 1, 0, 1)\n        self.Process = ConvBlock(self.inner_size, self.inner_size, 3, 1, 1)\n        self.Expand = ConvBlock(self.inner_size, self.input_size, 1, 0, 1)\n        self.SE = RegularSE(self.input_size, self.input_size // 16)\n        self.gamma = nn.Parameter(torch.zeros(1, device = self.device))\n    def forward(self, x):\n        squeezed = self.Squeeze(x)\n        processed = self.Process(squeezed)\n        expand = self.Expand(processed)\n        excited = self.SE(expand)\n        return self.gamma * excited + x\nclass DownConvolutionBlock(nn.Module):\n    '''\n    Uses very cheap operations to process and downconvolve the massive image\n    '''\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        self.in_features = 3\n        self.avgPool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = 2)\n        self.downConv = DownSampleConvBlock(3, 5, 3, 1, 2, 1) # (2048 -> 1024)\n        \n        self.downConv2 = DownSampleConvBlock(8, 16, 3, 1, 2, 1) # (1024 -> 512)\n        \n        self.downConv3 = DownSampleConvBlock(24, 64, 3, 1, 2, 1) # (512 -> 256)\n        self.process3 = nn.Sequential(*[\n            BottleNeck(64, 16, self.device) for i in range(3) # A little bit of processing\n        ])\n        \n        self.proj = nn.Sequential(*[\n            ConvBlock(64, 32, 1, 0, 1),\n            ConvBlock(32, 3, 1, 0, 1)])\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n        \n    def forward(self, x):\n        '''\n        Initial DownConvolution\n        x: Tensor(B, 3, 2048, 2048)\n        '''\n        B, _, _, _ = x.shape\n        interpolated = F.interpolate(x, (256, 256), mode = 'bilinear')\n        avgPool = self.avgPool(x) # (B, 3, 1024, 1024)\n        downConv = self.downConv(x) # (B, 5, 1024, 1024)\n        # Concatenate Features\n        concatted = torch.cat([downConv, avgPool], dim = 1) # (B, 8, 1024, 1024)\n        # DownConv again \n        avgPool2 = self.avgPool(concatted) # (B, 8, 512, 512)\n        downConv2 = self.downConv2(concatted) # (B, 32, 512, 512)\n        concatted2 = torch.cat([downConv2, avgPool2], dim = 1) # (B, 40, 512, 512)\n        # Conv Stride a Few times\n        conv3 = self.process3(self.downConv3(concatted2)) # (B, 64, 256, 256)  \n        proj = self.proj(conv3) # (B, 64, 128, 128)\n        return proj * self.gamma + interpolated","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class DownConvolutionBlock(nn.Module):\n    '''\n    Uses very cheap operations to process and downconvolve the massive image\n    '''\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        self.in_features = 3\n        self.avgPool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = 2)\n        self.downConv = DownSampleConvBlock(3, 5, 3, 1, 2, 1) # (2048 -> 1024)\n        \n        self.downConv2 = DownSampleConvBlock(8, 16, 3, 1, 2, 1) # (1024 -> 512)\n        \n        self.downConv3 = DownSampleConvBlock(24, 64, 3, 1, 2, 1) # (512 -> 256)\n        self.process3 = nn.Sequential(*[\n            BottleNeck(64, 16, self.device) for i in range(3) # A little bit of processing\n        ])\n        \n        self.proj = nn.Sequential(*[\n            ConvBlock(64, 32, 1, 0, 1),\n            ConvBlock(32, 3, 1, 0, 1)])\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n        \n    def forward(self, x):\n        '''\n        Initial DownConvolution\n        x: Tensor(B, 3, 2048, 2048)\n        '''\n        B, _, _, _ = x.shape\n        interpolated = F.interpolate(x, (256, 256), mode = 'bilinear')\n        avgPool = self.avgPool(x) # (B, 3, 1024, 1024)\n        downConv = self.downConv(x) # (B, 5, 1024, 1024)\n        # Concatenate Features\n        concatted = torch.cat([downConv, avgPool], dim = 1) # (B, 8, 1024, 1024)\n        # DownConv again \n        avgPool2 = self.avgPool(concatted) # (B, 8, 512, 512)\n        downConv2 = self.downConv2(concatted) # (B, 32, 512, 512)\n        concatted2 = torch.cat([downConv2, avgPool2], dim = 1) # (B, 40, 512, 512)\n        # Conv Stride a Few times\n        conv3 = self.process3(self.downConv3(concatted2)) # (B, 64, 256, 256)  \n        proj = self.proj(conv3) # (B, 64, 128, 128)\n        return proj * self.gamma + interpolated","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class InvertedResidualBlock(nn.Module):\n    def __init__(self, in_features, inner_features, device):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.device = device\n        self.expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n        self.depthwise = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n        self.SE = RegularSE(self.inner_features, self.inner_features // 16)\n        self.squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        expanded = self.expand(x)\n        depthwise = self.depthwise(expanded)\n        se = self.SE(depthwise)\n        squeezed = self.squeeze(se)\n        return self.gamma * squeezed + x","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class TransposedConvBlock(nn.Module):\n    '''\n    Same as ConvBlock, but now allows for transposed convolution \n    '''\n    def __init__(self, in_features, out_features, kernel_size, output_padding, stride):\n        super().__init__()\n        self.convT = nn.ConvTranspose2d(in_features, out_features, kernel_size = kernel_size, output_padding = output_padding, stride = stride)\n        self.act1 = nn.SiLU(inplace = True)\n        self.bn1 = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn1(self.act1(self.convT(x)))","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Blocks(Larger Components)"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class ModifiedEfficientNetStudent(nn.Module):\n    '''\n    Student uses Down Convolutional Block to quickly downsample super large images.\n    '''\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = True\n    def __init__(self, num_classes, device, model_name = 'efficientnet_b3_pruned', drop_prob = 0.0):\n        super().__init__()\n        self.num_classes = num_classes \n        self.device = device\n        self.model_name = model_name\n        self.drop_prob = drop_prob\n        self.model = timm.create_model(self.model_name, pretrained = False)\n        \n        # Extract Layers\n        self.downsampled = DownConvolutionBlock(self.device)\n        self.conv1 = self.model.conv_stem\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        \n        self.block0 = self.model.blocks[0]\n        self.block1 = self.model.blocks[1]\n        self.block2 = self.model.blocks[2]\n        self.block3 = self.model.blocks[3]\n        self.block4 = self.model.blocks[4]\n        self.block5 = self.model.blocks[5]\n        self.block6 = self.model.blocks[6]\n        \n        # Custom Layers\n        self.Attention1 = RegularSE(12, 4)\n        self.Attention2 = RegularSE(40, 16)\n        self.Attention3 = RegularSE(120, 32)\n        self.Attention4 = RegularSE(384, 64)\n        # Freeze Initial Layers\n        self.freeze(self.downsampled)\n        self.freeze(self.conv1)\n        self.freeze(self.bn1)\n        \n        self.freeze(self.block0)\n        self.freeze(self.block1)\n        self.freeze(self.block2)\n        self.freeze(self.block3)\n        self.freeze(self.block4)\n        self.freeze(self.Attention1)\n        self.freeze(self.Attention2)\n        self.freeze(self.Attention3)\n        \n        \n        self.layer4 = nn.Sequential(*[\n            DownSampleConvBlock(384, 384, 5, 2, 2, 384), # (B, 320, 4, 4)\n            ConvBlock(384, 768, 1, 0, 1)] + # (B, 768, 4, 4)\n        [\n            InvertedResidualBlock(768, 1536, self.device) for i in range(5)\n         \n        ]\n        )\n        self.conv2 = ConvBlock(768, 1536, 1, 0, 1)\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(1536, self.num_classes)\n    def forward(self, x):\n        '''\n        x: Tensor(B, 3, 320, 320)\n        '''\n        downsampled = self.downsampled(x) # (B, 3, 256, 256)\n        conv1 = self.bn1(self.act1(self.conv1(downsampled))) # (B, 32, 256, 256)\n        # Extract Features\n        block0 = self.block0(conv1)\n        block1 = self.block1(block0) # (B, 24, 64, 64) \n        attention1 = self.Attention1(block1)\n        \n        block2 = self.block2(attention1) # (B, 40, 32, 32)\n        attention2 = self.Attention2(block2)\n        \n        block3 = self.block3(attention2)\n        block4 = self.block4(block3) # (B, 112, 16, 16)\n        attention3= self.Attention3(block4)\n        \n        block5 = self.block5(attention3)\n        block6 = self.block6(block5) # (B, 320, 8, 8)\n        attention4 = self.Attention4(block6)\n        # Custom Layer 4\n        layer4 = self.layer4(attention4) # (B, 512, 4, 4)\n        return attention1, attention3, layer4","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class ClassificationAlpha(nn.Module):\n    def __init__(self, device, file_path = None):\n        super().__init__()\n        self.file_path = file_path\n        self.num_classes = 1\n        self.device = device\n        self.model = ModifiedEfficientNetStudent(self.num_classes, self.device, drop_prob = 0.1)\n        if file_path != None:\n            self.load_state_dict(torch.load(self.file_path, map_location = self.device))\n    def forward(self, x):\n        return self.model(x)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class ObjectDetectorQT(nn.Module):\n    def __init__(self, in_features, lower_in_features, inner_features, num_obj_classes, device, num_anchors = 3):\n        super().__init__()\n        self.inner_features = inner_features\n        self.lower_in_features = lower_in_features\n        self.in_features = in_features\n        self.num_classes = num_obj_classes\n        self.num_anchors = num_anchors\n        self.device = device\n        \n        # Projection Layers to map embeddings to the correct size\n        self.proj = TransposedConvBlock(self.lower_in_features, self.in_features, 1, 1, 2)\n        self.proj2 = ConvBlock(2 * self.in_features, self.inner_features, 1, 0, 1)\n        \n        self.ConvPipeLine = nn.Sequential(*[\n            BottleNeck(self.inner_features, self.inner_features // 4, self.device) for i in range(6)\n        ])\n        \n        self.predict = nn.Conv2d(self.inner_features, self.num_anchors * (5 + self.num_classes), kernel_size = 1)\n    def forward(self, x, lower_x):\n        '''\n        x: Tensor(B, C, H, W), \n        lower_x: Tensor(B, C_prime, H_prime, W_prime), features from the previous Object Detector Layer\n        '''\n        # project lower_x into x\n        proj_x = self.proj(lower_x) # (B, in_features, H, W)\n        # Concatenate the features\n        concatenated = torch.cat([x, proj_x], dim = 1) # (B, self.in_features * 2, H, W) \n        # Project into ConvPipeLine\n        proj_conv = self.proj2(concatenated) \n        # Run through convPipeLine for features\n        features = self.ConvPipeLine(proj_conv)\n        return self.predict(features), features","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class ObjectDetectorAlpha(nn.Module):\n    def __init__(self, in_features, inner_features, num_obj_classes, device, num_anchors = 3):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_classes = num_obj_classes\n        self.num_anchors = num_anchors\n        self.device = device\n        \n        self.proj = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n        \n        self.ConvPipeLine = nn.Sequential(*[\n            BottleNeck(self.inner_features, self.inner_features // 4, self.device) for i in range(6)\n        ])\n        \n        self.predict = nn.Conv2d(self.inner_features, self.num_anchors * (5 + self.num_classes), kernel_size = 1)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W), \n        '''\n        proj_conv = self.proj(x) \n        # Extract Features\n        features = self.ConvPipeLine(proj_conv)\n        return self.predict(features), features","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class YOLOQT(nn.Module):\n    def __init__(self, num_obj_classes, num_anchors, device, file_path = None):\n        super().__init__()\n        self.num_anchors = num_anchors\n        self.num_obj_classes = num_obj_classes\n        self.device = device\n        self.file_path = file_path\n        \n        self.feature_extractor = ClassificationAlpha(self.device, file_path = self.file_path)\n        \n        # Object Detector Layers\n        \n        self.obj_detect_1 = ObjectDetectorAlpha(768, 256, self.num_obj_classes, self.device, num_anchors = self.num_anchors)\n        self.obj_detect_2 = ObjectDetectorQT(248, 128, 256, self.num_obj_classes, self.device, num_anchors = self.num_anchors)\n        self.obj_detect_3 = ObjectDetectorQT(76, 128, 128, self.num_obj_classes, self.device, num_anchors = self.num_anchors)\n        \n        # Upsample and Further Processing Layers\n        \n        self.upsample_3 = TransposedConvBlock(768, 256, 1, 1, 2)\n        self.process_3 = BottleNeck(256, 32, self.device)\n        self.upsample3_5 = TransposedConvBlock(256, 128, 1, 1, 2)\n        self.process3_5 = BottleNeck(128, 16, self.device) \n        self.sequential_3 = nn.Sequential(*[\n            BottleNeck(248, 64, self.device) for i in range(3)\n        ])\n        \n        self.upsample_2 = TransposedConvBlock(248, 128, 1, 1, 2)\n        self.process_2 = BottleNeck(128, 32, self.device)\n    \n        self.upsample_1 = TransposedConvBlock(128, 64, 1, 1, 2)\n        self.process_1 = BottleNeck(64, 16, self.device)\n        \n        \n        \n        self.upsample_small = TransposedConvBlock(256, 128, 1, 1, 2)\n        self.process_small = BottleNeck(128, 32, self.device)\n        self.upsample_middle = TransposedConvBlock(256, 128, 1, 1, 2)\n        self.process_middle = BottleNeck(128, 32, self.device)\n        self.sequential_1 = nn.Sequential(*[\n            BottleNeck(76, 24, self.device) for i in range(3)\n        ])\n        \n    def forward(self, x):\n        layer2, layer3, layer4 = self.feature_extractor(x)\n        # Layer2: Tensor(B, 24, 64, 64)\n        # Layer3: Tensor(B, 112, 16, 16)\n        # Layer4: Tensor(B, 768, 4, 4) \n        # Make Large Predictions\n        bbox_pred_large, features_large = self.obj_detect_1(layer4)\n        # features_large: Tensor(B, 256, 4, 4)\n        # Upsample and Process Layer 4\n        upsampled_3 = self.upsample_3(layer4) # (B, 256, 8, 8)\n        processed_3 = self.process_3(upsampled_3) \n        upsampled_3 = self.upsample3_5(processed_3) # (B, 128, 16, 16)\n        processed_3 = self.process3_5(upsampled_3) # (B, 128, 16, 16)\n        \n        upsample_layer3 = self.upsample_small(features_large) # (B, 128, 8, 8)\n        processed_layer3 = self.process_small(upsample_layer3) # (B, 128, 8, 8)\n        # Concatenate\n        concat_3 = torch.cat([layer3, processed_3], dim = 1) # (B, 264, 16, 16)\n        sequential_3 = self.sequential_3(concat_3) # (B, 264, 16, 16)\n        # Object Detector 3(Medium Predictions)\n        bbox_pred_medium, features_medium = self.obj_detect_2(sequential_3, processed_layer3) \n        # Features_medium: Tensor(B, 256, 16, 16) \n        # Upsample and Process Layer 3\n        upsampled_2 = self.upsample_2(sequential_3) \n        processed_2 = self.process_2(upsampled_2) # (B, 128, 32, 32)\n        upsampled_1 = self.upsample_1(processed_2)\n        processed_1 = self.process_1(upsampled_1) # (B, 64, 64, 64)\n        # Concatenate\n        concat_1 = torch.cat([layer2, processed_1], dim = 1) # (B, 96, 64, 64) \n        sequential_1 = self.sequential_1(concat_1) # (B, 96, 64, 64) \n        \n        # Process Features Medium\n        upsample_medium = self.upsample_middle(features_medium)\n        processed_medium = self.process_middle(upsample_medium) # (B, 128, 32, 32)\n        # Object Detector 2(Small Predictions)\n        bbox_pred_small, features_small = self.obj_detect_3(sequential_1, processed_medium) \n        \n        return bbox_pred_small, bbox_pred_medium, bbox_pred_large\n        # Bbox_Small: Tensor(B, anchors(5 + C), S_small, S_small), BBox_Medium: Tensor(B, anchors(5 + C), S_medium, S_medium), Bbox_Large: Tensor(B, anchors(5 + C), S_large, S_large)\n        ","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Helper Classes for Training"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class YOLOv3LossConfig(nn.Module):\n    '''\n    Helper Class that configurates the loss\n    '''\n    def __init__(self, input_size, anchor_sizes, grid_sizes, num_obj_classes, device):\n        super().__init__()\n        self.input_size = input_size\n        self.anchor_sizes = anchor_sizes\n        self.grid_sizes = grid_sizes\n        self.num_obj_classes = num_obj_classes \n        self.device = device       \n        \n        self.num_anchors = len(self.anchor_sizes)\n        self.num_grids = len(self.grid_sizes)\n        \n        self.Yconfig = YOLOv3Config(self.input_size, self.num_anchors, self.num_grids, self.device)\n        # Variety of Loss Functions\n        self.obj_crit = nn.BCEWithLogitsLoss()\n        self.MSELoss = nn.MSELoss()\n        self.cls_crit = nn.CrossEntropyLoss()\n        # Weights\n        self.obj_loss = 0.1\n        self.mse_loss = 2\n        self.cls_loss = 1 \n    def _not_selected(self, bounding_boxes, indices):\n        '''\n        Bounding_Boxes: (NUM_ANCHORS, S_Scale, S_Scale, 18)\n        indices: (N, 3) \n        '''\n        _, S_scale, _, _ = bounding_boxes.shape\n        x_coords = indices[:, 0]\n        y_coords = indices[:, 1]\n        anchor_idx = indices[:, 2]\n        selected_vals = [(x_coords[i].item(), y_coords[i].item(), anchor_idx[i].item()) for i in range(x_coords.shape[0])] \n        all_x_coords = [i for i in range(S_scale)] \n        all_y_coords = [i for i in range(S_scale)]\n        all_anchor_idx = [i for i in range(self.num_anchors)]\n        all_possible_combinations = list(itertools.product(all_x_coords, all_y_coords, all_anchor_idx))\n        \n        not_selected_combinations = torch.tensor([combo for combo in all_possible_combinations if combo not in selected_vals], device = self.device)\n        return not_selected_combinations\n        \n        \n    def _loss(self, bbox_scale, GT_scale):\n        '''\n        Computes the Loss for a given BBox Scale\n        bbox_scale; Tensor(NUM_ANCHORS(5 + C), S_scale, S_scale)\n        '''\n        _, S_Scale, _ = bbox_scale.shape\n        grid, anchor, index, GT_delta, GT_bbox= GT_scale\n        if index == None:\n            return 0\n        # Select the GT Anchor\n        selected_anchor = self.Yconfig._lookup_anchors(anchor, index)\n        # split out the bbox_scale into (3, 5 + C) \n        bbox_scale = bbox_scale.view(self.num_anchors, 5 + self.num_obj_classes, S_Scale, S_Scale);\n        bbox_scale = bbox_scale.transpose(1, 2).transpose(2, 3)\n        # Select BBoxes\n        selected_bboxes = self.Yconfig._lookup_anchors(bbox_scale, index)\n        not_selected_bboxes_inds = self._not_selected(bbox_scale, index)\n        not_selected_bboxes= self.Yconfig._lookup_anchors(bbox_scale, not_selected_bboxes_inds)\n        # Selected Bounding Box Losses\n        selected_N, selected_C = selected_bboxes.shape\n        not_selected_N, _ = not_selected_bboxes.shape\n        \n        # Extract CLS, OBJ, and Deltas\n        class_selected = selected_bboxes[:, selected_C - self.num_obj_classes:]\n        bbox_selected = selected_bboxes[:, :selected_C - self.num_obj_classes] # (cx, cy, w, h) shifts\n        deltas_selected = bbox_selected[:, :-1]\n        obj_selected = bbox_selected[:, -1]\n        \n        # Extract CLS and Deltas\n        deltas_GT = GT_delta[:, :-1]\n        class_GT = GT_delta[:, -1]\n        \n        obj_loss_selected = self.obj_crit(obj_selected, torch.ones_like(obj_selected, device = obj_selected.device)) * self.obj_loss\n        cls_loss_selected = self.cls_crit(class_selected, class_GT.to(torch.long)) * self.cls_loss\n        delta_loss_selected = self.MSELoss(deltas_selected, deltas_GT) * self.mse_loss\n        selected_loss = obj_loss_selected + cls_loss_selected + delta_loss_selected\n        \n        # Extract OBJ from not selected\n        obj_not_selected = not_selected_bboxes[:, 4]\n        obj_loss_not = self.obj_crit(obj_not_selected, torch.zeros_like(obj_not_selected, device = obj_not_selected.device)) * self.obj_loss\n        not_selected_loss= obj_loss_not\n        # Add Losses\n        loss = selected_loss + not_selected_loss\n        return loss\n        \n    def forward(self, bbox_small, bbox_medium, bbox_large, GT_bboxes):\n        '''\n        bbox_small: Tensor(anchors(5 + C), S_small, S_small)\n        bbox_medium: Tensor(anchors(5 + C), S_medium, S_medium)\n        bbox_large: Tensor(anchors(5 + C), S_large, S_large)\n        GT_bboxes: Tensor(N, 5)\n        \n        Note: This operation cannot be batched.\n        '''\n        N, _, _ = bbox_small.shape\n        # Compute Grids, and Necessary fucntions\n        GT_large, GT_medium, GT_small = self.Yconfig.forward_GT(self.grid_sizes, self.anchor_sizes, GT_bboxes)\n        # Compute Loss small\n        loss_small = self._loss(bbox_small, GT_small)\n        # Compute Loss medium\n        loss_medium = self._loss(bbox_medium, GT_medium)\n        # Compute Loss Large\n        loss_large = self._loss(bbox_large, GT_large)\n        # Add Losses(This will be super large to backprop through)\n        total_loss = (loss_small + loss_medium + loss_large) / N\n        return total_loss","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class YOLOv3Inference(nn.Module):\n    '''\n    Helper Class For Inference with YOLO\n    '''\n    def __init__(self, input_size, anchor_boxes, grid_sizes, num_obj_classes, device, obj_thresh = 0.0, iou_thresh = 0.5):\n        super().__init__()\n        self.device = device\n        self.obj_thresh = obj_thresh\n        self.iou_thresh = iou_thresh\n        self.input_size = input_size\n        self.anchor_boxes = anchor_boxes \n        self.grid_sizes = grid_sizes\n        self.num_obj_classes = num_obj_classes\n        \n        self.num_anchor_boxes = len(self.anchor_boxes)\n        self.num_grid_sizes = len(self.grid_sizes)\n        \n        self.Yconfig = YOLOv3Config(self.input_size, self.num_anchor_boxes, self.num_grid_sizes, self.device)\n    def process_size(self, anchor_size, bbox_shape):\n        '''\n        bbox_shape: Tensor(B, ANCHOR_BOXES(5 + C), S_size, S_size)\n        anchor_sizes: Tensor(ANCHOR_BOXES, S_size, S_size, 4)\n        '''\n        B, C, S_size, _ = bbox_shape.shape\n        bbox_shape = bbox_shape.view(B, self.num_anchor_boxes, 5 + self.num_obj_classes, S_size, S_size)\n        # Extract Out Classes\n        classes = F.softmax(bbox_shape[:, :, 5:, :, :]) # (B, NUM_ANCHORS, self.num_obj_classes, S_size, S_size)\n        # Argmax over classes\n        _, classes_ind = torch.max(classes, dim = 2) # (B, NUM_ANCHORS, S_size, S_size)\n        \n        bboxes = bbox_shape[:, :, :5, :, :] # (B, NUM_ANCHORS, 5, S_size, S_size)\n        \n        deltas = bboxes[:, :, :-1, :, :] # (B, NUM_ANCHORS, 4, S_size, S_size)\n       \n        # Extract Out (cx, cy, w, h) shifts \n        cx_delta = deltas[:, :, 0, :, :]\n        cy_delta = deltas[:, :, 1, :, :]\n        w_delta = deltas[:, :, 2, :, :]\n        h_delta = deltas[:, :, 3, :, :]\n        \n        obj_scores = torch.sigmoid(bboxes[:, :, -1, :, :]) # (B, NUM_ANCHORS, S_size, S_size)\n        \n        threshold = obj_scores >= self.obj_thresh # (B, NUM_ANCHORS, S_size, S_size)\n        kept_sorted_bboxes = []\n        for b in range(B):\n            threshold_batch = threshold[b] # (NUM_ANCHORS, S_size, S_size)\n            cx_batch = cx_delta[b]\n            cy_batch = cy_delta[b]\n            w_batch = w_delta[b]\n            h_batch = h_delta[b]\n            classes_batch = classes_ind[b]\n            obj_scores_batch = obj_scores[b] \n            anchor_selected = anchor_size[threshold_batch, :] # (N, 4)\n            # Inflate anchors\n            anchor_selected = self.Yconfig._convert_bbox(anchor_selected) # (N, 5) \n            anchor_selected = anchor_selected.to(self.device)\n            kept_cx = cx_batch[threshold_batch]\n            kept_cy = cy_batch[threshold_batch]\n            kept_w = w_batch[threshold_batch]\n            kept_h = h_batch[threshold_batch]\n            kept_deltas = torch.stack([kept_cx, kept_cy, kept_w, kept_h], dim = -1) # (N, 4)\n            \n            kept_bbox = self.Yconfig.apply_GT_deltas(anchor_selected, kept_deltas)[:, :-1] # (N, 4)\n            kept_classes = classes_batch[threshold_batch]\n            kept_obj_scores = obj_scores_batch[threshold_batch]\n            \n            # NMS Threshold Current Bounding Boxes\n            kept_idx = torchvision.ops.batched_nms(kept_bbox, kept_obj_scores, kept_classes, iou_threshold = self.iou_thresh)\n            kept_bbox = kept_bbox[kept_idx]\n            kept_classes = kept_classes[kept_idx].unsqueeze(-1)\n            kept_obj_scores = kept_obj_scores[kept_idx].unsqueeze(-1)\n            kept_bbox = torch.cat([kept_bbox, kept_obj_scores, kept_classes], dim = -1) # (N, 6)\n            kept_sorted_bboxes += [kept_bbox]\n        return torch.stack(kept_sorted_bboxes)\n\n    def forward(self, bbox_small, bbox_medium, bbox_large):\n        '''\n        grid_small: Tensor(B, NUM_ANCHORS(5 + C), S_small, S_small)\n        grid_medium: Tensor(B, NUM_ANCHORS(5 + C), S_medium, S_medium)\n        grid_large: Tensor(B, NUM_ANCHORS(5 + C), S_large, S_large)\n        '''\n        B, C, _, S_small = bbox_small.shape\n        _, _, _, S_medium = bbox_medium.shape\n        _, _, _, S_large = bbox_large.shape\n        grids, anchors = self.Yconfig(self.grid_sizes, self.anchor_boxes)\n        grid_small, grid_medium, grid_large = grids\n        anchor_large, anchor_medium, anchor_small = anchors\n        # anchor_small: Tensor(NUM_ANCHORS,S_small, S_small, 4), anchor_medium: Tensor(NUM_ANCHORS, S_medium, S_medium, 4), anchor_large: Tensor(NUM_ANCHORS, S_large, S_large, 4)\n        # grid_small: Tensor(S_small, S_small, 4), grid_medium: Tenso(S_medium, S_medium, 4), grid_large: Tensor(S_large, S_large, 4)\n        # Load onto Device\n        grid_small = grid_small.to(self.device)\n        grid_medium = grid_medium.to(self.device)\n        grid_large = grid_large.to(self.device)\n        \n        anchor_large = anchor_large.to(self.device)\n        anchor_medium = anchor_medium.to(self.device)\n        anchor_small = anchor_small.to(self.device) \n        \n        small_selected = self.process_size(anchor_small, bbox_small) # (B, selected_small, 6)\n        medium_selected = self.process_size(anchor_medium, bbox_medium) # (B, selected_medium, 6)\n        large_selected = self.process_size(anchor_large, bbox_large) # (B, selected_large, 6)\n        # NMS Threshold Bounding Boxes\n        kept_bboxes = []\n        for b in range(B):\n            small_bboxes = small_selected[b] # (selected_small, 6)\n            medium_bboxes = medium_selected[b] # (selected_medium, 6)\n            large_bboxes = large_selected[b] # (selected_large, 6)\n            # Concatenate\n            bboxes = torch.cat([small_bboxes, medium_bboxes, large_bboxes], dim = 0) # (s + m + l, 6)\n            # Extract Class and OBJ\n            bbox = bboxes[:, :4]\n            obj = bboxes[:, 4]\n            classes = bboxes[:, -1] \n            # NMS Thresh \n            kept_bboxes_ind = torchvision.ops.batched_nms(bbox, obj, classes, iou_threshold = self.iou_thresh)\n            kept_bboxes += [bboxes[kept_bboxes_ind]]\n        return kept_bboxes\n            \n        ","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class YOLOSolver(nn.Module):\n    def __init__(self, input_size, anchor_boxes, grid_sizes, num_obj_classes, device, file_path = None):\n        super().__init__()\n        self.file_path = file_path\n        self.input_size = input_size\n        self.anchor_boxes = anchor_boxes\n        self.grid_sizes = grid_sizes\n        self.num_obj_classes = num_obj_classes\n        self.device = device\n        self.num_anchors = len(anchor_boxes)\n        self.num_grids = len(self.grid_sizes)\n        \n        self.Yconfig = YOLOv3Config(self.input_size, self.num_anchors, self.num_grids, self.device)\n        self.lossConfig = YOLOv3LossConfig(self.input_size, self.anchor_boxes, self.grid_sizes, self.num_obj_classes, self.device)\n        self.YInference = YOLOv3Inference(self.input_size, self.anchor_boxes, self.grid_sizes, self.num_obj_classes, self.device)\n        \n        self.model = YOLOQT(self.num_obj_classes, self.num_anchors, self.device, file_path = self.file_path)\n        self.optim = optim.Adam(self.model.parameters(), lr = 3e-4, weight_decay = 1e-4)\n        self.lr_decay = optim.lr_scheduler.StepLR(self.optim, 5, 0.95)\n        self.lr_decay2 = optim.lr_scheduler.CosineAnnealingLR(self.optim, 5, eta_min = 1e-7)\n    def forward(self, x):\n        '''\n        Runs a Test Time Run through the Object Detector.\n        '''\n        self.eval()\n        with torch.no_grad():\n            grid_small, grid_medium, grid_large = self.model(self.Yconfig.convert_image(x))\n            # Extract Bounding Boxes out and Threshold their values\n            selected_bounding_boxes = self.YInference(grid_small, grid_medium, grid_large)\n            return selected_bounding_boxes\n    def compute_loss(self, images, bboxes):\n        bbox_small, bbox_medium, bbox_large = self.model(self.Yconfig.convert_image(images))\n        B, _, _, _ = images.shape\n        t_loss = torch.zeros((1), device = self.device)\n        for b in range(B):\n            one_small = bbox_small[b]\n            one_medium = bbox_medium[b]\n            one_large = bbox_large[b]\n            bbox = bboxes[b].to(self.device)\n            loss = self.lossConfig(one_small, one_medium, one_large, bbox)\n            t_loss = t_loss + loss\n        del bbox_small\n        del bbox_medium\n        del bbox_large\n        torch.cuda.empty_cache()\n        return t_loss\n    def MaP(self, predicted_bboxes, bboxes):\n        '''\n        predicted_bboxes: (N_prime, 6)\n        bboxes: (N, 5), both in (cx, cy, w, h, cls) format.\n        '''\n        # Computes the Mean Average Precision between predicted and GT bounding boxes\n        Ground_Truths = {}\n        GT_deltas = bboxes[:, :-1].cpu().numpy().tolist()\n        GT_classes = bboxes[:, -1].cpu().numpy().tolist()\n        Ground_Truths['boxes'] = GT_deltas\n        Ground_Truths['labels'] = GT_classes\n        \n        \n        pred = {}\n        pred_deltas = predicted_bboxes[:, :-2].cpu().numpy().tolist()\n        pred_obj_scores = predicted_bboxes[:, -2].cpu().numpy().tolist()\n        pred_classes = predicted_bboxes[:, -1].cpu().numpy().tolist()\n        \n        pred['boxes'] =  pred_deltas\n        pred['scores'] = pred_obj_scores\n        pred['labels'] = pred_classes\n        \n        MaP = calculate_map(Ground_Truths, pred, 0.4) # VOC MaP @ 0.4 IOU\n        return MaP\n    def Accuracy(self, images, bboxes):\n        B, _, _, _ = images.shape\n        selected_bboxes = self.forward(self.Yconfig.convert_image(images))\n        MAP = 0.0\n        for idx in range(B):\n            predicted_bounding_boxes= selected_bboxes[idx] # (N', 6)\n            bbox = bboxes[idx].to(self.device) # (N, 5)\n            Mean_Average_Precision = self.MaP(predicted_bounding_boxes, bbox)\n            MAP += Mean_Average_Precision\n        return MAP\n    def validation_run(self, valloader):\n        self.eval()\n        with torch.no_grad():\n            ValMAP = 0\n            count = 0\n            loss = 0\n            for images, bboxes in tqdm.tqdm(valloader):\n                B, _, _, _ = images.shape\n                images = images.to(self.device)\n                ValMAP += self.Accuracy(images.clone(), bboxes)\n                loss += self.compute_loss(images, bboxes).item() \n                count += 1\n            ValMAP /= count\n            loss /= count\n        return ValMAP, loss\n            \n    def training_loop(self, trainloader, valloader, NUM_EPOCHS, display_every = 64):\n        '''\n        Trains the model.\n        '''\n        liveloss = livelossplot.PlotLosses()\n        best_val_acc = 0\n        best_val_loss = 999\n        torch.cuda.empty_cache();\n        for EPOCH in range(NUM_EPOCHS):\n            self.train();\n            logs = {}\n            total_loss = 0.0\n            count = 0\n            for images, bboxes in trainloader:\n                self.optim.zero_grad();\n                B, _, _, _ = images.shape\n                images = images.to(self.device)\n                loss = self.compute_loss(images, bboxes) / B\n                print(f\"STEP: {count}, L: {round(loss.item(), 3)}\")\n                # Get Grid Cells\n                loss.backward();\n                self.optim.step();\n                count += 1\n                total_loss += loss.item();\n                del images\n                del bboxes\n                torch.cuda.empty_cache();\n                if count == display_every:\n                    break\n            logs['loss'] = total_loss / count\n            print(f\"EPOCH: {EPOCH}, total_loss: {total_loss / count}\")\n            # Validation Run Through:\n            self.lr_decay.step()\n            self.lr_decay2.step()\n            '''\n            valMAP, val_loss = self.validation_run(valloader)\n            logs['val_loss'] = val_loss\n            logs['accuracy'] = valMAP\n            '''\n            torch.cuda.empty_cache()\n            liveloss.update(logs)\n            liveloss.send()\n            '''\n            # Save Highest Performing Model\n            if logs['val_loss'] <= best_val_loss:\n                best_val_loss = logs['val_loss']\n                torch.save(self.state_dict(), \"./BestLoss.pth\")\n            if logs['accuracy'] >= best_val_acc:\n                best_val_acc = logs['accuracy']\n                torch.save(self.state_dict(), './BestAcc.pth')\n            print(f\"E: {EPOCH}, L: {round(logs['loss'], 3)}, VL: {round(logs['val_loss'], 3)}, VA: {round(logs['accuracy'], 3)}\")\n            '''","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nyoloSolver = YOLOSolver(IMAGE_SIZE, ANCHOR_BOXES, GRID_SIZES, NUM_OBJ_CLASSES, device, file_path = train_state_dict_path)\nyoloSolver.to(device)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yoloSolver.training_loop(TrainOBJDataloader, ValOBJDataloader, 10, display_every = 48)","execution_count":null,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n  \"See the documentation of nn.Upsample for details.\".format(mode))\n","name":"stderr"},{"output_type":"stream","text":"STEP: 0, L: 216.429\nSTEP: 1, L: 157.321\nSTEP: 2, L: 179.391\nSTEP: 3, L: 171.029\nSTEP: 4, L: 156.688\nSTEP: 5, L: 172.274\nSTEP: 6, L: 153.758\nSTEP: 7, L: 206.983\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save Various Trained Parameters\ntorch.save(yoloSolver.state_dict(), \"./FinalModel.pth\")\ntorch.save(ANCHOR_BOXES, \"./AnchorBoxes.pth\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}