{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install ../input/segmentation-models-pytorch-0-1-3/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n!pip install ../input/segmentation-models-pytorch-0-1-3/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3\n!pip install ../input/segmentation-models-pytorch-0-1-3/timm-0.3.2-py3-none-any.whl\n!pip install ../input/segmentation-models-pytorch-0-1-3/segmentation_models.pytorch.0.1.3/segmentation_models.pytorch.0.1.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nimport os\nimport gc\nimport cv2\nimport pdb\nimport glob\nimport pytz\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\nimport warnings\n\nimport rasterio\nfrom rasterio.windows import Window\n\nimport torch\nimport pytorch_lightning as pl\n\nimport sys\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master/\")\nimport timm\nfrom torchvision.models.resnet import ResNet, Bottleneck\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nfrom albumentations import (\n    Compose,\n    CenterCrop,\n    CLAHE,\n    Resize,\n    Normalize\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height, width = 512, 512\nreduce = 2\nTHRESHOLD = 0.5\nVOTERS = 0.5\nwindow = 1024\nmin_overlap = 256\nuse_TTA = False\nDATA = '../input/hubmap-kidney-segmentation/test/'\nfold0 = '../input/resnet/models/model.pth'\nMODELS = [fold0]\ndf_sample = pd.read_csv('../input/hubmap-kidney-segmentation/sample_submission.csv')\nbatch_size = 2\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask to Rle and Rle to Mask","metadata":{}},{"cell_type":"code","source":"#functions to convert encoding to mask and mask to encoding\ndef enc2mask(encs, shape):\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for m,enc in enumerate(encs):\n        if isinstance(enc,np.float) and np.isnan(enc): continue\n        s = enc.split()\n        for i in range(len(height)//2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1 + m\n    return img.reshape(shape).T\n\ndef mask2enc(mask, n=1):\n    pixels = mask.T.flatten()\n    encs = []\n    for i in range(1,n+1):\n        p = (pixels == i).astype(np.int8)\n        if p.sum() == 0: encs.append(np.nan)\n        else:\n            p = np.concatenate([[0], p, [0]])\n            runs = np.where(p[1:] != p[:-1])[0] + 1\n            runs[1::2] -= runs[::2]\n            encs.append(' '.join(str(x) for x in runs))\n    return encs\n\n#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n#with transposed mask\ndef rle_encode_less_memory(img):\n    #the image should be transposed\n    pixels = img.T.flatten()\n    \n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    \n    return ' '.join(str(x) for x in runs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imagenet statistics Mean and variance\nmean = np.array([0.63701495, 0.4709702, 0.6817423])\nstd = np.array([0.15978882, 0.2245109, 0.14173926])\nidentity = rasterio.Affine(1, 0, 0, 0, 1, 0)\ndef get_transforms(mean, std):\n    list_transforms = [Normalize(mean = mean, std = std), ToTensorV2()]\n    \n    list_trfms = Compose(list_transforms)\n    return list_trfms\n\ndef make_grid(shape, window=256, min_overlap=32):\n    \"\"\"\n        Return Array of size (N,4), where N - number of tiles,\n        2nd axis represente slices: x1,x2,y1,y2 \n    \"\"\"\n    x, y = shape\n    nx = x // (window - min_overlap) + 1\n    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n    x1[-1] = x - window\n    x2 = (x1 + window).clip(0, x)\n    ny = y // (window - min_overlap) + 1\n    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n    y1[-1] = y - window\n    y2 = (y1 + window).clip(0, y)\n    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n    \n    for i in range(nx):\n        for j in range(ny):\n            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n    return slices.reshape(nx*ny,4)\n\nclass HuBMAPDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        if self.data.count != 3:\n            subdatasets = self.data.subdatasets\n            self.layers = []\n            if len(subdatasets) > 0:\n                for i, subdataset in enumerate(subdatasets, 0):\n                    self.layers.append(rasterio.open(subdataset))\n        self.shape = self.data.shape\n        self.mask_grid = make_grid(self.data.shape, window=window, min_overlap=min_overlap)\n        self.transforms = get_transforms(mean, std)\n        \n    def __len__(self):\n        return len(self.mask_grid)\n        \n    def __getitem__(self, idx):\n        x1, x2, y1, y2 = self.mask_grid[idx]\n        if self.data.count == 3:\n            img = data.read([1,2,3], window=Window.from_slices((x1, x2), (y1, y2)))\n            img = np.moveaxis(img, 0, -1)\n        else:\n            img = np.zeros((window, window, 3), dtype=np.uint8)\n            for i, layer in enumerate(self.layers):\n                img[:,:,i] = layer.read(window=Window.from_slices((x1,x2),(y1,y2)))\n        img = cv2.resize(img, (height, width), cv2.INTER_AREA)\n        augmented = self.transforms(image=img)\n        img = augmented['image']\n        vetices = torch.tensor([x1, x2, y1, y2])\n        \n        return img, vetices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize models and load checkpoints","metadata":{}},{"cell_type":"markdown","source":"Full Model","metadata":{}},{"cell_type":"code","source":"class Config:\n    IMAGE_SIZE = 512\n    BATCH_SIZE = 8 # Small-ish batch size needed to support ASPP + FPN\n    NUM_EPOCHS = 48 #200  \n    \n    NUM_WORKERS = 4\n    device = device\n    \n    encoder_type = 'effnet'\n    num_classes = 2\n    use_ASPP = True\n    use_FPN = True\n    attention_type = \"none\"\n    use_linkNet = True # linkNet Blocks should perform better\n    use_decoder_attention = False # Special Attention\n    gate_attention = False# Reduces Instability of Attention Layers at Beginning of Training.\n    act = 'mish' # Actually Performs better than SiLU.\n    bottleneck_type = 'inverse'\n    buffed_decoder = False # Adds BottleNecks and More Processing to the Decoder.\n    buffed_encoder = False # Adds BottleNecks to the Encoder, After the ASPP module.\n    num_blocks = 1\n    use_bam = False # In Testing.\n    bam_dilate = 3\n    use_sem = False # In Testing.\n    reduction = 1 # reduction factor\n    aspp_reduction = 2 # Reduction factor for ASPP Modules.\n    expand = 2 # Expansion Factor ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_weights(layer):\n    # More Optimal Initialization for CNNs\n    for m in layer.modules():\n        if isinstance(m, nn.Conv2d):\n            # Kaiming + ReLU\n            nn.init.kaiming_normal_(m.weight, nonlinearity = 'relu')\n        elif isinstance(m, nn.BatchNorm2d):\n            # 1's and 0's\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mish(pl.LightningModule):\n    # Mish activation, can act as a drop in replacement.\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\ndef replace_all(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, (nn.ReLU, nn.SiLU, timm.models.layers.activations.Swish)):\n            setattr(model, child_name, Mish())\n        else:\n            replace_all(child)\nclass Act(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.act_type = Config.act\n        if self.act_type == 'silu':\n            self.act = nn.SiLU(inplace = True)\n        elif self.act_type == 'mish':\n            self.act = Mish()\n        else:\n            self.act = nn.ReLU(inplace = True)\n    def forward(self, x):\n        return self.act(x)\nclass ConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.act1 = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.act1(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x  \nclass SCSE(pl.LightningModule):\n    # Spatial Channel Squeeze Excite\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features  = in_features\n        self.inner_features = inner_features\n        \n        self.squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.spatial = nn.Conv2d(self.in_features, 1, kernel_size = 1)\n        initialize_weights(self)\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.Act(self.squeeze(mean))\n        excite = torch.sigmoid(self.excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        \n        spatial = torch.sigmoid(self.spatial(x)) * x\n        \n        excited = (excite + spatial) / 2 \n        return excited\n\nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.attention_type = Config.attention_type\n        self.gate_attention = Config.gate_attention\n        if self.attention_type == 'se':\n            self.layer = SqueezeExcite(in_features, inner_features)\n        elif self.attention_type == 'scse':\n            self.layer = SCSE(in_features, inner_features)\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        if self.attention_type == 'none':\n            return x\n        processed = self.layer(x)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * processed + (1 - gamma) * x\n        else:\n            return processed\n\nclass BottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = Config.reduction\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1) \n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        self.Expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        self.SE = Attention(self.in_features, self.in_features // self.reduction)\n\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        gamma = torch.sigmoid(self.gamma)\n        return SE * gamma + (1 - gamma) * x\nclass InverseBottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = Config.reduction\n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1) \n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.SE = Attention(self.inner_features, self.inner_features//self.reduction)\n        self.Squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        se = self.SE(dw)\n        squeeze = self.Squeeze(se)\n        gamma = torch.sigmoid(self.gamma)\n        return squeeze * gamma + (1 - gamma) * x\nclass AstrousConvolution(pl.LightningModule):\n    '''\n    Astrous(More Properly - à trous(at holes in french)) Convolution\n    '''\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation):\n        super().__init__()\n        self.astrous = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act1(self.astrous(x)))\nclass ASPP_Pool(pl.LightningModule):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.pooling_type = 'mean'\n        if self.pooling_type == 'mean':\n            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        else:\n            self.pool = nn.AdaptiveMaxPool2d((1, 1))\n        self.process = nn.Sequential(*[\n            ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1)\n        ])\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # Pool\n        pooled = self.pool(x)\n        processed = self.process(pooled)\n        upsampled = F.interpolate(processed, size = (H, W), mode = 'bilinear')\n        return upsampled\nclass ASPP(pl.LightningModule):\n    '''\n    à trous spatial pooling pyramid block. No further Processing, this should be added later.\n    \n    5 Part:\n    - Normal Conv 1x1\n    - à trous: 4 dilation\n    - à trous: 5 dilation\n    - à trous: 7 dilation\n    '''\n    def __init__(self, in_features, inner_features, out_features, stride = 1):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.num_groups = 4\n        \n        self.pool = ASPP_Pool(self.in_features, self.inner_features)\n        self.conv1 = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.conv2 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 1, self.num_groups, 1, self.stride * 1)\n        self.conv3 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 3, self.num_groups, 1, self.stride * 3)\n        self.conv4 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 5, self.num_groups, 1, self.stride * 5)\n        self.conv5 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 7, self.num_groups, 1, self.stride * 7)\n        \n        self.conv_proj = ConvBlock(self.inner_features * 6, self.out_features, 1, 0, 1, 1)\n        initialize_weights(self)\n    def forward(self, x):\n        pool = self.pool(x)\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(x)\n        conv3 = self.conv3(x)\n        conv4 = self.conv4(x)\n        conv5 = self.conv5(x)\n        \n        concat = torch.cat([pool, conv1, conv2, conv3, conv4, conv5], dim = 1)\n        return self.conv_proj(concat)\nclass BAM(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.dilation = Config.bam_dilate\n        self.SqueezeConv = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DA = AstrousConvolution(self.inner_features, self.inner_features, 3, self.dilation, self.inner_features, 1, self.dilation)\n        self.ExciteConv = ConvBlock(self.inner_features, 1, 1, 0, 1, 1)\n        self.gate_attention = Config.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        mean = torch.squeeze(x, dim = -1)\n        mean = torch.squeeze(mean, dim = -1)\n        \n        squeeze = self.Act(self.Squeeze(mean))\n        excite = self.Excite(squeeze).unsqueeze(-1).unsqueeze(-1)\n        \n        squeeze_conv = self.SqueezeConv(x)\n        DA = self.DA(squeeze_conv)\n        excite_conv = self.ExciteConv(DA)\n        \n        excited = torch.sigmoid((excite_conv + excite) / 2) * x\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excited + (1 - gamma) * x\n        return excited\nclass SEM(pl.LightningModule):\n    def __init__(self, in_features, inner_features, stride = 1):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.stride = stride\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.FS = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        \n        # Dilation ASPP\n        self.conv1 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 1, self.inner_features, 1, self.stride * 1)\n        self.conv2 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 2, self.inner_features, 1, self.stride * 2)\n        self.conv3 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 3, self.inner_features, 1, self.stride * 3)\n        self.conv4 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 4, self.inner_features, 1, self.stride * 4)\n        \n        self.proj = ConvBlock(self.inner_features * 4 + self.in_features, self.in_features, 1, 0, 1, 1)\n    def forward(self, x):\n        squeezed = self.Squeeze(x)\n        FS = self.FS(squeezed)\n        \n        conv1 = self.conv1(FS)\n        conv2 = self.conv2(FS)\n        conv3 = self.conv3(FS)\n        conv4 = self.conv4(FS)\n        \n        concat = torch.cat([x, conv1, conv2, conv3, conv4], dim = 1)\n        proj = self.proj(concat)\n        return proj","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderUNext(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.model = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4)\n        #weights = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnext50_32x4d_swsl')\n        #self.model.load_state_dict(weights.state_dict())\n        \n        self.conv1 = self.model.conv1 # 64\n        self.bn1 = self.model.bn1\n        self.act1 = Mish()\n        self.maxpool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1 # 256\n        self.layer2 = self.model.layer2 # 512\n        # Freeze Initial Layers\n        self.freeze([self.conv1, self.bn1, self.layer1])\n        \n        self.layer3 = self.model.layer3 # 1024\n        self.layer4 = self.model.layer4 # 2048\n        \n        self.aspp_reduction = Config.aspp_reduction\n        self.ASPP = ASPP(2048, 2048 // self.aspp_reduction, 512)\n        del self.model\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x)))\n        layer1 = self.layer1(self.maxpool(features0))\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        layer4 = self.layer4(layer3)\n        layer4 = self.ASPP(layer4)\n        \n        features = [x, features0, layer1, layer2, layer3, layer4]\n        return features\nclass EncoderResNet(pl.LightningModule):\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'resnet34d'\n        self.model = timm.create_model(self.model_name, pretrained = False)\n        # Extract Layers\n        self.enc_dims = [64, 64, 128, 256, 512]\n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.maxpool = self.model.maxpool\n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4\n        \n        self.aspp_reduction = Config.aspp_reduction\n        self.use_aspp = Config.use_ASPP\n        if self.use_aspp:\n            self.ASPP = ASPP(self.enc_dims[-1], self.enc_dims[-1] // self.aspp_reduction, self.enc_dims[-1])\n        \n        \n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x))) # 64 \n        layer1 = self.layer1(self.maxpool(features0)) # 64\n        layer2 = self.layer2(layer1) # 128\n        layer3 = self.layer3(layer2) # 256\n        layer4 = self.layer4(layer3) # 512\n        \n        layer4 = self.ASPP(layer4)\n        features = [x, features0, layer1, layer2, layer3, layer4]\n        return features\n        \nclass EncoderQTPi(pl.LightningModule):\n    def freeze_beginning(self):\n        self.freeze([self.model.encoder])\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.enc_dims = [3, 32, 16, 24, 40, 80, 112, 320]\n        # HYPER PARAMETERS\n        self.base_name = 'timm-efficientnet-b0'\n        self.model_name = 'tf_efficientnet_b0_ns' # Larger Efficientnets provide similar performance. If needed, I can scale this up.\n        # END OF HYPER PARAMETERS\n        self.model = smp.Unet(self.base_name, encoder_weights = None)\n        self.weights = timm.create_model(self.model_name, pretrained = False)\n        self.model.encoder.load_state_dict(self.weights.state_dict())\n        del self.weights\n        # Freeze Layer\n        # Custom Layers(Attention - SE, Dropout2d)\n        self.use_ASPP = Config.use_ASPP\n        self.aspp_reduction = Config.aspp_reduction\n    \n        if self.use_ASPP:\n            self.block7 = nn.Sequential(*[\n                ASPP(self.enc_dims[7], self.enc_dims[7] // self.aspp_reduction, self.enc_dims[7])\n            ])\n        else:\n            self.block7 = nn.Identity()\n        self.buff_encoder = Config.buffed_encoder\n        if self.buff_encoder:\n            self.num_blocks = Config.num_blocks\n            self.expansion = Config.expand\n            self.block8 = nn.Sequential(*[\n                InverseBottleNeckBlock(self.enc_dims[7], self.enc_dims[7] * self.expansion) for i in range(self.num_blocks)\n            ])\n        else:\n            self.block8 = nn.Identity()\n        self.use_bam = Config.use_bam\n        self.reduction = Config.reduction\n        if self.use_bam:\n            # Two BAM blocks added, one after the encoder, and one after ASPP\n            self.bam1 = BAM(self.enc_dims[7], self.enc_dims[7] // self.reduction)\n            self.bam2 = BAM(self.enc_dims[7], self.enc_dims[7] // self.reduction)\n        else:\n            self.bam1 = nn.Identity()\n            self.bam2 = nn.Identity()\n    def forward(self, x):\n        '''\n        x: Tensor(B, 3, 512, 512)\n        Returns:\n        l0: Tensor(B, 16, 256, 256)\n        l1: Tensor(B, 24, 128, 128)\n        l2: Tensor(B, 48, 64, 64)\n        l3: Tensor(B, 120, 32, 32)\n        l4: Tensor(B, 352, 16, 16)\n        l5: Tensor(B, 512, 8, 8) \n        '''\n        x, l0, l1, l2, l3, l4 = tuple(self.model.encoder(x))\n        l4 = self.bam1(l4)\n        l4 = self.block7(l4)\n        l4 = self.block8(l4)\n        l4 = self.bam2(l4)\n        features = [x, l0, l1, l2, l3, l4]\n        return features","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Special Convolutional Blocks for the UNet Decoder:\nclass RecurrentConvolution(pl.LightningModule):\n    '''\n    Recurrent Convolution Block\n    '''\n    def __init__(self, in_features, kernel_size, padding, groups, t = 2):\n        super().__init__()\n        self.in_features = in_features\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.groups = groups\n        self.t = t\n        \n        self.block = ConvBlock(self.in_features, self.in_features, self.kernel_size, self.padding, self.groups, 1)\n    def forward(self, x):\n        for t in range(self.t):\n            if t == 0:\n                x1 = self.block(x)\n            else:\n                x1 = self.block((x + x1) / 2)\n        return x1\nclass RecurrentBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.reduction = Config.reduction\n        self.conv = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.recurrent = RecurrentConvolution(self.inner_features, 3, 1, self.inner_features)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.conv2 = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        proj_down = self.conv(x)\n        recurrent = self.recurrent(proj_down)\n        se = self.SE(recurrent)\n        conv2 = self.conv2(se)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * conv2 + (1 - gamma) * x\nclass GatedSpatialAttention(pl.LightningModule):\n    '''\n    Base Gated Spatial Attention\n    '''\n    def __init__(self, left_features, down_features, inner_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.inner_features = inner_features\n        \n        self.ConvLeft = nn.Conv2d(self.left_features, self.inner_features, kernel_size = 1, bias = False)\n        self.ConvDown = nn.Conv2d(self.down_features, self.inner_features, kernel_size = 1, bias = False)\n        \n        self.BatchNorm = nn.BatchNorm2d(self.inner_features)\n        self.act = Act()\n        \n        self.ConvBlock = nn.Conv2d(self.inner_features, self.left_features, kernel_size = 1, bias = False)\n        self.BatchNorm2 = nn.BatchNorm2d(self.left_features)\n        self.gate_attention = Config.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n        initialize_weights(self)\n    def forward(self, left_features, down_features):\n        conv_left = self.ConvLeft(left_features)\n        conv_down = self.ConvDown(down_features)\n    \n        conv = self.BatchNorm(self.act((conv_down + conv_left) / 2))\n        logits = torch.sigmoid(self.BatchNorm2(self.ConvBlock(conv)))\n        excite = logits * left_features\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excite + (1 - gamma) * left_features\n        return excite\n        \nclass GatedChannelAttention(pl.LightningModule):\n    '''\n    Similar to the Attention UNet, but with SE principles.\n    \n    I find that Conv2d never works for attention.\n    '''\n    def __init__(self, left_features, down_features, inner_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.inner_features = inner_features\n        \n        self.LeftSqueeze = nn.Linear(self.left_features, self.inner_features)\n        self.Act = Act()\n        self.DownSqueeze = nn.Linear(self.down_features, self.inner_features)\n        \n        self.Excite = nn.Linear(self.inner_features, self.left_features)\n        self.gate_attention = Config.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, left_features, down_features):\n        \n        mean_left = torch.mean(left_features, dim = -1)\n        mean_left = torch.mean(mean_left, dim = -1)\n        \n        mean_down = torch.mean(down_features, dim = -1)\n        mean_down = torch.mean(mean_down, dim = -1)\n        \n        squeeze_left = self.LeftSqueeze(mean_left)\n        squeeze_down = self.DownSqueeze(mean_down)\n        \n        squeeze = self.Act((squeeze_left + squeeze_down) / 2)\n        \n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * left_features\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excite + (1 - gamma) * left_features\n        return excite\nclass ChooseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.bottleneck_type = Config.bottleneck_type\n        assert self.bottleneck_type in ['none', 'recurrent', 'inverse', 'bottleneck']\n        if self.bottleneck_type == 'recurrent':\n            self.layer = RecurrentBlock(self.in_features, self.inner_features)\n        elif self.bottleneck_type == 'inverse':\n            self.layer = InverseBottleNeckBlock(self.in_features, self.inner_features)\n        elif self.bottleneck_type == 'bottleneck':\n            self.layer = BottleNeckBlock(self.in_features, self.inner_features)\n        else:\n            self.layer = nn.Identity()\n    def forward(self, x):\n        return self.layer(x)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FPN(pl.LightningModule):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        assert isinstance(self.in_channels, list) \n    \n        self.conv_proj = nn.ModuleList([\n            nn.Sequential(*[\n                ConvBlock(self.in_channels[idx], self.out_channels * 2, 3, 1, 1, 1),\n                ConvBlock(self.out_channels * 2, self.out_channels, 3, 1, 1, 1)\n            ]) \n            for idx in range(len(self.in_channels))])\n        \n    def forward(self, features, last_dim):\n        B, C, H, W = last_dim.shape\n        concatted_features = []\n        for idx in range(len(features)):\n            processed = self.conv_proj[idx](features[idx])\n            upsampled = F.interpolate(processed, size = (H, W), mode = 'bilinear')\n            concatted_features += [upsampled]\n        concat = torch.cat([last_dim] + concatted_features, dim = 1)\n        return concat\n\nclass LinkNetBlockQTPi(pl.LightningModule):\n    def __init__(self, left_features, down_features, out_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        \n        self.PixelShuffle = PixelShuffle_ICNR(self.down_features, self.down_features, blur = True)\n        self.reduction = Config.reduction\n        \n        self.use_attention = Config.use_decoder_attention\n        self.Conv1 = ConvBlock((self.down_features + self.left_features), self.out_features, 3, 1, 1, 1)\n        self.Conv2 = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1)\n        self.attention2 = Attention(self.out_features, self.out_features // self.reduction)\n        if self.use_attention and self.left_features != 0:\n            self.attention1 = GatedChannelAttention(self.left_features, self.down_features, self.left_features // self.reduction)\n        self.buff_decoder = Config.buffed_decoder\n        if self.buff_decoder:\n            # Add a Few Residual Blocks\n            self.num_blocks = Config.num_blocks\n            self.expand = Config.expand \n            self.additional_blocks = nn.Sequential(*[\n                InverseBottleNeckBlock(self.out_features, self.out_features * self.expand) for i in range(self.num_blocks)\n            ])\n    def forward(self, left_features, down_features):\n        down_features = self.PixelShuffle(down_features)\n        if left_features is not None:\n            if self.use_attention:\n                left_features = self.attention1(left_features, down_features)\n            down_features = torch.cat([down_features, left_features], dim = 1)\n        conv1 = self.Conv1(down_features)\n        conv2 = self.Conv2(conv1)\n        attention2 = self.attention2(conv2)\n        if self.buff_decoder:\n            attention2 = self.additional_blocks(attention2) # gives slightly more power to the decoder. Use with risk.\n        return attention2\nclass DecoderBlockQTPi(pl.LightningModule):\n    def __init__(self, left_features, down_features, out_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        self.reduction = Config.reduction\n        \n        self.use_attention = Config.use_decoder_attention\n        self.conv1 = ConvBlock(self.left_features + self.down_features, self.out_features, 3, 1, 1, 1)\n        self.conv2 = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1)\n        self.att2 = Attention(self.out_features, self.out_features // self.reduction)\n        if self.use_attention and self.left_features != 0 and self.down_features != 0:\n            self.attention = GatedChannelAttention(self.left_features, self.down_features, self.left_features // self.reduction)\n        self.buff_decoder = Config.buffed_decoder\n        if self.buff_decoder:\n            self.num_blocks = Config.num_blocks\n            self.expand = Config.expand\n            self.additional_blocks = nn.Sequential(*[\n                InverseBottleNeckBlock(self.out_features, self.out_features * self.expand) for i in range(self.num_blocks)\n            ])\n    def forward(self, left_features, down_features):\n        down_features = F.interpolate(down_features, scale_factor = 2, mode = 'nearest')\n        if left_features is not None:\n            # Attend\n            if self.use_attention:\n                left_features = self.attention(left_features, down_features)\n            down_features = torch.cat([down_features, left_features], dim = 1)\n        conv1 = self.conv1(down_features)\n        conv2 = self.conv2(conv1)\n        conv2 = self.att2(conv2)\n        if self.buff_decoder:\n            conv2= self.additional_blocks(conv2)\n        return conv2\nclass DecoderQTPi(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_classes = Config.num_classes\n        self.encoder_type = Config.encoder_type\n        if self.encoder_type == 'resnet':\n            self.left_dim = [256, 128, 64, 64, 0]\n            self.down_dim = [512, 256, 128, 64, 32, 16]\n        elif self.encoder_type == 'unext':\n            self.left_dim = [1024, 512, 256, 64, 0]\n            self.down_dim = [512, 256, 128, 64, 32, 16]\n        else:    \n            self.left_dim = [112,  40,  24, 32,  0]\n            self.down_dim = [320, 256, 128, 64, 32, 16] \n        \n        self.useLinkNet = Config.use_linkNet\n        def block(idx):\n            if self.useLinkNet:\n                return LinkNetBlockQTPi(self.left_dim[idx], self.down_dim[idx], self.down_dim[idx + 1])\n            else:\n                return DecoderBlockQTPi(self.left_dim[idx], self.down_dim[idx], self.down_dim[idx + 1])\n            \n        self.decoder_blocks = nn.ModuleList([\n            block(i) for i in range(len(self.left_dim)) \n        ])\n        self.use_SEM = Config.use_sem\n        self.aspp_reduction = Config.aspp_reduction\n        if self.use_SEM:\n            # 2 SEM Blocks - Like the 2 BAM Blocks in Encoder - in early decoder to save memory\n            self.sem1 = SEM(self.down_dim[1], self.down_dim[1] // self.aspp_reduction)\n            self.sem2 = SEM(self.down_dim[2], self.down_dim[2] // self.aspp_reduction)\n        else:\n            self.sem1 = nn.Identity()\n            self.sem2 = nn.Identity()\n        self.use_FPN = Config.use_FPN\n        if self.use_FPN:\n            self.FPN = FPN(self.down_dim[0:-2], self.down_dim[-2])\n        self.drop_final = nn.Dropout2d(0.0) # Small DropProb at end 0.1 Default\n        self.drop_middle = nn.Dropout2d(0.0) # Large Drop in Middle, 0.5 for ASPP\n        if self.use_FPN:\n            self.fpn_proj = ConvBlock(self.down_dim[-2] * 5, self.down_dim[-2], 1, 0, 1, 1)\n        \n        self.proj = nn.Conv2d(16, self.num_classes, kernel_size = 3, padding = 1)\n        \n    def forward(self, x0, l0, l1, l2, l3, l4):\n        '''\n        l0: Tensor(B, 16, 128, 128)\n        l1: Tensor(B, 24, 64, 64) - FPN 2x\n        l2: Tensor(B, 40, 32, 32) - FPN 4x\n        l3: Tensor(B, 112, 16, 16) - FPN 8x\n        l4: Tensor(B, 320, 8, 8) - FPN 16x\n        '''\n        # Drop Middle\n        l4 = self.drop_middle(l4)\n        d4 = self.decoder_blocks[0](l3, l4) # 16\n        d4 = self.sem1(d4)\n        \n        d3 = self.decoder_blocks[1](l2, d4) # 32\n        d3 = self.sem2(d3)\n        \n        d2 = self.decoder_blocks[2](l1, d3) # 64\n        \n        d1 = self.decoder_blocks[3](l0, d2) # 128\n        if self.use_FPN:\n            d1 = self.FPN([l4, d4, d3, d2], d1)\n            d1 = self.fpn_proj(d1)\n        d0 = self.decoder_blocks[4](None, d1) # 256\n        # Drop Final\n        d0 = self.drop_final(d0)\n        # Segmentation Head\n        pred = self.proj(d0)\n        return pred","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetQTPi(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder_type = Config.encoder_type\n        if self.encoder_type == 'resnet':\n            self.encoder = EncoderResNet()\n        elif self.encoder_type == 'unext':\n            self.encoder = EncoderUNext()\n        else:\n            self.encoder = EncoderQTPi()\n            \n        self.decoder = DecoderQTPi()\n        if Config.act == 'mish':\n            replace_all(self)\n    def forward(self, x):\n        return torch.squeeze(self.decoder(*self.encoder(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingConfig:\n    lr = 1e-3\n    weight_decay = 1e-3 # Increase Later\n    # Increase Dropout Later\n    NUM_WORKERS = 4\n    patience = 3\n    factor = 0.2\n    use_SWA = False # Stochastic Weight Averaging leads to better results often.\n    mish = True\n    eta_min = 1e-9\n    num_steps = 5\n    clip_grads = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SMP(pl.LightningModule):\n    def __init__(self):\n        # BaseLine SMP Model\n        super().__init__()\n        self.Model = smp.Unet('efficientnet-b0', encoder_weights = None, classes = Config.num_classes)\n    def forward(self, x):\n        return self.Model(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = SMP()\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nmodel = SMP()\nmodel.load_state_dict(torch.load(MODELS[0], map_location = device))\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TTAModel(pl.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n    def forward(self, x):\n        flip0 = x\n        flip1 = x.flip(-1)\n        flip2 = x.flip(-1).flip(-2)\n        flip3 = x.flip(-2)\n        \n        pred0 = self.model(flip0)\n        pred1 = self.model(flip1)\n        pred2 = self.model(flip2)\n        pred3 = self.model(flip3)\n        \n        # UnFlip\n        pred = pred0\n        pred = pred + pred1.flip(-1)\n        pred = pred + pred2.flip(-1).flip(-2)\n        pred = pred + pred3.flip(-2)\n        return pred / 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EnsembleModel(pl.LightningModule):\n    def __init__(self, models):\n        super().__init__()\n        self.models = models\n        #self.use_tta = use_TTA\n        #if self.use_tta:\n        #    models = []\n        #    for model in self.models:\n        #        models += [TTAModel(model)]\n        #    self.models = models\n        #self.num_models = len(self.models)\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            with torch.cuda.amp.autocast():\n                logits = F.softmax(self.models(x), dim = 1)\n                #for model in self.models:\n                #    vals = F.softmax(model(x), dim = 1) # (B, 2, H, W)\n                #    if logits is None:\n                #        logits = vals\n                #    else:\n                #        logits = logits + vals\n                #logits = logits / self.num_models\n                # ArgMax over dims\n                _, selected = torch.max(logits, dim = 1) # (B, H, W)\n                return torch.squeeze(selected)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EnsembleModel(TTAModel(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference","metadata":{}},{"cell_type":"code","source":"names, predictions = [],[]\nfor idx, row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n    imageId = row['id']\n    data = rasterio.open(os.path.join(DATA, imageId+'.tiff'), transform = identity, num_threads='all_cpus')\n    preds = np.zeros(data.shape, dtype=np.uint8)\n    dataset = HuBMAPDataset(data)\n    dataloader = DataLoader(dataset, batch_size = batch_size, num_workers=0, shuffle=False, pin_memory=True)\n    for i, (img, vertices) in enumerate(dataloader):\n        img = img.to(device)\n        pred = model(img)\n        pred = pred.squeeze().cpu().float()\n        pred = pred.numpy()\n        vertices = vertices.numpy()\n        assert not np.isnan(np.sum(pred))\n        for p, vert in zip(pred, vertices):\n            x1, x2, y1, y2 = vert\n            p = cv2.resize(p, (window, window))\n            p = (p > THRESHOLD).astype(np.uint8)\n            preds[x1:x2,y1:y2] += p\n    del dataset\n    del dataloader\n    preds = (preds > VOTERS).astype(np.uint8)\n\n    #convert to rle\n    rle = rle_encode_less_memory(preds)\n    names.append(imageId)\n    predictions.append(rle)\n    del preds\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'id':names,'predicted':predictions}).set_index('id')\ndf = df.reset_index()\ndf.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}