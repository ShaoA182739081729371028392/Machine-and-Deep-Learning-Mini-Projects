{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secret-stereo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:22.689130Z",
     "iopub.status.busy": "2021-05-03T00:22:22.687273Z",
     "iopub.status.idle": "2021-05-03T00:22:22.701789Z",
     "shell.execute_reply": "2021-05-03T00:22:22.701086Z"
    },
    "papermill": {
     "duration": 0.036301,
     "end_time": "2021-05-03T00:22:22.701976",
     "exception": false,
     "start_time": "2021-05-03T00:22:22.665675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tpu v3-8 https://www.kaggle.com/docs/tpu#tpu2\n",
    "P = {}\n",
    "P['EPOCHS'] = 200\n",
    "# 不同base 的efficeinet 似乎只有模型规模的指数不同! \n",
    "# 不是的！！！！！ 不同 baseline 对应了 不同分辨率 https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
    "P['BACKBONE'] = 'efficientnetb2' \n",
    "P['NFOLDS'] = 5\n",
    "P['SEED'] = 7788\n",
    "P['VERBOSE'] = 0 # One lIne per Epoch\n",
    "P['BATCH_COE'] = 12\n",
    "\n",
    "P['DIM'] = 512\n",
    "\n",
    "P['LR'] = 1e-4 # for tpu\n",
    "P['STEPS_COE'] = 1\n",
    "\n",
    "P['patience'] = 30\n",
    "P['extenal'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removed-anchor",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:22.756808Z",
     "iopub.status.busy": "2021-05-03T00:22:22.755632Z",
     "iopub.status.idle": "2021-05-03T00:22:39.958656Z",
     "shell.execute_reply": "2021-05-03T00:22:39.959159Z"
    },
    "papermill": {
     "duration": 17.24163,
     "end_time": "2021-05-03T00:22:39.959377",
     "exception": false,
     "start_time": "2021-05-03T00:22:22.717747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models -q\n",
    "\n",
    "import os\n",
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "import glob\n",
    "\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "import segmentation_models as sm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "import math\n",
    "import random\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-angola",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:39.994705Z",
     "iopub.status.busy": "2021-05-03T00:22:39.994020Z",
     "iopub.status.idle": "2021-05-03T00:22:46.039492Z",
     "shell.execute_reply": "2021-05-03T00:22:46.040076Z"
    },
    "papermill": {
     "duration": 6.064624,
     "end_time": "2021-05-03T00:22:46.040284",
     "exception": false,
     "start_time": "2021-05-03T00:22:39.975660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accelerators:  8\n",
      "BATCH_SIZE:  96\n"
     ]
    }
   ],
   "source": [
    "try: # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "except ValueError: # no TPU found, detect GPUs\n",
    "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "\n",
    "BATCH_SIZE = P['BATCH_COE'] * strategy.num_replicas_in_sync\n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "print(\"BATCH_SIZE: \", str(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-syndicate",
   "metadata": {
    "papermill": {
     "duration": 0.01632,
     "end_time": "2021-05-03T00:22:46.073235",
     "exception": false,
     "start_time": "2021-05-03T00:22:46.056915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## GCS_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "korean-count",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:46.113326Z",
     "iopub.status.busy": "2021-05-03T00:22:46.112525Z",
     "iopub.status.idle": "2021-05-03T00:22:46.117020Z",
     "shell.execute_reply": "2021-05-03T00:22:46.116336Z"
    },
    "papermill": {
     "duration": 0.027319,
     "end_time": "2021-05-03T00:22:46.117160",
     "exception": false,
     "start_time": "2021-05-03T00:22:46.089841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FOLD_FILES(cfg):\n",
    "    GCS_PATH = KaggleDatasets().get_gcs_path(cfg.train_images)\n",
    "    TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "    GCS_PATH = KaggleDatasets().get_gcs_path(cfg.external_images)\n",
    "    EXTERNAL = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "    \n",
    "    return TRAINING_FILENAMES, EXTERNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floating-rates",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:46.159212Z",
     "iopub.status.busy": "2021-05-03T00:22:46.158480Z",
     "iopub.status.idle": "2021-05-03T00:22:47.140839Z",
     "shell.execute_reply": "2021-05-03T00:22:47.140102Z"
    },
    "papermill": {
     "duration": 1.007669,
     "end_time": "2021-05-03T00:22:47.140998",
     "exception": false,
     "start_time": "2021-05-03T00:22:46.133329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIGURATION:\n",
    "    def __init__(self):\n",
    "        self.train_images = '512x512-train'\n",
    "        self.external_images = '512x512tfrecs'\n",
    "        \n",
    "        self.NFOLDS = 5\n",
    "        self.mean_train = np.array([0.63701495, 0.4709702, 0.6817423])\n",
    "        self.mean_train = np.expand_dims(np.expand_dims(self.mean_train, axis = 0), axis = 0)\n",
    "        self.std_train = np.array([0.15978882, 0.2245109, 0.14173926])\n",
    "        self.std_train = np.expand_dims(np.expand_dims(self.std_train, axis = 0), axis = 0)\n",
    "\n",
    "        self.stats = (self.mean_train, self.std_train)\n",
    "        self.TRAINING_FILENAMES, self.EXTERNAL = FOLD_FILES(self)\n",
    "        self.ALL_TRAINING_FILENAMES = self.TRAINING_FILENAMES + self.EXTERNAL\n",
    "Config = CONFIGURATION()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-prevention",
   "metadata": {
    "papermill": {
     "duration": 0.016909,
     "end_time": "2021-05-03T00:22:47.175844",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.158935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datasets pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cardiovascular-shakespeare",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.226900Z",
     "iopub.status.busy": "2021-05-03T00:22:47.225791Z",
     "iopub.status.idle": "2021-05-03T00:22:47.245837Z",
     "shell.execute_reply": "2021-05-03T00:22:47.245213Z"
    },
    "papermill": {
     "duration": 0.053285,
     "end_time": "2021-05-03T00:22:47.246001",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.192716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIM = P['DIM']\n",
    "def dropout(image, mask, CT = 10, CT_WIDTH = 0.05):\n",
    "    # input - one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "\n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    for k in range(CT):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = random.randint(0, DIM - 1)\n",
    "        y = random.randint(0,DIM - 1)\n",
    "\n",
    "        # COMPUTE SQUARE \n",
    "        WIDTH = DIM * CT_WIDTH\n",
    "        ya = int(max(0,y-WIDTH//2))\n",
    "        yb = int(min(DIM,y+WIDTH//2))\n",
    "        xa = int(max(0,x-WIDTH//2))\n",
    "        xb = int(min(DIM,x+WIDTH//2))\n",
    "\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa, 3]) \n",
    "        three = image[ya:yb,xb:DIM,:]\n",
    "\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n",
    "\n",
    "        one = mask[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,1]) \n",
    "        three = mask[ya:yb,xb:DIM,:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        mask = tf.concat([mask[0:ya,:,:],middle,mask[yb:DIM,:,:]],axis=0)\n",
    "\n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n",
    "    image = tf.reshape(image,[DIM,DIM,3])\n",
    "    mask = tf.reshape(mask, [DIM, DIM, 1])\n",
    "    return image, tf.cast(mask, tf.bool) \n",
    "def get_mat(height_zoom, width_zoom):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "        \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    \n",
    "    return zoom_matrix\n",
    "def transform(image,label):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    label = tf.cast(label, dtype = tf.float32)\n",
    "    rot = 360. * tf.random.uniform([1],dtype='float32', minval = -1, maxval = 1)\n",
    "    h_zoom = 1.0 + tf.random.uniform([1],dtype='float32', minval = 0, maxval = 0.5)/10.\n",
    "    w_zoom = 1.0 + tf.random.uniform([1],dtype='float32', minval = 0, maxval = 0.5)/10.\n",
    "    \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(h_zoom,w_zoom) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    trans = tf.transpose(idx3)\n",
    "    d = tf.gather_nd(image, trans)\n",
    "    mask = tf.gather_nd(label, trans)\n",
    "    image = tf.reshape(d,[DIM,DIM,3])\n",
    "    # Round Masks to nearest and return to bool\n",
    "    mask = tf.reshape(mask, [DIM, DIM, 1])\n",
    "    mask = tf.cast(tf.greater(mask, tf.constant(0.5)), tf.float32)\n",
    "    \n",
    "    image = tfa.image.rotate(image, rot, fill_mode = 'reflect')\n",
    "    mask = tfa.image.rotate(mask, rot, fill_mode = 'reflect')\n",
    "    return image, tf.cast(mask, tf.bool)\n",
    "@tf.function\n",
    "def shift_scale_rotate(img, mask):\n",
    "    return transform(img, mask)\n",
    "def random_gamma(img):\n",
    "    gamma = (80, 120)\n",
    "    gamma = random.randint(*gamma) / 100\n",
    "    return tf.image.adjust_gamma(img, gamma = gamma)\n",
    "def normalize(image):\n",
    "    mean, std = Config.stats\n",
    "    image = image - mean\n",
    "    image = image / std\n",
    "    return image\n",
    "def visualize(image, mask):\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(mask, alpha = 0.5)\n",
    "    plt.show()\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "therapeutic-first",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.300206Z",
     "iopub.status.busy": "2021-05-03T00:22:47.294554Z",
     "iopub.status.idle": "2021-05-03T00:22:47.308272Z",
     "shell.execute_reply": "2021-05-03T00:22:47.308812Z"
    },
    "papermill": {
     "duration": 0.045702,
     "end_time": "2021-05-03T00:22:47.309015",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.263313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIM = P['DIM']\n",
    "seed = P['SEED']\n",
    "def _parse_image_function(example_proto, augment = True):\n",
    "    image_feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'mask': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    single_example = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image = tf.reshape( tf.io.decode_raw(single_example['image'],out_type=np.dtype('uint8')), (DIM,DIM, 3))\n",
    "    mask =  tf.reshape(tf.io.decode_raw(single_example['mask'],out_type='bool'),(DIM,DIM,1))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    if augment == True:\n",
    "        print(\"AUGMENTING\")\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_left_right(image)\n",
    "            mask = tf.image.flip_left_right(mask)    \n",
    "        if tf.random.uniform(()) > 0.5:\t        \n",
    "            image = tf.image.flip_up_down(image)\t      \n",
    "            mask = tf.image.flip_up_down(mask)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.rot90(image)\n",
    "            mask = tf.image.rot90(mask)      \n",
    "\n",
    "            \n",
    "        if tf.random.uniform(()) > 0.75:\n",
    "            uniform = tf.random.uniform(())\n",
    "            if uniform < 1/3:\n",
    "                image = tf.image.random_contrast(image, lower=0.8, upper=1.2, seed=seed)\n",
    "            elif uniform < 2/3:\n",
    "                image = tf.image.random_brightness(image, max_delta=0.2, seed=seed)\n",
    "            else:\n",
    "                image = random_gamma(image) \n",
    "\n",
    "        if tf.random.uniform(()) < 0.5:\n",
    "            image, mask = shift_scale_rotate(image, mask)\n",
    "    \n",
    "        if tf.random.uniform(()) < 0.5:\n",
    "            image = tfa.image.gaussian_filter2d(image, filter_shape = (3, 7), sigma = (0.8, 1.4))\n",
    "        \n",
    "        if tf.random.uniform(()) < 0.5:\n",
    "            noise = tf.random.normal(shape=tf.shape(image), mean=0.002, stddev=(50)/(255), dtype=tf.float32)\n",
    "            image = image + noise\n",
    "        if tf.random.uniform(()) < 0.25:\n",
    "            image, mask = dropout(image, mask)\n",
    "    image = normalize(image) # Normalize, both at train and val time\n",
    "    return tf.cast(image, tf.float16), tf.cast(mask, tf.float32)\n",
    "\n",
    "def load_dataset(filenames, augment = True):\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.map(lambda image: _parse_image_function(image, augment = augment), num_parallel_calls=AUTO)           \n",
    "    return dataset\n",
    "\n",
    "def get_training_dataset(fold_idx, index= P['SEED']):\n",
    "    print(\"trainning data load\")\n",
    "    files = []\n",
    "    for idx in range(Config.NFOLDS):\n",
    "        \n",
    "        if idx != fold_idx:\n",
    "            files += [Config.TRAINING_FILENAMES[idx]]\n",
    "    files += Config.EXTERNAL\n",
    "    dataset = load_dataset(files)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(128, seed = index) # High Memory Consumption I think. I think it's tradeoff memory for performance:<\n",
    "    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(fold_idx, ordered=True):\n",
    "    print(\"validate data load\")\n",
    "    files = Config.TRAINING_FILENAMES[fold_idx]\n",
    "    dataset = load_dataset(files, augment = False)\n",
    "    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-obligation",
   "metadata": {
    "papermill": {
     "duration": 0.01628,
     "end_time": "2021-05-03T00:22:47.342125",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.325845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "flexible-motion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.381736Z",
     "iopub.status.busy": "2021-05-03T00:22:47.380763Z",
     "iopub.status.idle": "2021-05-03T00:22:47.384764Z",
     "shell.execute_reply": "2021-05-03T00:22:47.384061Z"
    },
    "papermill": {
     "duration": 0.026168,
     "end_time": "2021-05-03T00:22:47.384913",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.358745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    num_classes = 2\n",
    "    lr =1e-4\n",
    "    weight_decay = 0.0\n",
    "    max_lr = 10\n",
    "    min_lr = 1e-4\n",
    "    \n",
    "    patience = 3\n",
    "    early_stop = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advanced-killing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.428210Z",
     "iopub.status.busy": "2021-05-03T00:22:47.427467Z",
     "iopub.status.idle": "2021-05-03T00:22:47.440243Z",
     "shell.execute_reply": "2021-05-03T00:22:47.439693Z"
    },
    "papermill": {
     "duration": 0.038791,
     "end_time": "2021-05-03T00:22:47.440414",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.401623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CRITERION = keras.losses.CategoricalCrossentropy(from_logits = False, reduction = 'none')\n",
    "def jaccard_loss(y_true, y_pred):\n",
    "    # Y_true: Tensor(B, 512, 512, 1)\n",
    "    # Y_pred: Tensor(B, 512, 512, 2)\n",
    "    y_ones = y_pred[:, :, :, 1]\n",
    "    eps = 1e-8\n",
    "    y_true = tf.squeeze(y_true)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_ones * y_true)\n",
    "    cardinality = tf.reduce_sum(y_ones + y_true)\n",
    "    cardinality = cardinality - intersection\n",
    "    \n",
    "    jaccard = (intersection + eps) / (cardinality + eps)\n",
    "    loss = 1 - jaccard \n",
    "    return tf.math.log((tf.math.exp(loss) + tf.math.exp(-loss)) / 2)\n",
    "def ce_loss(y_true, y_pred):\n",
    "    # Y_true: (B, 512, 512)\n",
    "    # Y_pred: (B, 512, 512, 2)\n",
    "    y_true = tf.squeeze(tf.one_hot(tf.cast(y_true, tf.int32), 2))\n",
    "    loss = CRITERION(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "def loss_fn(y_true, y_pred):\n",
    "    ce = ce_loss(y_true, y_pred)\n",
    "    jaccard = jaccard_loss(y_true, y_pred)\n",
    "    return ce + jaccard\n",
    "class Dice(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'dice', **kwargs):\n",
    "        super().__init__(name = name, **kwargs)\n",
    "        self.inter = tf.Variable(0.0)\n",
    "        self.union = tf.Variable(0.0)\n",
    "    def result(self):\n",
    "        eps = 1e-8\n",
    "        return (2 * self.inter + eps)  / (self.union + eps)\n",
    "    def inter_union(self, y_true, y_pred):\n",
    "        y_true = tf.squeeze(y_true)\n",
    "        y_ones = y_pred[:, :, :, 1]\n",
    "        self.inter.assign_add(tf.reduce_sum(y_ones * y_true))\n",
    "        self.union.assign_add(tf.reduce_sum(y_ones + y_true))\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        self.inter_union(y_true, y_pred)\n",
    "    def reset_states(self):\n",
    "        self.inter.assign(0.0)\n",
    "        self.union.assign(0.0)\n",
    "class SingleDice(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name = 'dice', **kwargs):\n",
    "        super().__init__(name = name, **kwargs)\n",
    "        self.inter = tf.Variable(0.0)\n",
    "        self.union = tf.Variable(0.0)\n",
    "    def result(self):\n",
    "        eps = 1e-8\n",
    "        return (2 * self.inter + eps) / (self.union + eps)\n",
    "    def inter_union(self, y_true, y_pred):\n",
    "        self.inter.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.union.assign_add(tf.reduce_sum(y_true + y_pred))\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        self.inter_union(y_true, y_pred)\n",
    "    def reset_states(self):\n",
    "        self.inter.assign(0.0)\n",
    "        self.union.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wireless-radius",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.491263Z",
     "iopub.status.busy": "2021-05-03T00:22:47.485231Z",
     "iopub.status.idle": "2021-05-03T00:22:47.518316Z",
     "shell.execute_reply": "2021-05-03T00:22:47.518847Z"
    },
    "papermill": {
     "duration": 0.060758,
     "end_time": "2021-05-03T00:22:47.519050",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.458292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParamScheduler:\n",
    "    def __init__(self, start, end, num_iter):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.num_iter = num_iter\n",
    "        self.idx = -1\n",
    "        \n",
    "        \n",
    "    def step(self):\n",
    "        self.idx+=1\n",
    "        return self.func(self.start, self.end, self.idx/self.num_iter)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.idx=-1\n",
    "        \n",
    "    def is_complete(self):\n",
    "        return self.idx >= self.num_iter\n",
    "\n",
    "class CosineScheduler(ParamScheduler):\n",
    "    def func(self, start_val, end_val, pct):\n",
    "        cos_out = np.cos(np.pi * pct) + 1\n",
    "        return end_val + (start_val - end_val)/2 * cos_out\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, init_lr, max_lr, min_lr, momentums=(0.95,0.85), start_div=25., pct_start=0.3, verbose=True, sched=CosineScheduler, end_div=None):\n",
    "        self.max_lr, self.momentums, self.start_div, self.pct_start, self.verbose, self.sched, self.end_div = max_lr, momentums, start_div, pct_start, verbose, sched, end_div\n",
    "        if self.end_div is None:\n",
    "            self.end_div = start_div * 1e4\n",
    "        self.logs = {}\n",
    "        self.min_lr = min_lr\n",
    "        self.init_lr = init_lr\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.num_epochs = self.params['epochs']\n",
    "        self.steps_per_epoch = self.params['steps']\n",
    "        self.start_lr = self.max_lr/self.start_div * self.init_lr\n",
    "        self.end_lr = self.max_lr/self.end_div * self.init_lr\n",
    "        self.num_iter = self.num_epochs * self.steps_per_epoch\n",
    "        self.num_iter_1 = int(self.pct_start*self.num_iter)\n",
    "        self.num_iter_2 = self.num_iter - self.num_iter_1\n",
    "        self.lr_scheds = (self.sched(self.start_lr, self.max_lr * self.init_lr, self.num_iter_1), self.sched(self.max_lr * self.init_lr, self.end_lr, self.num_iter_2))\n",
    "        self.sched_idx = 0\n",
    "        self.optimizer_params_step()   \n",
    "        \n",
    "    def optimizer_params_step(self):\n",
    "        cur_lr = min(self.model.optimizer.lr, self.init_lr)\n",
    "        max_lr = cur_lr * self.max_lr\n",
    "        min_lr = cur_lr * self.min_lr # scale with Reduce LR on Plateau\n",
    "        # Change Param Scheduler\n",
    "        if self.sched_idx == 0:\n",
    "            self.lr_scheds[self.sched_idx].start = cur_lr\n",
    "            self.lr_scheds[self.sched_idx].end = max_lr\n",
    "        else:\n",
    "            self.lr_scheds[self.sched_idx].start = cur_lr\n",
    "            self.lr_scheds[self.sched_idx].end = min_lr\n",
    "        next_lr = self.lr_scheds[self.sched_idx].step()\n",
    "        # update optimizer params\n",
    "        K.set_value(self.model.optimizer.lr, max(next_lr, min_lr))\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.sched_idx >= len(self.lr_scheds):\n",
    "            self.model.stop_training=True\n",
    "            return\n",
    "        self.optimizer_params_step()\n",
    "        if self.lr_scheds[self.sched_idx].is_complete():\n",
    "            self.sched_idx += 1\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= self.num_epochs:\n",
    "            self.model.stop_training=True\n",
    "            return\n",
    "class BestDiceTh(keras.metrics.Metric):\n",
    "    def __init__(self, name = 'best_dice_th', **kwargs):\n",
    "        super().__init__(name = name, **kwargs)\n",
    "        self.th = np.arange(0, 1, 0.01)\n",
    "        self.num_thresh = len(self.th)\n",
    "        self.inter = [tf.Variable(0.0) for i in range(self.num_thresh)]\n",
    "        self.union = [tf.Variable(0.0) for i in range(self.num_thresh)]\n",
    "    def inter_union(self, y_true, y_pred):\n",
    "        y_ones = y_pred[:, :, :, 1]\n",
    "        y_true = tf.squeeze(y_true)\n",
    "        for idx in range(self.num_thresh):\n",
    "            th = self.th[idx]\n",
    "            pred = tf.cast((y_ones > th), tf.float32)\n",
    "            self.inter[idx].assign_add(tf.reduce_sum(pred * y_true))\n",
    "            self.union[idx].assign_add(tf.reduce_sum(pred + y_true))\n",
    "    def result(self):\n",
    "        best_th = 0.0\n",
    "        best = 0.0\n",
    "        eps = 1e-8\n",
    "        for idx in range(self.num_thresh):\n",
    "            dice = (2 * self.inter[idx] + eps) / (self.union[idx] + eps)\n",
    "            if dice > best:\n",
    "                best = tf.cast(dice, tf.float32)\n",
    "                best_th = tf.cast(self.th[idx], tf.float32)\n",
    "        return best_th\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        self.inter_union(y_true, y_pred)\n",
    "    def reset_states(self):\n",
    "        for idx in range(self.num_thresh):\n",
    "            self.inter[idx].assign(0.0)\n",
    "            self.union[idx].assign(0.0)\n",
    "class DiceTh(keras.metrics.Metric):\n",
    "    def __init__(self, name = 'dice_th', **kwargs):\n",
    "        super().__init__(name = name, **kwargs)\n",
    "        self.th = np.arange(0, 1, 0.01)\n",
    "        self.num_thresh = len(self.th)\n",
    "        self.inter = [tf.Variable(0.0) for i in range(self.num_thresh)]\n",
    "        self.union = [tf.Variable(0.0) for i in range(self.num_thresh)]\n",
    "    def reset(self):\n",
    "        for idx in range(self.num_thresh):\n",
    "            self.inter[idx].assign(0.0)\n",
    "            self.union[idx].assign(0.0)\n",
    "    def inter_union(self, y_true, y_pred):\n",
    "        y_ones = y_pred[:, :, :, 1]\n",
    "        y_true = tf.squeeze(y_ones)\n",
    "        for idx in range(self.num_thresh):\n",
    "            th = self.th[idx]\n",
    "            pred = tf.cast((y_ones > th), tf.float32) \n",
    "            self.inter[idx].assign_add(tf.reduce_sum(pred * y_true))\n",
    "            self.union[idx].assign_add(tf.reduce_sum(pred + y_true))\n",
    "    def update_state(self, y_true, y_pred, sample_weight = None):\n",
    "        self.inter_union(y_true, y_pred)\n",
    "    def result(self):\n",
    "        best_dice = 0.0\n",
    "        eps = 1e-8\n",
    "        for idx in range(self.num_thresh):\n",
    "            dice = (2 * self.inter[idx] + eps) / (self.union[idx] + eps) \n",
    "            if dice >best_dice:\n",
    "                best_dice = tf.cast(dice, tf.float32)\n",
    "        return best_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "frank-brand",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.556683Z",
     "iopub.status.busy": "2021-05-03T00:22:47.555702Z",
     "iopub.status.idle": "2021-05-03T00:22:47.562803Z",
     "shell.execute_reply": "2021-05-03T00:22:47.563271Z"
    },
    "papermill": {
     "duration": 0.02758,
     "end_time": "2021-05-03T00:22:47.563471",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.535891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.dice_soft = 0\n",
    "        self.dice_th = 0\n",
    "        self.best_dice_th = 0\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        if logs is not None:\n",
    "            if logs['val_dice'] > self.dice_soft:\n",
    "                self.dice_soft = logs['val_dice']\n",
    "            if logs['val_dice_th'] > self.dice_th:\n",
    "                self.dice_th = logs['val_dice_th']\n",
    "                self.best_dice_th = logs['val_best_dice_th']\n",
    "        print(f\"E: {epoch}, BD: {round(self.dice_soft, 3)}, BDT: {round(self.dice_th, 3)}, BBDT: {round(self.best_dice_th, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "major-substitute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.600907Z",
     "iopub.status.busy": "2021-05-03T00:22:47.599961Z",
     "iopub.status.idle": "2021-05-03T00:22:47.607218Z",
     "shell.execute_reply": "2021-05-03T00:22:47.607699Z"
    },
    "papermill": {
     "duration": 0.027597,
     "end_time": "2021-05-03T00:22:47.607897",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.580300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        if TrainConfig.num_classes == 1:\n",
    "            self.model = sm.Unet(P['BACKBONE'], activation = 'sigmoid', classes = TrainConfig.num_classes, encoder_weights='imagenet')\n",
    "        else:\n",
    "            self.model = sm.Unet(P['BACKBONE'], activation = 'softmax', classes = TrainConfig.num_classes, encoder_weights ='imagenet')\n",
    "        self.layer = tf.keras.layers.Layer(dtype = tf.float32)\n",
    "    def __call__(self, x, training = True):\n",
    "        return self.layer(self.model(x, training = training))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-tribune",
   "metadata": {
    "papermill": {
     "duration": 0.017443,
     "end_time": "2021-05-03T00:22:47.642588",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.625145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "traditional-jewelry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-03T00:22:47.680830Z",
     "iopub.status.busy": "2021-05-03T00:22:47.679815Z",
     "iopub.status.idle": "2021-05-03T03:23:53.881661Z",
     "shell.execute_reply": "2021-05-03T03:23:53.881052Z"
    },
    "papermill": {
     "duration": 10866.222096,
     "end_time": "2021-05-03T03:23:53.881815",
     "exception": false,
     "start_time": "2021-05-03T00:22:47.659719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b2_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "31940608/31936256 [==============================] - 1s 0us/step\n",
      "trainning data load\n",
      "AUGMENTING\n",
      "validate data load\n",
      "\n",
      "Epoch 00001: val_dice improved from -inf to 0.10538, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 0, BD: 0.105, BDT: 0.661, BBDT: 0.97\n",
      "\n",
      "Epoch 00002: val_dice improved from 0.10538 to 0.14559, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 1, BD: 0.146, BDT: 0.661, BBDT: 0.97\n",
      "\n",
      "Epoch 00003: val_dice improved from 0.14559 to 0.17961, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 2, BD: 0.18, BDT: 0.661, BBDT: 0.97\n",
      "\n",
      "Epoch 00004: val_dice improved from 0.17961 to 0.25341, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 3, BD: 0.253, BDT: 0.661, BBDT: 0.97\n",
      "\n",
      "Epoch 00005: val_dice improved from 0.25341 to 0.36691, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 4, BD: 0.367, BDT: 0.661, BBDT: 0.97\n",
      "\n",
      "Epoch 00006: val_dice improved from 0.36691 to 0.52257, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 5, BD: 0.523, BDT: 0.661, BBDT: 0.97\n",
      "\n",
      "Epoch 00007: val_dice improved from 0.52257 to 0.65863, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 6, BD: 0.659, BDT: 0.688, BBDT: 0.62\n",
      "\n",
      "Epoch 00008: val_dice improved from 0.65863 to 0.75178, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 7, BD: 0.752, BDT: 0.786, BBDT: 0.63\n",
      "\n",
      "Epoch 00009: val_dice improved from 0.75178 to 0.80984, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 8, BD: 0.81, BDT: 0.849, BBDT: 0.75\n",
      "\n",
      "Epoch 00010: val_dice improved from 0.80984 to 0.84266, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 9, BD: 0.843, BDT: 0.879, BBDT: 0.32\n",
      "\n",
      "Epoch 00011: val_dice improved from 0.84266 to 0.87129, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 10, BD: 0.871, BDT: 0.915, BBDT: 0.81\n",
      "\n",
      "Epoch 00012: val_dice improved from 0.87129 to 0.89201, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 11, BD: 0.892, BDT: 0.929, BBDT: 0.56\n",
      "\n",
      "Epoch 00013: val_dice improved from 0.89201 to 0.90751, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 12, BD: 0.908, BDT: 0.945, BBDT: 0.66\n",
      "\n",
      "Epoch 00014: val_dice improved from 0.90751 to 0.91348, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 13, BD: 0.913, BDT: 0.951, BBDT: 0.68\n",
      "\n",
      "Epoch 00015: val_dice improved from 0.91348 to 0.92146, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 14, BD: 0.921, BDT: 0.961, BBDT: 0.54\n",
      "\n",
      "Epoch 00016: val_dice improved from 0.92146 to 0.92523, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 15, BD: 0.925, BDT: 0.964, BBDT: 0.76\n",
      "\n",
      "Epoch 00017: val_dice improved from 0.92523 to 0.92580, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 16, BD: 0.926, BDT: 0.964, BBDT: 0.76\n",
      "\n",
      "Epoch 00018: val_dice improved from 0.92580 to 0.92869, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 17, BD: 0.929, BDT: 0.967, BBDT: 0.35\n",
      "\n",
      "Epoch 00019: val_dice improved from 0.92869 to 0.93348, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 18, BD: 0.933, BDT: 0.972, BBDT: 0.53\n",
      "\n",
      "Epoch 00020: val_dice improved from 0.93348 to 0.93551, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 19, BD: 0.936, BDT: 0.976, BBDT: 0.7\n",
      "\n",
      "Epoch 00021: val_dice did not improve from 0.93551\n",
      "E: 20, BD: 0.936, BDT: 0.979, BBDT: 0.5\n",
      "\n",
      "Epoch 00022: val_dice improved from 0.93551 to 0.93797, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 21, BD: 0.938, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00023: val_dice did not improve from 0.93797\n",
      "E: 22, BD: 0.938, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00024: val_dice did not improve from 0.93797\n",
      "E: 23, BD: 0.938, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00025: val_dice did not improve from 0.93797\n",
      "E: 24, BD: 0.938, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00026: val_dice improved from 0.93797 to 0.94065, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 25, BD: 0.941, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00027: val_dice improved from 0.94065 to 0.94167, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 26, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00028: val_dice did not improve from 0.94167\n",
      "E: 27, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00029: val_dice did not improve from 0.94167\n",
      "E: 28, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00030: val_dice improved from 0.94167 to 0.94227, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 29, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00031: val_dice did not improve from 0.94227\n",
      "E: 30, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00032: val_dice did not improve from 0.94227\n",
      "E: 31, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00033: val_dice did not improve from 0.94227\n",
      "E: 32, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00034: val_dice did not improve from 0.94227\n",
      "E: 33, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00035: val_dice did not improve from 0.94227\n",
      "E: 34, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00036: val_dice did not improve from 0.94227\n",
      "E: 35, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00037: val_dice did not improve from 0.94227\n",
      "E: 36, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00038: val_dice did not improve from 0.94227\n",
      "E: 37, BD: 0.942, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00039: val_dice improved from 0.94227 to 0.94304, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 38, BD: 0.943, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00040: val_dice did not improve from 0.94304\n",
      "E: 39, BD: 0.943, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00041: val_dice did not improve from 0.94304\n",
      "E: 40, BD: 0.943, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00042: val_dice did not improve from 0.94304\n",
      "E: 41, BD: 0.943, BDT: 0.984, BBDT: 0.77\n",
      "\n",
      "Epoch 00043: val_dice did not improve from 0.94304\n",
      "E: 42, BD: 0.943, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00044: val_dice improved from 0.94304 to 0.94431, saving model to /kaggle/working/model-fold-0.h5\n",
      "E: 43, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00045: val_dice did not improve from 0.94431\n",
      "E: 44, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00046: val_dice did not improve from 0.94431\n",
      "E: 45, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00047: val_dice did not improve from 0.94431\n",
      "E: 46, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00048: val_dice did not improve from 0.94431\n",
      "E: 47, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00049: val_dice did not improve from 0.94431\n",
      "E: 48, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00050: val_dice did not improve from 0.94431\n",
      "E: 49, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00051: val_dice did not improve from 0.94431\n",
      "E: 50, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00052: val_dice did not improve from 0.94431\n",
      "E: 51, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00053: val_dice did not improve from 0.94431\n",
      "E: 52, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00054: val_dice did not improve from 0.94431\n",
      "E: 53, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00055: val_dice did not improve from 0.94431\n",
      "E: 54, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00056: val_dice did not improve from 0.94431\n",
      "E: 55, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00057: val_dice did not improve from 0.94431\n",
      "E: 56, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00058: val_dice did not improve from 0.94431\n",
      "E: 57, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00059: val_dice did not improve from 0.94431\n",
      "E: 58, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00060: val_dice did not improve from 0.94431\n",
      "E: 59, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00061: val_dice did not improve from 0.94431\n",
      "E: 60, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00062: val_dice did not improve from 0.94431\n",
      "E: 61, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00063: val_dice did not improve from 0.94431\n",
      "E: 62, BD: 0.944, BDT: 0.991, BBDT: 0.98\n",
      "\n",
      "Epoch 00064: val_dice did not improve from 0.94431\n",
      "E: 63, BD: 0.944, BDT: 0.991, BBDT: 0.98\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = P['STEPS_COE'] * 11000 // BATCH_SIZE # approx 11000 images in the dataset.\n",
    "FOLDS_TO_TRAIN = [0]\n",
    "for fold in FOLDS_TO_TRAIN:\n",
    "    # BUILD MODEL\n",
    "    K.clear_session()\n",
    "    with strategy.scope():   \n",
    "        model = Model()\n",
    "        model.compile(optimizer=tfa.optimizers.Lookahead(\n",
    "            tfa.optimizers.AdamW(learning_rate=TrainConfig.lr, weight_decay = TrainConfig.weight_decay)\n",
    "          ),\n",
    "                      \n",
    "          loss = loss_fn,\n",
    "          metrics=[Dice(),\n",
    "                   BestDiceTh(),\n",
    "                   DiceTh()\n",
    "        ])\n",
    "        \n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('/kaggle/working/model-fold-%i.h5'%fold,\n",
    "                                 verbose=1, monitor='val_dice',\n",
    "                                 mode='max',\n",
    "                                save_weights_only = True,\n",
    "                                save_best_only=True)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_dice',mode = 'max', patience=TrainConfig.early_stop, restore_best_weights=True)\n",
    "    reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=TrainConfig.patience, min_lr=TrainConfig.min_lr)\n",
    "    cyclic_lr = OneCycleScheduler(TrainConfig.lr, TrainConfig.max_lr, TrainConfig.min_lr)\n",
    "    logs = LogCallback()\n",
    "    cbs = [\n",
    "        checkpoint,\n",
    "        early_stop,\n",
    "        reduce,\n",
    "        cyclic_lr,\n",
    "        logs\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        get_training_dataset(fold),\n",
    "        epochs = P['EPOCHS'],\n",
    "        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "        callbacks = cbs,\n",
    "        validation_data = get_validation_dataset(fold),\n",
    "        verbose=P['VERBOSE']\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-arrest",
   "metadata": {
    "papermill": {
     "duration": 0.049835,
     "end_time": "2021-05-03T03:23:53.981709",
     "exception": false,
     "start_time": "2021-05-03T03:23:53.931874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-yesterday",
   "metadata": {
    "papermill": {
     "duration": 0.050608,
     "end_time": "2021-05-03T03:23:54.084199",
     "exception": false,
     "start_time": "2021-05-03T03:23:54.033591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-needle",
   "metadata": {
    "papermill": {
     "duration": 0.049942,
     "end_time": "2021-05-03T03:23:54.184258",
     "exception": false,
     "start_time": "2021-05-03T03:23:54.134316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10903.51837,
   "end_time": "2021-05-03T03:23:57.337618",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-03T00:22:13.819248",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}