{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installation and package loading","metadata":{}},{"cell_type":"code","source":"%%capture\nimport torch\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport tqdm.notebook as tqdm\n\n!pip install timm\nimport timm\n\nimport cv2\nimport os\nimport random\nimport math\nimport sys\nimport copy\n\nimport numpy\nimport numpy as np \nimport pandas as pd\nimport lovask\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# imports\n!pip install lycon\nimport lycon\n\n!pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp\nfrom fastai.vision.all import *\nfrom torchvision.models.resnet import ResNet, Bottleneck\nimport glob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class CONFIG():\n    \n    image_size = 512\n    if image_size == 512:\n        train_path_images = \"../input/hubmap-512x512/train/\"\n        train_path_masks = '../input/hubmap-512x512/masks/'\n\n        pseudo_path_images = '../input/512x512ppseudo/test/'\n        pseudo_path_masks = '../input/512x512ppseudo/masks/'\n\n        external_path_images = '../input/external-512x512/images/images/'\n        external_path_masks = '../input/external-512x512/masks/masks/'\n\n\n    else:\n        train_path_images = \"../input/hubmap-256x256/train/\"\n        train_path_masks = '../input/hubmap-256x256/masks/'\n\n        pseudo_path_images = '../input/512x512-pseudo/test/'\n        pseudo_path_masks = '../input/512x512-pseudo/masks/'\n\n        external_path_images = '../input/external-data/images/images/'\n        external_path_masks = '../input/external-data/masks/masks/'\n\n    info_path = \"../input/hubmap-kidney-segmentation/train.csv\"\n    \n    \n    train_samples = -1\n    pseudo_samples = -1\n    external_samples = -1\n    \n    files = pd.read_csv(info_path).id.values\n    mean_train = np.array([0.63701495, 0.4709702, 0.6817423])\n    std_train = np.array([0.15978882, 0.2245109, 0.14173926])\n    stats = (mean_train, std_train)\n    #stats = imagenet_stats\n    n_folds = 5 # 2 Kidneys in Validation, 6 Kidneys train(+ pseudo and External)\n    \n    use_pseudo = True\n    use_external = True\ncfg = CONFIG()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reproducibility:\ndef seed_all():\n    seed = 42\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    # Slight Stochasticity Tradeoff for Quicker Comp.\n    torch.backends.cudnn.benchmark = True # True for faster\n    pl.seed_everything()\n    set_seed(42, True)\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\nseed_all()\n\ntrain_transforms = A.Compose([\n    A.OneOf([\n        A.RandomBrightness(limit=.2, p=1), \n        A.RandomContrast(limit=.2, p=1), \n        A.RandomGamma(p=1)\n    ], p=.5),\n    A.OneOf([\n        A.Blur(blur_limit=3, p=1),\n        A.MedianBlur(blur_limit=3, p=1)\n    ], p=.25),\n    A.OneOf([\n        A.GaussNoise(0.002, p=.5),\n        A.IAAAffine(p=.5),\n    ], p=.25),\n    A.OneOf([\n            A.ElasticTransform(alpha=120, sigma=120 * .05, alpha_affine=120 * .03, p=.5),\n            A.GridDistortion(p=.5),\n            A.OpticalDistortion(distort_limit=2, shift_limit=.5, p=1)                  \n    ], p=.25),\n    A.RandomRotate90(p=.5),\n    A.HorizontalFlip(p=.5),\n    A.VerticalFlip(p=.5),\n    A.Cutout(num_holes=10, \n                max_h_size=int(.1 * cfg.image_size), max_w_size=int(.1 * cfg.image_size), \n                p=.25),\n    A.ShiftScaleRotate(p=.5)\n])    \ntest_transforms = A.Compose([\n    A.Normalize(mean = cfg.stats[0], std = cfg.stats[1]),\n    ToTensorV2()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_folds():\n    files = np.array(os.listdir(cfg.train_path_images))\n    n_folds = cfg.n_folds\n    splitter = KFold(n_splits = n_folds, shuffle = True, random_state = 42)\n    FOLDS = []\n    for train, test in splitter.split(files):\n        FOLDS += [(files[train], files[test])]\n    return FOLDS\nFOLDS = get_folds()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"class TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, files):\n        self.files = files\n        self.num_samples = cfg.train_samples\n        self.actual_length = len(self.files)\n        if self.num_samples == -1:\n            self.num_samples = self.actual_length\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        if self.num_samples != self.actual_length:\n            idx = random.randint(0, self.actual_length - 1)\n        file = self.files[idx]\n        image_file = f\"{cfg.train_path_images}{file}\"\n        mask_file = f\"{cfg.train_path_masks}{file}\"\n        \n        image = lycon.load(image_file)\n        mask = lycon.load(mask_file)[:, :, 0]\n        \n        twos = mask == 2\n        mask[twos] = 1\n        \n        augmented = train_transforms(image = image, mask = mask)\n        image = test_transforms(image = augmented['image'])['image']\n        mask = augmented['mask']\n        return image, mask\nclass ValDataset(torch.utils.data.Dataset):\n    def __init__(self, files):\n        self.files = files\n        self.actual_length = len(self.files)\n    def __len__(self):\n        return self.actual_length\n    def __getitem__(self, idx):\n        file = self.files[idx]\n        image_file = f\"{cfg.train_path_images}{file}\"\n        mask_file = f\"{cfg.train_path_masks}{file}\"\n        \n        image = lycon.load(image_file)\n        mask = lycon.load(mask_file)[:, :, 0]\n        \n        twos = mask == 2\n        mask[twos] = 1\n        \n        image = test_transforms(image = image)['image']\n        return image, mask \nclass OtherDataset(torch.utils.data.Dataset):\n    def __init__(self, file_base_images, file_base_masks, num_samples):\n        self.file_base_images = file_base_images\n        self.file_base_masks = file_base_masks\n        \n        self.num_samples = num_samples\n        \n        self.all_files = os.listdir(self.file_base_masks)\n        self.actual_length = len(self.all_files)\n        if self.num_samples == -1:\n            self.num_samples = self.actual_length\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        if self.num_samples != self.actual_length:\n            idx = random.randint(0, self.actual_length - 1) \n        file = self.all_files[idx]\n        image_file = f\"{self.file_base_images}{file}\"\n        mask_file = f\"{self.file_base_masks}{file}\"\n        \n        image = lycon.load(image_file)\n        mask = lycon.load(mask_file)[:, :, 0]\n        \n        twos = mask == 2\n        mask[twos] = 1\n        \n        augmented = train_transforms(image = image, mask = mask)\n        image = test_transforms(image = augmented['image'])['image']\n        mask = augmented['mask']\n        \n        return image, mask\nclass ConcatDataset(torch.utils.data.Dataset):\n    def __init__(self, files):\n        self.train_dataset = TrainDataset(files)\n        self.pseudo_dataset = OtherDataset(cfg.pseudo_path_images, cfg.pseudo_path_masks, cfg.pseudo_samples)\n        self.external_dataset = OtherDataset(cfg.external_path_images, cfg.external_path_masks, cfg.external_samples)\n        self.use_external = cfg.use_external\n        self.use_pseudo = cfg.use_pseudo\n    def __len__(self):\n        length = len(self.train_dataset)\n        if cfg.use_external:\n            length += len(self.external_dataset)\n        if self.use_pseudo:\n            length += len(self.pseudo_dataset)\n        return length\n    def __getitem__(self, idx):\n        if idx < len(self.train_dataset):\n            return self.train_dataset.__getitem__(idx)\n        elif idx < len(self.external_dataset) + len(self.train_dataset):\n            return self.external_dataset.__getitem__(idx - len(self.train_dataset))\n        else:\n            return self.pseudo_dataset.__getitem__(idx - len(self.train_dataset) - len(self.external_dataset))\nclass DataModule:\n    @classmethod\n    def get_both(cls, idx):\n        train, val = FOLDS[idx]\n        train_dataset = ConcatDataset(train)\n        val_dataset = ValDataset(val)\n        return train_dataset, val_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_image_np(image):\n    plt.imshow(image)\n    plt.show()\ndef display_image(image):\n    plt.imshow(image.cpu().transpose(0, 1).transpose(1, 2))\n    plt.show() ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoder","metadata":{}},{"cell_type":"code","source":"def initialize_weights(layer):\n    # More Optimal Initialization for CNNs\n    for m in layer.modules():\n        if isinstance(m, nn.Conv2d):\n            # Kaiming + ReLU\n            nn.init.kaiming_normal_(m.weight, nonlinearity = 'relu')\n        elif isinstance(m, nn.BatchNorm2d):\n            # 1's and 0's\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mish(pl.LightningModule):\n    # Mish activation, can act as a drop in replacement.\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\ndef replace_all(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, (nn.ReLU, nn.SiLU, timm.models.layers.activations.Swish)):\n            setattr(model, child_name, Mish())\n        else:\n            replace_all(child)\nclass Act(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.act_type = Config.act\n        if self.act_type == 'silu':\n            self.act = nn.SiLU(inplace = True)\n        elif self.act_type == 'mish':\n            self.act = Mish()\n        else:\n            self.act = nn.ReLU(inplace = True)\n    def forward(self, x):\n        return self.act(x)\nclass ConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.act1 = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.act1(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x  \nclass SCSE(pl.LightningModule):\n    # Spatial Channel Squeeze Excite\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features  = in_features\n        self.inner_features = inner_features\n        \n        self.squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.spatial = nn.Conv2d(self.in_features, 1, kernel_size = 1)\n        initialize_weights(self)\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.Act(self.squeeze(mean))\n        excite = torch.sigmoid(self.excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        \n        spatial = torch.sigmoid(self.spatial(x)) * x\n        \n        excited = (excite + spatial) / 2 \n        return excited\n\nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.attention_type = Config.attention_type\n        self.gate_attention = Config.gate_attention\n        if self.attention_type == 'se':\n            self.layer = SqueezeExcite(in_features, inner_features)\n        elif self.attention_type == 'scse':\n            self.layer = SCSE(in_features, inner_features)\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        if self.attention_type == 'none':\n            return x\n        processed = self.layer(x)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * processed + (1 - gamma) * x\n        else:\n            return processed\n\nclass BottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = Config.reduction\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1) \n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        self.Expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        self.SE = Attention(self.in_features, self.in_features // self.reduction)\n\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        gamma = torch.sigmoid(self.gamma)\n        return SE * gamma + (1 - gamma) * x\nclass InverseBottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = Config.reduction\n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1) \n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.SE = Attention(self.inner_features, self.inner_features//self.reduction)\n        self.Squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        se = self.SE(dw)\n        squeeze = self.Squeeze(se)\n        gamma = torch.sigmoid(self.gamma)\n        return squeeze * gamma + (1 - gamma) * x\nclass AstrousConvolution(pl.LightningModule):\n    '''\n    Astrous(More Properly - à trous(at holes in french)) Convolution\n    '''\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation):\n        super().__init__()\n        self.astrous = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act1 = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act1(self.astrous(x)))\nclass ASPP_Pool(pl.LightningModule):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.pooling_type = 'mean'\n        if self.pooling_type == 'mean':\n            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        else:\n            self.pool = nn.AdaptiveMaxPool2d((1, 1))\n        self.process = nn.Sequential(*[\n            ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1)\n        ])\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # Pool\n        pooled = self.pool(x)\n        processed = self.process(pooled)\n        upsampled = F.interpolate(processed, size = (H, W), mode = 'bilinear')\n        return upsampled\nclass ASPP(pl.LightningModule):\n    '''\n    à trous spatial pooling pyramid block. No further Processing, this should be added later.\n    \n    5 Part:\n    - Normal Conv 1x1\n    - à trous: 4 dilation\n    - à trous: 5 dilation\n    - à trous: 7 dilation\n    '''\n    def __init__(self, in_features, inner_features, out_features, stride = 1):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.num_groups = 4\n        \n        self.pool = ASPP_Pool(self.in_features, self.inner_features)\n        self.conv1 = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.conv2 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 1, self.num_groups, 1, self.stride * 1)\n        self.conv3 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 3, self.num_groups, 1, self.stride * 3)\n        self.conv4 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 5, self.num_groups, 1, self.stride * 5)\n        self.conv5 = AstrousConvolution(self.in_features, self.inner_features, 3, self.stride * 7, self.num_groups, 1, self.stride * 7)\n        \n        self.conv_proj = ConvBlock(self.inner_features * 6, self.out_features, 1, 0, 1, 1)\n        initialize_weights(self)\n    def forward(self, x):\n        pool = self.pool(x)\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(x)\n        conv3 = self.conv3(x)\n        conv4 = self.conv4(x)\n        conv5 = self.conv5(x)\n        \n        concat = torch.cat([pool, conv1, conv2, conv3, conv4, conv5], dim = 1)\n        return self.conv_proj(concat)\nclass BAM(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.dilation = Config.bam_dilate\n        self.SqueezeConv = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DA = AstrousConvolution(self.inner_features, self.inner_features, 3, self.dilation, self.inner_features, 1, self.dilation)\n        self.ExciteConv = ConvBlock(self.inner_features, 1, 1, 0, 1, 1)\n        self.gate_attention = Config.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.Act(self.Squeeze(mean))\n        excite = self.Excite(squeeze).unsqueeze(-1).unsqueeze(-1)\n        \n        squeeze_conv = self.SqueezeConv(x)\n        DA = self.DA(squeeze_conv)\n        excite_conv = self.ExciteConv(DA)\n        \n        excited = torch.sigmoid((excite_conv + excite) / 2) * x\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excited + (1 - gamma) * x\n        return excited\nclass SEM(pl.LightningModule):\n    def __init__(self, in_features, inner_features, stride = 1):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.stride = stride\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.FS = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        \n        # Dilation ASPP\n        self.conv1 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 1, self.inner_features, 1, self.stride * 1)\n        self.conv2 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 2, self.inner_features, 1, self.stride * 2)\n        self.conv3 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 3, self.inner_features, 1, self.stride * 3)\n        self.conv4 = AstrousConvolution(self.inner_features, self.inner_features, 3, self.stride * 4, self.inner_features, 1, self.stride * 4)\n        \n        self.proj = ConvBlock(self.inner_features * 4 + self.in_features, self.in_features, 1, 0, 1, 1)\n    def forward(self, x):\n        squeezed = self.Squeeze(x)\n        FS = self.FS(squeezed)\n        \n        conv1 = self.conv1(FS)\n        conv2 = self.conv2(FS)\n        conv3 = self.conv3(FS)\n        conv4 = self.conv4(FS)\n        \n        concat = torch.cat([x, conv1, conv2, conv3, conv4], dim = 1)\n        proj = self.proj(concat)\n        return proj","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simplified Model to Match SMP performance, then improve on it.","metadata":{}},{"cell_type":"code","source":"class EncoderUNext(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.model = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4)\n        weights = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnext50_32x4d_swsl')\n        self.model.load_state_dict(weights.state_dict())\n        \n        self.conv1 = self.model.conv1 # 64\n        self.bn1 = self.model.bn1\n        self.act1 = Mish()\n        self.maxpool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1 # 256\n        self.layer2 = self.model.layer2 # 512\n        # Freeze Initial Layers\n        self.freeze([self.conv1, self.bn1, self.layer1])\n        \n        self.layer3 = self.model.layer3 # 1024\n        self.layer4 = self.model.layer4 # 2048\n        \n        self.aspp_reduction = Config.aspp_reduction\n        self.ASPP = ASPP(2048, 2048 // self.aspp_reduction, 512)\n        del self.model\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x)))\n        layer1 = self.layer1(self.maxpool(features0))\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        layer4 = self.layer4(layer3)\n        layer4 = self.ASPP(layer4)\n        \n        features = [x, features0, layer1, layer2, layer3, layer4]\n        return features\nclass EncoderResNet(pl.LightningModule):\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def unfreeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'resnet34d'\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        # Extract Layers\n        self.enc_dims = [64, 64, 128, 256, 512]\n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.maxpool = self.model.maxpool\n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4\n        \n        self.aspp_reduction = Config.aspp_reduction\n        self.use_aspp = Config.use_ASPP\n        if self.use_aspp:\n            self.ASPP = ASPP(self.enc_dims[-1], self.enc_dims[-1] // self.aspp_reduction, self.enc_dims[-1])\n        \n        \n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x))) # 64 \n        layer1 = self.layer1(self.maxpool(features0)) # 64\n        layer2 = self.layer2(layer1) # 128\n        layer3 = self.layer3(layer2) # 256\n        layer4 = self.layer4(layer3) # 512\n        \n        layer4 = self.ASPP(layer4)\n        features = [x, features0, layer1, layer2, layer3, layer4]\n        return features\n        \nclass EncoderQTPi(pl.LightningModule):\n    def freeze_beginning(self):\n        self.freeze([self.model.encoder])\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.enc_dims = [3, 32, 16, 24, 40, 80, 112, 320]\n        # HYPER PARAMETERS\n        self.base_name = 'efficientnet-b0'\n        # END OF HYPER PARAMETERS\n        self.model = smp.Unet(self.base_name)\n        # Freeze Layer\n        # Custom Layers(Attention - SE, Dropout2d)\n        self.use_ASPP = Config.use_ASPP\n        self.aspp_reduction = Config.aspp_reduction\n    \n        if self.use_ASPP:\n            self.block7 = nn.Sequential(*[\n                ASPP(self.enc_dims[7], self.enc_dims[7] // self.aspp_reduction, self.enc_dims[7])\n            ])\n        else:\n            self.block7 = nn.Identity()\n        self.buff_encoder = Config.buffed_encoder\n        if self.buff_encoder:\n            self.num_blocks = Config.num_blocks\n            self.expansion = Config.expand\n            self.block8 = nn.Sequential(*[\n                InverseBottleNeckBlock(self.enc_dims[7], self.enc_dims[7] * self.expansion) for i in range(self.num_blocks)\n            ])\n        else:\n            self.block8 = nn.Identity()\n        self.use_bam = Config.use_bam\n        self.reduction = Config.reduction\n        if self.use_bam:\n            # Two BAM blocks added, one after the encoder, and one after ASPP\n            self.bam1 = BAM(self.enc_dims[7], self.enc_dims[7] // self.reduction)\n            self.bam2 = BAM(self.enc_dims[7], self.enc_dims[7] // self.reduction)\n        else:\n            self.bam1 = nn.Identity()\n            self.bam2 = nn.Identity()\n    def forward(self, x):\n        '''\n        x: Tensor(B, 3, 512, 512)\n        Returns:\n        l0: Tensor(B, 16, 256, 256)\n        l1: Tensor(B, 24, 128, 128)\n        l2: Tensor(B, 48, 64, 64)\n        l3: Tensor(B, 120, 32, 32)\n        l4: Tensor(B, 352, 16, 16)\n        l5: Tensor(B, 512, 8, 8) \n        '''\n        x, l0, l1, l2, l3, l4 = tuple(self.model.encoder(x))\n        l4 = self.bam1(l4)\n        l4 = self.block7(l4)\n        l4 = self.block8(l4)\n        l4 = self.bam2(l4)\n        features = [x, l0, l1, l2, l3, l4]\n        return features","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Special Convolutional Blocks for the UNet Decoder:\nclass RecurrentConvolution(pl.LightningModule):\n    '''\n    Recurrent Convolution Block\n    '''\n    def __init__(self, in_features, kernel_size, padding, groups, t = 2):\n        super().__init__()\n        self.in_features = in_features\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.groups = groups\n        self.t = t\n        \n        self.block = ConvBlock(self.in_features, self.in_features, self.kernel_size, self.padding, self.groups, 1)\n    def forward(self, x):\n        for t in range(self.t):\n            if t == 0:\n                x1 = self.block(x)\n            else:\n                x1 = self.block((x + x1) / 2)\n        return x1\nclass RecurrentBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.reduction = Config.reduction\n        self.conv = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.recurrent = RecurrentConvolution(self.inner_features, 3, 1, self.inner_features)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.conv2 = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        proj_down = self.conv(x)\n        recurrent = self.recurrent(proj_down)\n        se = self.SE(recurrent)\n        conv2 = self.conv2(se)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * conv2 + (1 - gamma) * x\nclass GatedSpatialAttention(pl.LightningModule):\n    '''\n    Base Gated Spatial Attention\n    '''\n    def __init__(self, left_features, down_features, inner_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.inner_features = inner_features\n        \n        self.ConvLeft = nn.Conv2d(self.left_features, self.inner_features, kernel_size = 1, bias = False)\n        self.ConvDown = nn.Conv2d(self.down_features, self.inner_features, kernel_size = 1, bias = False)\n        \n        self.BatchNorm = nn.BatchNorm2d(self.inner_features)\n        self.act = Act()\n        \n        self.ConvBlock = nn.Conv2d(self.inner_features, self.left_features, kernel_size = 1, bias = False)\n        self.BatchNorm2 = nn.BatchNorm2d(self.left_features)\n        self.gate_attention = Config.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n        initialize_weights(self)\n    def forward(self, left_features, down_features):\n        conv_left = self.ConvLeft(left_features)\n        conv_down = self.ConvDown(down_features)\n    \n        conv = self.BatchNorm(self.act((conv_down + conv_left) / 2))\n        logits = torch.sigmoid(self.BatchNorm2(self.ConvBlock(conv)))\n        excite = logits * left_features\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excite + (1 - gamma) * left_features\n        return excite\n        \nclass GatedChannelAttention(pl.LightningModule):\n    '''\n    Similar to the Attention UNet, but with SE principles.\n    \n    I find that Conv2d never works for attention.\n    '''\n    def __init__(self, left_features, down_features, inner_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.inner_features = inner_features\n        \n        self.LeftSqueeze = nn.Linear(self.left_features, self.inner_features)\n        self.Act = Act()\n        self.DownSqueeze = nn.Linear(self.down_features, self.inner_features)\n        \n        self.Excite = nn.Linear(self.inner_features, self.left_features)\n        self.gate_attention = Config.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, left_features, down_features):\n        \n        mean_left = torch.mean(left_features, dim = -1)\n        mean_left = torch.mean(mean_left, dim = -1)\n        \n        mean_down = torch.mean(down_features, dim = -1)\n        mean_down = torch.mean(mean_down, dim = -1)\n        \n        squeeze_left = self.LeftSqueeze(mean_left)\n        squeeze_down = self.DownSqueeze(mean_down)\n        \n        squeeze = self.Act((squeeze_left + squeeze_down) / 2)\n        \n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * left_features\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excite + (1 - gamma) * left_features\n        return excite\nclass ChooseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.bottleneck_type = Config.bottleneck_type\n        assert self.bottleneck_type in ['none', 'recurrent', 'inverse', 'bottleneck']\n        if self.bottleneck_type == 'recurrent':\n            self.layer = RecurrentBlock(self.in_features, self.inner_features)\n        elif self.bottleneck_type == 'inverse':\n            self.layer = InverseBottleNeckBlock(self.in_features, self.inner_features)\n        elif self.bottleneck_type == 'bottleneck':\n            self.layer = BottleNeckBlock(self.in_features, self.inner_features)\n        else:\n            self.layer = nn.Identity()\n    def forward(self, x):\n        return self.layer(x)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FPN(pl.LightningModule):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        assert isinstance(self.in_channels, list) \n    \n        self.conv_proj = nn.ModuleList([\n            nn.Sequential(*[\n                ConvBlock(self.in_channels[idx], self.out_channels * 2, 3, 1, 1, 1),\n                ConvBlock(self.out_channels * 2, self.out_channels, 3, 1, 1, 1)\n            ]) \n            for idx in range(len(self.in_channels))])\n        \n    def forward(self, features, last_dim):\n        B, C, H, W = last_dim.shape\n        concatted_features = []\n        for idx in range(len(features)):\n            processed = self.conv_proj[idx](features[idx])\n            upsampled = F.interpolate(processed, size = (H, W), mode = 'bilinear')\n            concatted_features += [upsampled]\n        concat = torch.cat([last_dim] + concatted_features, dim = 1)\n        return concat\n\nclass LinkNetBlockQTPi(pl.LightningModule):\n    def __init__(self, left_features, down_features, out_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        \n        self.PixelShuffle = PixelShuffle_ICNR(self.down_features, self.down_features, blur = True)\n        self.reduction = Config.reduction\n        \n        self.use_attention = Config.use_decoder_attention\n        self.Conv1 = ConvBlock((self.down_features + self.left_features), self.out_features, 3, 1, 1, 1)\n        self.Conv2 = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1)\n        self.attention2 = Attention(self.out_features, self.out_features // self.reduction)\n        if self.use_attention and self.left_features != 0:\n            self.attention1 = GatedChannelAttention(self.left_features, self.down_features, self.left_features // self.reduction)\n        self.buff_decoder = Config.buffed_decoder\n        if self.buff_decoder:\n            # Add a Few Residual Blocks\n            self.num_blocks = Config.num_blocks\n            self.expand = Config.expand \n            self.additional_blocks = nn.Sequential(*[\n                InverseBottleNeckBlock(self.out_features, self.out_features * self.expand) for i in range(self.num_blocks)\n            ])\n    def forward(self, left_features, down_features):\n        down_features = self.PixelShuffle(down_features)\n        if left_features is not None:\n            if self.use_attention:\n                left_features = self.attention1(left_features, down_features)\n            down_features = torch.cat([down_features, left_features], dim = 1)\n        conv1 = self.Conv1(down_features)\n        conv2 = self.Conv2(conv1)\n        attention2 = self.attention2(conv2)\n        if self.buff_decoder:\n            attention2 = self.additional_blocks(attention2) # gives slightly more power to the decoder. Use with risk.\n        return attention2\nclass DecoderBlockQTPi(pl.LightningModule):\n    def __init__(self, left_features, down_features, out_features):\n        super().__init__()\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        self.reduction = Config.reduction\n        \n        self.use_attention = Config.use_decoder_attention\n        self.conv1 = ConvBlock(self.left_features + self.down_features, self.out_features, 3, 1, 1, 1)\n        self.conv2 = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1)\n        self.att2 = Attention(self.out_features, self.out_features // self.reduction)\n        if self.use_attention and self.left_features != 0 and self.down_features != 0:\n            self.attention = GatedChannelAttention(self.left_features, self.down_features, self.left_features // self.reduction)\n        self.buff_decoder = Config.buffed_decoder\n        if self.buff_decoder:\n            self.num_blocks = Config.num_blocks\n            self.expand = Config.expand\n            self.additional_blocks = nn.Sequential(*[\n                InverseBottleNeckBlock(self.out_features, self.out_features * self.expand) for i in range(self.num_blocks)\n            ])\n    def forward(self, left_features, down_features):\n        down_features = F.interpolate(down_features, scale_factor = 2, mode = 'nearest')\n        if left_features is not None:\n            # Attend\n            if self.use_attention:\n                left_features = self.attention(left_features, down_features)\n            down_features = torch.cat([down_features, left_features], dim = 1)\n        conv1 = self.conv1(down_features)\n        conv2 = self.conv2(conv1)\n        conv2 = self.att2(conv2)\n        if self.buff_decoder:\n            conv2= self.additional_blocks(conv2)\n        return conv2\nclass DecoderQTPi(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_classes = Config.num_classes\n        self.encoder_type = Config.encoder_type\n        if self.encoder_type == 'resnet':\n            self.left_dim = [256, 128, 64, 64, 0]\n            self.down_dim = [512, 256, 128, 64, 32, 16]\n        elif self.encoder_type == 'unext':\n            self.left_dim = [1024, 512, 256, 64, 0]\n            self.down_dim = [512, 256, 128, 64, 32, 16]\n        else:    \n            self.left_dim = [112,  40,  24, 32,  0]\n            self.down_dim = [320, 256, 128, 64, 32, 16] \n        \n        self.useLinkNet = Config.use_linkNet\n        def block(idx):\n            if self.useLinkNet:\n                return LinkNetBlockQTPi(self.left_dim[idx], self.down_dim[idx], self.down_dim[idx + 1])\n            else:\n                return DecoderBlockQTPi(self.left_dim[idx], self.down_dim[idx], self.down_dim[idx + 1])\n            \n        self.decoder_blocks = nn.ModuleList([\n            block(i) for i in range(len(self.left_dim)) \n        ])\n        self.use_SEM = Config.use_sem\n        self.aspp_reduction = Config.aspp_reduction\n        if self.use_SEM:\n            # 2 SEM Blocks - Like the 2 BAM Blocks in Encoder - in early decoder to save memory\n            self.sem1 = SEM(self.down_dim[1], self.down_dim[1] // self.aspp_reduction)\n            self.sem2 = SEM(self.down_dim[2], self.down_dim[2] // self.aspp_reduction)\n        else:\n            self.sem1 = nn.Identity()\n            self.sem2 = nn.Identity()\n        self.use_FPN = Config.use_FPN\n        if self.use_FPN:\n            self.FPN = FPN(self.down_dim[0:-2], self.down_dim[-2])\n        self.drop_final = nn.Dropout2d(0.0) # Small DropProb at end 0.1 Default\n        self.drop_middle = nn.Dropout2d(0.0) # Large Drop in Middle, 0.5 for ASPP\n        if self.use_FPN:\n            self.fpn_proj = ConvBlock(self.down_dim[-2] * 5, self.down_dim[-2], 1, 0, 1, 1)\n        \n        self.proj = nn.Conv2d(16, self.num_classes, kernel_size = 3, padding = 1)\n        \n    def forward(self, x0, l0, l1, l2, l3, l4):\n        '''\n        l0: Tensor(B, 16, 128, 128)\n        l1: Tensor(B, 24, 64, 64) - FPN 2x\n        l2: Tensor(B, 40, 32, 32) - FPN 4x\n        l3: Tensor(B, 112, 16, 16) - FPN 8x\n        l4: Tensor(B, 320, 8, 8) - FPN 16x\n        '''\n        # Drop Middle\n        l4 = self.drop_middle(l4)\n        d4 = self.decoder_blocks[0](l3, l4) # 16\n        d4 = self.sem1(d4)\n        \n        d3 = self.decoder_blocks[1](l2, d4) # 32\n        d3 = self.sem2(d3)\n        \n        d2 = self.decoder_blocks[2](l1, d3) # 64\n        \n        d1 = self.decoder_blocks[3](l0, d2) # 128\n        if self.use_FPN:\n            d1 = self.FPN([l4, d4, d3, d2], d1)\n            d1 = self.fpn_proj(d1)\n        d0 = self.decoder_blocks[4](None, d1) # 256\n        # Drop Final\n        d0 = self.drop_final(d0)\n        # Segmentation Head\n        pred = self.proj(d0)\n        return pred","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetQTPi(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder_type = Config.encoder_type\n        if self.encoder_type == 'resnet':\n            self.encoder = EncoderResNet()\n        elif self.encoder_type == 'unext':\n            self.encoder = EncoderUNext()\n        else:\n            self.encoder = EncoderQTPi()\n            \n        self.decoder = DecoderQTPi()\n        if Config.act == 'mish':\n            replace_all(self)\n    def forward(self, x):\n        return torch.squeeze(self.decoder(*self.encoder(x)))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Lovask(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n    def forward(self, y_pred, y_true):\n        '''\n        y_pred: Logits\n        y_true: Targets\n        '''\n        y_pred = torch.squeeze(y_pred)\n        return 0.5 * lovask.lovasz_hinge(y_pred, y_true) + 0.5 * lovask.lovasz_hinge(-y_pred, 1 - y_true)\nclass DiceLoss(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        \n    def soft_dice_score(self, output, target, smooth = 0.0):\n        assert output.size() == target.size()\n        eps = 1e-7\n        intersection = torch.sum(output * target) * 2 + eps\n        cardinality = torch.sum(output + target) + eps\n        dice_score =  intersection / cardinality\n        return dice_score\n\n    def forward(self, y_pred, y_true):\n        y_pred = torch.sigmoid(y_pred)\n        y_true = y_true.to(torch.float)\n        loss = 1 - self.soft_dice_score(y_pred, y_true)\n        return torch.log((torch.exp(loss) + torch.exp(-loss)) / 2)\nclass BCE(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.BCEWithLogitsLoss = nn.BCEWithLogitsLoss()\n        self.symmetric = False\n    \n    def symmetric_bce(self, y_pred, y_true):\n        # Symmetric\n        y_true = y_true.to(torch.float)\n        ones = y_true == 1\n        zeros = y_true == 0\n        \n        loss1 = self.BCEWithLogitsLoss(y_pred[ones], torch.ones_like(y_pred[ones], device = y_pred.device))\n        loss2 = self.BCEWithLogitsLoss(y_pred[zeros], torch.zeros_like(y_pred[zeros], device = y_pred.device))\n        return (loss1 + loss2) / 2\n    def regular_bce(self, y_pred, y_true):\n        y_true = y_true.to(torch.float)\n        loss = self.BCEWithLogitsLoss(y_pred, y_true)\n        return loss\n    def forward(self, y_pred, y_true):\n        if self.symmetric:\n            loss = self.symmetric_bce(y_pred, y_true)\n        else:\n            loss = self.regular_bce(y_pred, y_true)\n        return loss\nclass CEJaccard(pl.LightningModule):\n    # Log CosH Jaccard Loss\n    def __init__(self):\n        super().__init__()\n    def jaccard_score(self, y_pred, y_true):\n        y_pred = F.softmax(y_pred, dim = 1)\n        y_pred_ones = y_pred[:, 1, :, :]\n        eps = 1e-8\n        intersection = torch.sum(y_pred_ones * y_true)\n        cardinality = torch.sum(y_pred_ones + y_true)\n        cardinality = cardinality - intersection\n        return (intersection + eps) / (cardinality + eps)\n    def forward(self, y_pred, y_true):\n        jaccard = self.jaccard_score(y_pred, y_true)\n        loss = 1 - jaccard\n        return torch.log((torch.exp(loss) + torch.exp(-loss)) / 2)\nclass CEDice(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_classes = Config.num_classes\n    def dice_score(self, y_pred, y_true):\n        # One Hot Encode y_true \n        y_pred_ones = y_pred[:, 1, :, :]\n        eps = 1e-8\n        intersection = torch.sum(y_pred_ones * y_true) * 2 + eps\n        cardinality = torch.sum(y_pred_ones + y_true) + eps \n        return intersection / cardinality\n    def forward(self, y_pred, y_true):\n        y_pred = F.softmax(y_pred, dim = 1)\n        loss = self.dice_score(y_pred, y_true)\n        loss = 1 - loss\n        return torch.log((torch.exp(loss) + torch.exp(-loss)) / 2)\nclass CrossEntropy(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.CrossEntropy = nn.CrossEntropyLoss()\n    def forward(self, y_pred, y_true):\n        y_true = y_true.to(torch.long)\n        loss = self.CrossEntropy(y_pred, y_true)\n        return loss\nclass CrossEntropyDice(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.CE = CrossEntropy()\n        self.Dice = CEDice()\n    def forward(self, y_pred, y_true):\n        loss = self.CE(y_pred, y_true)\n        loss2 = self.Dice(y_pred, y_true) \n        return loss + loss2\nclass CrossEntropyJaccard(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.CE = CrossEntropy()\n        self.Dice = CEJaccard()\n    def forward(self, y_pred, y_true):\n        loss = self.CE(y_pred, y_true)\n        loss2 = self.Dice(y_pred, y_true) \n        return loss + loss2\n\n\nclass BCEDice(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.BCE = BCE()\n        self.Dice = DiceLoss()\n    \n    def forward(self, y_pred, y_true):\n        bce = self.BCE(y_pred, y_true)\n        dice = self.Dice(y_pred, y_true)\n        return bce + dice\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SMP(pl.LightningModule):\n    def __init__(self):\n        # BaseLine SMP Model\n        super().__init__()\n        self.Model = smp.Unet('efficientnet-b0', classes = Config.num_classes)\n        self.use_ASPP = Config.use_ASPP\n        self.aspp_reduction = Config.aspp_reduction\n        self.input_size = 320\n        if self.use_ASPP:\n            self.ASPP = ASPP(self.input_size, self.input_size // self.aspp_reduction, self.input_size)\n        else:\n            self.ASPP = nn.Identity()\n        self.use_BAM = Config.use_bam\n        self.bam_dilate = Config.bam_dilate\n        self.reduction = Config.reduction\n        \n        if self.use_BAM:\n            self.bam1 = BAM(self.input_size, self.input_size // self.reduction)\n            self.bam2 = BAM(self.input_size, self.input_size // self.reduction)\n        else:\n            self.bam1 = nn.Identity()\n            self.bam2 = nn.Identity()\n    def forward(self, x):\n        features = self.Model.encoder(x)\n        x, l0, l1, l2, l3, l4 = tuple(features)\n        l4 = self.bam1(l4)\n        l4 = self.ASPP(l4)\n        l4 = self.bam2(l4)\n        features = [x, l0, l1, l2, l3, l4]\n        return self.Model.segmentation_head(self.Model.decoder(*features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Losses","metadata":{}},{"cell_type":"code","source":"class Loss(Metric):\n    def __init__(self):\n        super().__init__()\n        self.count = 0\n        self.loss = 0\n        self.num_classes = Config.num_classes\n        if self.num_classes == 2:\n            self.BCE = CrossEntropyJaccard()\n        else:\n            self.BCE = BCEDice()\n    def reset(self):\n        self.count = 0\n        self.loss = 0\n    def accumulate(self, learn):\n        y_pred, y_true = learn.pred, learn.y\n        loss = self.BCE(y_pred, y_true)\n        self.loss += loss.item()\n        self.count += 1\n        return loss\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.loss / self.count, 3)\n        return 0\nclass Dice_soft(Metric):\n    def __init__(self, axis=1): \n        self.axis = axis \n        self.num_classes = Config.num_classes\n    def reset(self): self.inter,self.union = 0,0\n    def accumulate(self, learn):\n        if self.num_classes == 2:\n            pred, targ = F.softmax(learn.pred, dim = 1), learn.y\n            pred_ones = pred[:, 1, :, :]\n            \n            inter = (pred_ones * targ).float().sum().item()\n            union = (torch.sum(pred_ones + targ)).float().item()\n            \n            self.inter += inter\n            self.union += union\n\n        else:\n            pred,targ = torch.sigmoid(learn.pred), learn.y\n            self.inter += (pred*targ).float().sum().item()\n            self.union += (pred+targ).float().sum().item()\n    @property\n    def value(self):\n        dice = 2.0 * self.inter/self.union if self.union > 0 else None\n        print(f'--------DICE: {round(dice, 3)}')\n        return round(dice, 3)\nclass Dice_th(Metric):\n    def __init__(self, ths=np.arange(0.1,0.9,0.05), axis=1): \n        self.axis = axis\n        self.ths = ths\n        self.num_classes = Config.num_classes\n        self.CEDice = CEDice()\n        \n    def reset(self): \n        self.inter = torch.zeros(len(self.ths))\n        self.union = torch.zeros(len(self.ths))\n        \n    def accumulate(self, learn):\n        if self.num_classes == 2:\n            pred, targ = F.softmax(learn.pred, dim = 1), learn.y\n            pred_ones = pred[:, 1, :, :]\n            for i,th in enumerate(self.ths):\n                p_ones = (pred_ones > th).float()\n                self.inter[i] += (p_ones*targ).float().sum().item()\n                self.union[i] += (torch.sum(p_ones + targ)).float().item()\n\n\n        else:\n            pred,targ = torch.sigmoid(learn.pred), learn.y\n            for i,th in enumerate(self.ths):\n\n                p = (pred > th).float()\n                self.inter[i] += (p*targ).float().sum().item()\n                self.union[i] += (p+targ).float().sum().item()\n\n    @property\n    def value(self):\n        dices = torch.where(self.union > 0.0, \n                2.0*self.inter/self.union, torch.zeros_like(self.union))\n        \n        return round(dices.max().item(), 3)\nclass Best_dice_th(Metric):\n    def __init__(self, ths=np.arange(0.2,0.7,0.01), axis=1): \n        self.axis = axis\n        self.ths = ths\n        self.num_classes = Config.num_classes\n    def reset(self): \n        self.inter = torch.zeros(len(self.ths))\n        self.union = torch.zeros(len(self.ths))\n        \n    def accumulate(self, learn):\n        if self.num_classes == 2:\n            pred, targ = F.softmax(learn.pred, dim = 1), learn.y \n            pred_ones = pred[:, 1, :, :]\n            for i,th in enumerate(self.ths):\n                p_ones = (pred_ones > th).float()\n                self.inter[i] += (p_ones*targ).float().sum().item()\n                self.union[i] += (torch.sum(p_ones + targ)).float().item()\n\n        else:\n            pred,targ = torch.sigmoid(learn.pred), learn.y\n            for i,th in enumerate(self.ths):\n                p = (pred > th).float()\n                self.inter[i] += (p*targ).float().sum().item()\n                self.union[i] += (p+targ).float().sum().item()\n\n    @property\n    def value(self):\n        dices = torch.where(self.union > 0.0, \n                2.0*self.inter/self.union, torch.zeros_like(self.union))\n        # Find the Best Dice Threshold\n        dice = self.ths[dices.argmax()]\n        return round(dice.item(), 3)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    IMAGE_SIZE = cfg.image_size\n    BATCH_SIZE = 36 # Small-ish batch size needed to support ASPP + FPN\n    NUM_EPOCHS = 42 #200  \n    \n    NUM_WORKERS = 4\n    device = device\n    \n    encoder_type = 'effnet'\n    num_classes = 2\n    use_ASPP = False\n    use_FPN = False\n    attention_type = \"none\"\n    use_linkNet = True # linkNet Blocks should perform better\n    use_decoder_attention = False # Special Attention\n    gate_attention = True# Reduces Instability of Attention Layers at Beginning of Training.\n    act = 'relu' # Actually Performs better than SiLU.\n    bottleneck_type = 'inverse'\n    buffed_decoder = False # Adds BottleNecks and More Processing to the Decoder.\n    buffed_encoder = False # Adds BottleNecks to the Encoder, After the ASPP module.\n    num_blocks = 1\n    use_bam = False # In Testing.\n    bam_dilate = 3\n    use_sem = False # In Testing.\n    reduction = 1 # reduction factor\n    aspp_reduction = 2 # Reduction factor for ASPP Modules.\n    expand = 2 # Expansion Factor ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClipGrad(Callback):\n    def __init__(self, max_norm):\n        super().__init__()\n        self.max_norm = max_norm\n    def on_backward_end(self, **kwargs):\n        nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.max_norm)\nclass TrainingConfig:\n    lr = 5e-4\n    weight_decay = 0 # Increase Later\n    # Increase Dropout Later\n    NUM_WORKERS = 4\n    patience = 3\n    factor = 0.2\n    eta_min = 1e-9\n    num_steps = 5\n    clip_grads = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nclass Store():\n    def __init__(self, pred, y):\n        self.pred = pred\n        self.y = y\nclass TrainingModel(pl.LightningModule):\n    def unfreeze_model(self):\n        for parameter in self.model.parameters():\n            parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n        self.decay_after = 10\n        # Internal States\n        self.TrainLoss = Loss()\n        self.ValLoss = Loss()\n        self.DiceSoft = Dice_soft()\n        self.DiceTh = Dice_th()\n        self.BestDiceTh = Best_dice_th()\n        \n        self.best = {'val_thresh': 0, \"val_loss\": float('inf'), 'val_dice': 0, 'val_soft': 0}\n        self.EPOCHS = -1\n        self.reset()\n    def reset(self):\n        self.TrainLoss.reset()\n        self.ValLoss.reset()\n        self.DiceSoft.reset()\n        self.DiceTh.reset()\n        self.BestDiceTh.reset()\n        self.EPOCHS += 1\n    def configure_model(self):\n        model = UNetQTPi()\n        return model\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr = TrainingConfig.lr, weight_decay = TrainingConfig.weight_decay)\n        self.lr_decay1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', patience = TrainingConfig.patience, factor = TrainingConfig.factor, min_lr = TrainingConfig.eta_min, verbose = True)\n        self.lr_decay2 = optim.lr_scheduler.CosineAnnealingLR(optimizer, TrainingConfig.num_steps, eta_min = TrainingConfig.eta_min)\n        return optimizer\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.to(self.device)\n        y = y.to(self.device)\n        \n        pred = self.model(x)\n        store = Store(pred, y)\n        loss = self.TrainLoss.accumulate(store)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.to(self.device)\n        y = y.to(self.device)\n        \n        pred = self.model(x)\n        store= Store(pred, y)\n        self.ValLoss.accumulate(store)\n        self.DiceSoft.accumulate(store)\n        self.DiceTh.accumulate(store)\n        self.BestDiceTh.accumulate(store)\n        \n    def print_results(self):\n        print(f'-----------EPOCH {self.EPOCHS}-----------------')\n        ValLoss = self.ValLoss.value\n        TrainLoss = self.TrainLoss.value\n        DiceSoft = self.DiceSoft.value\n        DiceTh = self.DiceTh.value\n        BestDiceTh = self.BestDiceTh.value\n        \n        self.log('dice_soft', DiceSoft)\n        if self.EPOCHS >= self.decay_after:\n            self.lr_decay1.step(DiceSoft)\n        self.lr_decay2.step()\n        \n        if ValLoss <= self.best['val_loss']:\n            self.best['val_loss'] = ValLoss\n            torch.save(self.state_dict(), \"./loss.pth\")\n            print('-----------------------Saved Best Val Loss---------------------------')\n        if DiceTh >= self.best['val_dice']:\n            self.best['val_dice'] = DiceTh\n            self.best['val_thresh'] = BestDiceTh\n            torch.save(self.state_dict(),\"./dice.pth\")\n            print(\"----------------Saved Dice Th--------------------\")\n        if DiceSoft >= self.best['val_soft']:\n            self.best['val_soft'] = DiceSoft\n            torch.save(self.state_dict(), \"./soft.pth\")\n            print(\"-------------Saved Dice Soft-----------------\")\n            \n        print(f\"E: {self.EPOCHS} BT: {self.best['val_thresh']} BS: {self.best['val_soft']} BL: {self.best['val_loss']} BD: {self.best['val_dice']} TL: {TrainLoss} VL: {ValLoss} DS: {DiceSoft} DT: {DiceTh} BDT: {BestDiceTh} \")\n        \n    def validation_epoch_end(self, logs):\n        self.print_results()\n        self.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unfreeze_whole_model(model):\n    for parameter in model.parameters():\n        parameter.requires_grad = True\ndef train_folds(idx, model_path = \"./\", load_prev = None):\n    seed_all()    \n    model = SMP()\n    train, val = DataModule.get_both(idx)\n    # Dataloader and learner\n    dls= DataLoaders.from_dsets(train, val, shuffle = True, pin_memory = True, worker_init_fn = seed_worker, num_workers = TrainingConfig.NUM_WORKERS, bs=Config.BATCH_SIZE, after_batch=Normalize.from_stats(*cfg.stats))\n    if torch.cuda.is_available(): dls.cuda(), model.cuda()\n    metrics = [Dice_soft(), Dice_th(), Best_dice_th()]\n    cbs = [SaveModelCallback(monitor='dice_soft', comp = np.greater), ReduceLROnPlateau(monitor = 'dice_soft', comp = np.greater, patience = TrainingConfig.patience, factor = TrainingConfig.factor), EarlyStoppingCallback(monitor = 'dice_soft', patience = 10, comp = np.greater)]\n    def optimizer(*args, **kwargs):\n        return Lookahead(Adam(model.parameters(), lr = TrainingConfig.lr, wd = TrainingConfig.weight_decay))\n    learn = Learner(dls, model, metrics= metrics, wd=TrainingConfig.weight_decay, loss_func=CrossEntropyJaccard(), opt_func=optimizer , cbs=cbs)\n    learn.to_fp16()\n    learn.fit_one_cycle(Config.NUM_EPOCHS, lr_max = TrainingConfig.lr)\n    del model\n    del train, val\n    del dls\n    del cbs\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_folds(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}