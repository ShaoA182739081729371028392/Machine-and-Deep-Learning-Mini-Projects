{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Depencies ","metadata":{}},{"cell_type":"code","source":"%%capture\nimport torch\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport tqdm.notebook as tqdm\n\n!pip install livelossplot\nimport livelossplot\n\n!pip install timm\nimport timm\n\n!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as seg\n\nimport cv2\nimport PIL\nimport os\nimport random\nimport math\nimport sys\nimport copy\n\nimport numpy as np \nimport pandas as pd\nimport lovask\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n# Import Ranger Optimizer\n%cd ..\n!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer \n%cd Ranger-Deep-Learning-Optimizer\n!pip install -e .\n%cd ..\n%cd working\nimport sys\nsys.path.append(\"../Ranger-Deep-Learning-Optimizer\")\nfrom ranger import Ranger\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reproducibility:\nimport os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\n# Slight Stochasticity Tradeoff for Quicker Comp.\ntorch.backends.cudnn.benchmark = False # True for faster\npl.seed_everything()\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img = '../input/hubmap-256x256/train/'\ntrain_masks = '../input/hubmap-256x256/masks/'\n\npseudo_labelled_img = '../input/pseudolabelledhubmap/test/'\npseudo_labelled_masks = '../input/pseudolabelledhubmap/masks/'\ndef get_images():\n    train_images = np.array(os.listdir(train_img))\n    pseudo_images = np.array(os.listdir(pseudo_labelled_img))\n    return train_images,  pseudo_images\ndef display_image_np(image):\n    plt.imshow(image)\n    plt.show()\ndef display_image(image):\n    plt.imshow(image.cpu().transpose(0, 1).transpose(1, 2))\n    plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"markdown","source":"# Load External and Test Data","metadata":{}},{"cell_type":"code","source":"class Config:\n    IMAGE_SIZE = 256\n    NUM_FOLDS = 4\n    BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 48\n    NUM_EPOCHS = 30\n    device = device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_tensor = ToTensorV2()\ndef get_transforms():\n    # Data Augmentation on Images and Mask\n    train_transforms = A.Compose([\n        A.Flip(p = 0.5),\n        A.OneOf([\n            A.Blur(),\n            A.MultiplicativeNoise(),\n        ], p = 0.7),\n        A.OneOf([\n            A.OpticalDistortion(distort_limit=1.0),\n        #    #A.GridDistortion(num_steps=5, distort_limit=1.),\n        #    #A.ElasticTransform(alpha=3),\n        ], p=0.7),\n        A.CLAHE(),\n        A.ColorJitter(brightness = 0.1, hue = 0.1, contrast = 0.1, saturation = 0.1),\n        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n        A.RandomRotate90(),\n        A.Normalize()\n    ])\n\n    test_transforms = A.Compose([\n        A.Normalize()\n    ])\n    return train_transforms, test_transforms\n\ndef get_splits(all_images):\n    splitter = KFold(n_splits = 75, shuffle = True, random_state = 42)\n    KSPLITS = []\n    count = 0\n    for train, test in splitter.split(all_images):\n        KSPLITS += [(all_images[train], all_images[test])]\n        count += 1\n        if count == Config.NUM_FOLDS:\n            break\n    return KSPLITS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, image_ids, pseudo_ids, transforms):\n        # TO avoid messing up the splits, additional(external + pseudolabelled) is appended to train.\n        self.mask_base = train_masks\n        self.img_base = train_img\n        self.image_ids = image_ids\n        self.transforms = transforms\n        self.len_base = len(self.image_ids)\n        \n        self.pseudo_mask_base = pseudo_labelled_masks\n        self.pseudo_img_base = pseudo_labelled_img\n        self.pseudo_ids = pseudo_ids\n        self.len_pseudo = len(self.pseudo_ids) if self.pseudo_ids is not None else 0\n        \n        # Compute length of dataset\n        self.total_len = self.len_base + self.len_pseudo \n        # Concatenate the images\n        self.total_dataset = self.image_ids\n        if self.len_pseudo != 0:\n            self.total_dataset = np.concatenate([self.total_dataset, self.pseudo_ids])\n        \n    def __len__(self):\n        return self.total_len\n    def __getitem__(self, idx):\n        \n        if idx >= self.len_base:\n           \n            # Must be Pseudo Labelled\n            #print(\"USED PSEUDO\")\n            image_id = self.total_dataset[idx] \n            image_path = self.pseudo_img_base + image_id\n            mask_path = self.pseudo_mask_base + image_id\n\n            image = cv2.imread(image_path)\n            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n                \n        else:\n            image_id = self.total_dataset[idx]\n            image_path = self.img_base + image_id\n            mask_path = self.mask_base + image_id\n\n            # Load in image and masks\n            image = cv2.imread(image_path)\n            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        transform = self.transforms(image = image, mask = mask)\n        \n        image = to_tensor(image = transform['image'])['image']\n        mask = np.transpose(transform['mask'], [0, 1])\n    \n        return image, mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataModule(pl.LightningDataModule):\n    def __init__(self, config = Config):\n        super().__init__()\n        self.config = config\n        self.train_images, self.pseudo_images = get_images()\n        self.KSPLITS = get_splits(self.train_images)\n        self.train_transforms, self.test_transforms = get_transforms()\n    def train_dataloader(self, idx):\n        train, _ = self.KSPLITS[idx]\n        trainDataset = ImageDataset(train, self.pseudo_images, self.test_transforms)\n        dataloader = torch.utils.data.DataLoader(trainDataset, shuffle = True, batch_size = self.config.BATCH_SIZE, worker_init_fn = seed_worker)\n        return dataloader\n    def val_dataloader(self, idx):\n        _, val = self.KSPLITS[idx]\n        valDataset = ImageDataset(val,  None, self.test_transforms) \n        dataloader= torch.utils.data.DataLoader(valDataset, batch_size = self.config.TEST_BATCH_SIZE, worker_init_fn = seed_worker)\n        return dataloader\n    def get_both(self, idx):\n        trainloader = self.train_dataloader(idx) \n        valloader = self.val_dataloader(idx)\n        return trainloader, valloader\ndataModule = DataModule()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = dataModule.get_both(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for images, labels in train:\n    for image_id in range(len(images)):\n        \n        plt.imshow(images[image_id].transpose(0, 1).transpose(1, 2))\n        plt.show()\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ENCODER CNN BLOCKS","metadata":{}},{"cell_type":"code","source":"class ConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, act = 'relu'):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, act = \"relu\"):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.act1(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x * self.gamma + (1 - self.gamma) * x \nclass CBAMChannel(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev \n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features) \n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        max_pool, _ = torch.max(x, dim = -1) \n        max_pool, _ = torch.max(max_pool, dim = -1)\n        \n        squeeze_mean = self.act1(self.Squeeze(mean))\n        excite_mean = self.Excite(squeeze_mean)\n        \n        squeeze_max = self.act1(self.Squeeze(max_pool))\n        excite_max = self.Excite(squeeze_max)\n        \n        excite = torch.sigmoid((excite_mean + excite_max) / 2).unsqueeze(-1).unsqueeze(-1)\n        return excite * x * self.gamma + (1 - self.gamma) * x\n        \nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.attention_type = attention_type\n        assert self.attention_type in ['se', 'cbam', 'none']\n        if self.attention_type == 'se':\n            self.layer = SqueezeExcite(in_features, inner_features, dev, act = act)\n        elif self.attention_type == 'cbam':\n            self.layer = CBAMChannel(in_features, inner_features, dev, act = act)\n        else:\n            self.layer= nn.Identity()\n    def forward(self, x):\n        return self.layer(x)\n\n# Self Attention Blocks\nclass ConvPlusBatchNorm(pl.LightningModule):\n    '''\n    Conv2d + BN, no activation.\n    '''\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride)\n        self.bn1 = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn1(self.conv(x))\nclass SelfAttention(pl.LightningModule):\n    # Non Local Block.\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n        self.K = ConvPlusBatchNorm(self.in_features, self.inner_features * self.num_heads, 3, 1, 1, 1)\n        self.V = ConvPlusBatchNorm(self.in_features, self.inner_features * self.num_heads, 3, 1, 1, 1)\n        self.Q = ConvPlusBatchNorm(self.in_features, self.inner_features * self.num_heads, 3, 1, 1, 1)\n        self.Linear = ConvPlusBatchNorm(self.inner_features * self.num_heads, self.in_features, 3, 1, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        B, C, H, W = x.shape\n        Keys = self.K(x)\n        Values = self.V(x)\n        Queries = self.Q(x)\n        \n        Keys = Keys.reshape(B, self.num_heads, self.inner_features, H, W)\n        Values = Values.reshape(B, self.num_heads, self.inner_features, H, W)\n        Queries = Queries.reshape(B, self.num_heads, self.inner_features, H, W) \n        \n        Keys = Keys.reshape(B * self.num_heads, self.inner_features, H * W)\n        Values = Values.view(B * self.num_heads, self.inner_features, H * W)\n        Queries = Queries.view(B * self.num_heads, self.inner_features, H * W)\n        \n        att_mat = F.softmax(torch.bmm(Keys.transpose(1, 2), Queries) / math.sqrt(self.inner_features))\n        att_vals = torch.bmm(att_mat, Values.transpose(1, 2))\n        \n        scores = att_vals.view(B, self.num_heads, self.inner_features, H, W)\n        scores = scores.view(B, self.num_heads * self.inner_features, H, W)\n        output = self.Linear(scores) \n        return output * self.gamma + (1 - self.gamma) * x\nclass CBAMSqueezeAttend(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size, squeeze_factor = 4, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.out_size = out_size\n        self.squeeze_factor = squeeze_factor\n        self.act = act \n        \n        self.proj = ConvBlock(self.in_features, self.out_features, 3, 1, 1, 1, act = self.act)\n        self.max_pool = nn.MaxPool2d(kernel_size = 5, padding = 2, stride = self.squeeze_factor)\n        self.avg_pool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = self.squeeze_factor)\n        \n        self.Squeeze = ConvBlock(self.out_features, self.inner_features, 3, 1, 1, 1, act = self.act)\n        self.Excite = ConvPlusBatchNorm(self.inner_features, self.out_features, 3, 1, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x)\n        \n        max_pool = self.max_pool(x)\n        avg_pool = self.avg_pool(x)\n        \n        squeeze_max = self.Squeeze(max_pool)\n        squeeze_avg = self.Squeeze(avg_pool) \n        \n        excite_max = self.Excite(squeeze_max)\n        excite_avg = self.Excite(squeeze_avg)\n        \n        excite = torch.sigmoid((excite_max + excite_avg) / 2)\n        excited = avg_pool * self.gamma * excite + (1 - self.gamma) * avg_pool\n    \n        # Interpolate Upward\n        excited = F.interpolate(excited, size = (self.out_size, self.out_size), mode = 'nearest')\n        return excited\nclass SESqueezeAttend(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size, squeeze_factor = 4, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_size = out_size\n        self.squeeze_factor = squeeze_factor \n        self.act = act\n        self.avg_pool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = squeeze_factor)\n        \n        self.proj = ConvBlock(self.in_features, self.out_features, 3, 1, 1, 1, act = self.act)\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 1, act = self.act)\n        self.Excite = ConvPlusBatchNorm(self.inner_features, self.in_features, 3, 1, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        B, C, H, W = x.shape\n        x = self.proj(x)\n        pooled = self.avg_pool(x)\n        \n        squeeze = self.Squeeze(pooled)\n        excite = torch.sigmoid(self.Excite(squeeze))\n        \n        excited = self.gamma * pooled * excite + (1 - self.gamma) * pooled \n        \n        # Interpolate Back Up.\n        excited = F.interpolate(excited, size = (self.out_size, self.out_size), mode = 'nearest')\n        return excited\nclass SqueezeAttend(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size, squeeze_factor = 4, act = 'relu', attention_type = 'se'):\n        super().__init__()\n        \n        self.attention_type = attention_type\n        assert self.attention_type in ['se', 'cbam', 'none']\n        if self.attention_type == 'se':\n            self.layer = SESqueezeAttend(in_features, inner_features, out_features, out_size, squeeze_factor = squeeze_factor, act = act)\n        elif self.attention_type =='cbam':\n            self.layer = CBAMSqueezeAttend(in_features, inner_features, out_features, out_size, squeeze_factor = squeeze_factor, act = act)\n        else:\n            self.layer = nn.Identity()\n    def forward(self,x):\n        return self.layer(x)\n\nclass BottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, attention_type = 'se', stochastic_depth = 0, act = 'relu'):\n        super().__init__()\n        self.stochastic_depth = stochastic_depth\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev\n        self.attention_type = attention_type\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1, act = act) \n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1, act = act)\n        self.Expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1, act = act)\n        self.SE = Attention(self.in_features, self.in_features // 4, self.dev, attention_type = self.attention_type)\n\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        if self.training and random.random() < self.stochastic_depth:\n            return x\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        return SE * self.gamma + (1 - self.gamma) * x\n        \nclass InverseBottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, attention_type = 'se', stochastic_depth = 0, act = 'relu'):\n        super().__init__()\n        self.stochastic_depth = stochastic_depth\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev\n        self.attention_type = attention_type\n        \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1, act = act) \n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1, act = act)\n        self.SE = Attention(self.inner_features, self.inner_features//4, self.dev, attention_type = self.attention_type, act = act)\n        self.Squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1, act = act)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        if self.training and random.random() < self.stochastic_depth:\n            return x\n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        se = self.SE(dw)\n        squeeze = self.Squeeze(se)\n        return squeeze * self.gamma + (1 - self.gamma) * x\n\nclass DownSamplerBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride, dev, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.dev = dev\n        self.attention_type = attention_type\n        \n        self.pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride =stride)\n        self.pool_conv = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1, act = act)\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1, act = act) \n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1, act = act)\n        self.Expand = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, self.stride, act = act)\n        self.SE = Attention(self.out_features, self.out_features // 4, self.dev, attention_type = self.attention_type, act = act)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        pool = self.pool_conv(self.pool(x))\n        conv_features = self.SE(self.Expand(self.Process(self.Squeeze(x))))\n        return pool * self.gamma + pool * (1 - self.gamma)\nclass DownSamplerInverse(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride, dev, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.dev = dev\n        self.attention_type = attention_type\n    \n        self.pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.pool_conv = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1, act = act)\n        \n        self.squeeze = ConvBlock(self.in_features, self.inner_features,1, 0, 1, 1, act = act)\n        self.process = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1, act = act)\n        self.SE = Attention(self.inner_features, self.inner_features // 4, self.dev, act = act)\n        self.expand = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, self.stride, act = act)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        pool = self.pool_conv(self.pool(x))\n        conv = self.expand(self.SE(self.process(self.squeeze(x))))\n        return conv * self.gamma + (1 - self.gamma) * pool\n\nclass AstrousConvolution(pl.LightningModule):\n    '''\n    Astrous(More Properly - à trous(at holes in french)) Convolution\n    '''\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation, act = 'relu'):\n        super().__init__()\n        self.astrous = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.astrous(x)))\nclass ASPP(pl.LightningModule):\n    '''\n    à trous spatial pooling pyramid block. No further Processing, this should be added later.\n    \n    5 Part:\n    - Normal Conv\n    - à trous: 3 dilation\n    - à trous: 5 dilation\n    - à trous: 7 dilation\n    '''\n    def __init__(self, in_features, inner_features, out_features, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.conv1 = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 2, act = act)\n        self.conv2 = AstrousConvolution(self.in_features, self.inner_features, 3, 1, 1, 1, 3, act = act)\n        self.conv3 = AstrousConvolution(self.in_features, self.inner_features, 3, 3, 1, 1, 5, act = act)\n        self.conv4 = AstrousConvolution(self.in_features, self.inner_features, 3, 5, 1, 1, 7, act = act) \n        \n        self.proj = ConvBlock(4 * self.inner_features, self.out_features, 1, 0, 1, 1, act = act)\n        \n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(x)\n        conv3 = self.conv3(x)\n        conv4 = self.conv4(x)\n        # concat\n        concat = torch.cat([conv1, conv2, conv3, conv4], dim = 1)\n        proj = self.proj(concat)\n        return proj","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ENCODER","metadata":{}},{"cell_type":"markdown","source":"Various Encoder Types(BaseLine, ResNet, EffNet)","metadata":{}},{"cell_type":"code","source":"# ResNet Based Complex Encoder(+ SE + dropout2d)\nclass ResNetEncoderAlpha(pl.LightningModule):\n    '''\n    ResNet34d encoder + SE and Dropout\n    \n    I would scale the model larger(ex. ResNet50), but larger models have the wrong dimensions.\n    '''\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True \n    def increase_drop(self):\n        self.drop_prob += self.increase_dropout\n    def __init__(self, increase_drop, attention_type):\n        super().__init__()\n        # HYPER PARAMETERS\n        self.increase_dropout = increase_drop\n        self.attention_type = attention_type\n        self.drop_prob = 0\n        self.model_name = 'resnet34d'\n        # END OF HYPER PARAMETERS\n        self.model = timm.create_model(self.model_name, pretrained = True) \n        # Extract Layers\n        self.conv1 = self.model.conv1 # (B, 64, 128, 128)\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.maxpool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1 # (b, 64, 64, 64)\n        self.layer2 = self.model.layer2 # (b, 128, 32, 32)\n        self.layer3 = self.model.layer3 # (b, 256, 16, 16)\n        self.layer4 = self.model.layer4 # (b, 512, 8, 8)\n        \n        self.Dropout0 = nn.Dropout2d(self.drop_prob)\n        self.Attention0 = Attention(64, 16, self.device, attention_type = self.attention_type)\n        self.increase_drop()\n        \n        self.Dropout1 = nn.Dropout2d(self.drop_prob)\n        self.increase_drop()\n        self.Attention1 = Attention(64, 16, self.device, attention_type = self.attention_type)\n        self.Dropout2 = nn.Dropout2d(self.drop_prob)\n        self.Attention2 = Attention(128, 32, self.device, attention_type = self.attention_type)\n        self.increase_drop()\n        self.Dropout3 = nn.Dropout2d(self.drop_prob)\n        self.Attention3 = Attention(256, 64, self.device, attention_type = self.attention_type)\n        self.increase_drop()\n        self.Dropout4 = nn.Dropout2d(self.drop_prob)\n        self.Attention4 = Attention(512, 128, self.device, attention_type = self.attention_type)\n        \n        del self.model\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x)))\n        features0 = self.Dropout0(features0)\n        features0 = self.Attention0(features0)\n        \n        layer1 = self.layer1(self.maxpool(features0))\n        layer1 = self.Dropout1(layer1)\n        layer1 = self.Attention1(layer1)\n        \n        layer2 = self.layer2(layer1)\n        layer2 = self.Dropout2(layer2)\n        layer2 = self.Attention2(layer2)\n        \n        layer3 = self.layer3(layer2)\n        layer3 = self.Dropout3(layer3)\n        layer3 = self.Attention3(layer3)\n        \n        layer4 = self.layer4(layer3)\n        layer4 = self.Dropout4(layer4)\n        layer4 = self.Attention4(layer4)\n        \n        return x, features0, layer1, layer2, layer3, layer4\nclass EffNetEncoderAlpha(pl.LightningModule):\n    '''\n    EfficientNet-b4 based Encoder(SE + Dropout)\n    '''\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True \n    def increase_drop(self):\n        self.drop_prob += self.increase_drop\n    def __init__(self, increase_drop, attention_type, act = 'relu'):\n        super().__init__()\n        self.drop_prob = 0\n        self.act = act \n        self.increase_drop = increase_drop\n    \n        self.attention_type = attention_type\n        \n        self.model_name = 'tf_efficientnet_b4_ns'\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        \n        self.conv1 = self.model.conv_head\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n    \n        self.block0 = self.model.blocks[0] # 24\n        self.block1 = self.model.blocks[1] # 32\n        self.block2 = self.model.blocks[2] # 56\n        self.block3 = self.model.blocks[3] # 112\n        self.block4 = self.model.blocks[4] # 160 \n        self.block5 = self.model.blocks[5] # 272\n        self.block6 = self.model.blocks[6] # 448\n        \n        # Custom Layer\n        self.Dropout0 = nn.Dropout2d(self.drop_prob)\n        self.Attention0 = Attention(24, 6, self.device, attention_type= self.attention_type, act = self.act)\n        self.increase_dropout()\n        \n        self.Dropout1 = nn.Dropout2d(self.drop_prob)\n        self.Attention1 = Attention(32, 8, self.device, attention_type = self.attention_type, act = self.act)\n        self.increase_dropout()\n        \n        self.Dropout2 = nn.Dropout2d(self.drop_prob)\n        self.Attention2 = Attention(56, 16, self.device, attention_type = self.attention_type, act = self.act)\n        self.increase_dropout()\n    \n        self.Dropout3 = nn.Dropout2d(self.drop_prob)\n        self.Attention3 = Attention(160, 48, self.device, attention_type = self.attention_type, act = self.act)\n        self.increase_dropout()\n        \n        self.Dropout4 = nn.Dropout2d(self.drop_prob)\n        self.Attention4 = Attention(448, 128, self.device, attention_type = self.attention_type, act = self.act)\n        \n        # Proj Blocks(To Match ResBlocks)\n        self.proj0 = ConvBlock(24, 64, 3, 1, 1, 1)\n        self.proj1 = ConvBlock(32, 64, 3, 1, 1, 1)\n        self.proj2 = ConvBlock(56, 128, 3, 1, 1, 1)\n        self.proj3 = ConvBlock(160, 256, 3, 1, 1, 1)\n        self.proj4 = ConvBlock(448, 512, 3, 1, 1, 1)\n    def forward(self, x):\n        '''\n        l0: (b, 3, 256, 256)\n        l1: (B, 64, 128, 128)\n        l2: (B, 64, 64, 64)\n        l3: (B, 128, 32, 32)\n        l4: (B, 256, 16, 16)\n        l5: (B, 512, 8, 8)\n        '''\n        features0 = self.bn1(self.act1(self.conv1(x))) # (B, 48, 128, 128)\n        block0 = self.block0(features0) # (B, 24, 128, 128)\n        block0 = self.Dropout0(block0)\n        block0 = self.Attention0(block0)\n        \n        block1 = self.block1(block0) # (B, 32, 64, 64)\n        block1 = self.Dropout1(block1)\n        block1 = self.Attention1(block1)\n        \n        block2 = self.block2(block1) # (B, 56, 32, 32)\n        block2 = self.Dropout2(block2)\n        block2 = self.Attention2(block2)\n        \n        block3 = self.block3(block2) # (B, 112, 16, 16)\n        block4 = self.block4(block3) # (B, 160, 16, 16)\n        block4 = self.Dropout3(block4)\n        block4 = self.Attention3(block4)\n        \n        block5 = self.block5(block4) # (B, 272, 8, 8)\n        block6 = self.block6(block5) # (B, 448, 8, 8)\n        block6 = self.Dropout4(block6)\n        block6 = self.Attention4(block6)\n        \n        # Project Block\n        l1 = self.proj0(block0)\n        l2 = self.proj1(block1)\n        l3 = self.proj2(block2)\n        l4 = self.proj3(block4)\n        l5 = self.proj4(block6)\n        return x, l1, l2, l3, l4, l5   \n        \n\nclass EncoderQTPi(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def increase_dropout(self):\n        self.drop_prob += self.increase_drop\n    def increase_stochasticity(self):\n        '''\n        Increases the Rate of Stochastic Depth Dropout(Deeper should drop more.)\n        '''\n        self.stochastic_depth += self.increase_stoc\n    def __init__(self, increase_drop, increase_stoc, attention_type, use_ASPP = False, encoder_type = 'resnet', act = 'relu'):\n        # Suggested Increase_drop = 0.05, increase_stoc = 0.1\n        super().__init__()\n        self.act = act\n        self.encoder_type = encoder_type\n        assert self.encoder_type in ['resnet', 'effnet']\n        self.increase_drop = increase_drop\n        self.drop_prob = 5 * self.increase_drop\n        \n        self.stochastic_depth = 0\n        self.increase_stoc = increase_stoc\n        \n        self.attention_type = attention_type\n        \n        self.backbone = ResNetEncoderAlpha(self.increase_drop, self.attention_type) if self.encoder_type == 'resnet' else EffNetEncoderAlpha(self.increase_drop, self.attention_type, act = self.act)\n        \n        self.use_ASPP = use_ASPP\n        \n        def add_block_stoc(x):\n            self.increase_stochasticity()\n            return x\n        def add_block(x):\n            # Adds a Block and Increases the Stochasticity of the model\n            self.increase_dropout()\n            self.increase_stochasticity()\n            return x\n        if self.use_ASPP:\n            self.ASPP = nn.Sequential(*[\n                ASPP(512, 256, 1024, act = self.act)\n            ] + [\n                add_block_stoc(BottleNeckBlock(1024, 256, self.device, attention_type = self.attention_type, stochastic_depth = self.stochastic_depth, act = self.act)) for i in range(3)\n            ])\n            \n        else:\n            self.ASPP = nn.Sequential(*[\n                DownSamplerBottleNeck(512, 256, 1024, 2, self.device, attention_type = self.attention_type, act = self.act),\n            ] + [\n                add_block_stoc(BottleNeckBlock(1024, 256, self.device, attention_type = self.attention_type, stochastic_depth = self.stochastic_depth, act = self.act)) for i in range(3)\n            ])\n        \n        self.layer7 = nn.Sequential(*[\n            DownSamplerBottleNeck(1024, 512, 2048, 2, self.device, attention_type = self.attention_type, act = self.act)\n        ] + [\n            add_block_stoc(BottleNeckBlock(2048, 512, self.device, attention_type= self.attention_type, stochastic_depth = self.stochastic_depth, act = self.act)) for i in range(2)\n        ])\n        \n        self.Dropout6 = nn.Dropout2d(self.drop_prob)\n        self.increase_dropout()\n        self.Attention6 = Attention(1024, 256, self.device, attention_type = self.attention_type, act = self.act)\n        \n        self.Dropout7 = nn.Dropout2d(self.drop_prob)\n        self.increase_dropout()\n        self.Attention7 = Attention(2048, 512, self.device, attention_type = self.attention_type, act = self.act)\n        \n    def forward(self, x):\n        l0, l1, l2, l3, l4, l5 = self.backbone(x) \n        # L5: (B, 512, 8, 8) \n        l6 = self.ASPP(l5)\n        l6 = self.Dropout6(l6)\n        l6 = self.Attention6(l6)\n        \n        l7 = self.layer7(l6)\n        l7 = self.Dropout7(l7)\n        l7 = self.Attention7(l7)\n        return l0, l1, l2, l3, l4, l5, l6, l7","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BASE LINE CODE.","metadata":{}},{"cell_type":"code","source":"class EncoderBaseLine(pl.LightningModule):\n    '''\n    ResNet34 Pretrained Model\n    '''\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'resnet34d'\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        # Extract Layers\n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.pool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4\n        # Freeze Initial Layers\n        #self.freeze([self.conv1, self.bn1, self.layer1])\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x))) # (B, 64, 128, 128)\n        \n        layer1 = self.layer1(self.pool(features0)) # (B, 64, 64, 64)\n        layer2 = self.layer2(layer1) # (B, 128, 32, 32)\n        layer3 = self.layer3(layer2) # (B, 256, 16, 16)\n        layer4 = self.layer4(layer3) # (B, 512, 8, 8)\n        return x, features0, layer1, layer2, layer3, layer4\n\nclass BaseLineUNetBlock(pl.LightningModule):\n    '''\n    UNet Block, upsamples using interpolation(Transposed Convolutions are very unstable and annoying to deal with.)\n    '''\n    def __init__(self, left_features, down_features, out_features, act = 'relu'):\n        super().__init__()\n        self.act = act\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        \n        self.proj = ConvBlock(self.left_features + self.down_features, self.out_features, 3, 1, 1, 1, act = self.act) \n        \n        self.process = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1, act = self.act)\n        \n    def forward(self, left_features, down_features):\n        B, C, H, W = down_features.shape\n        upsampled = F.interpolate(down_features, scale_factor= 2, mode = 'nearest') # Upsample images\n        if left_features != None:\n            upsampled = torch.cat([left_features, upsampled], dim = 1) # Concatenate\n        features = self.proj(upsampled)\n        return self.process(features)\n\nclass DecoderBaseLine(pl.LightningModule):\n    '''\n    Decoder with nothing Fancy. For Testing and Sanity Check\n    '''\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n    \n        self.left_features = [256, 128, 64, 64, 0]\n        self.down_dims = [512, 256, 128, 64, 32, 16]\n        self.dec_blocks = nn.ModuleList([\n            BaseLineUNetBlock(self.left_features[i], self.down_dims[i], self.down_dims[i + 1]) for i in range(len(self.left_features))\n        ])\n        self.proj = nn.Conv2d(16, 1, kernel_size = 3, padding =1)\n    def forward(self, l0, l1, l2, l3, l4, l5):\n        '''\n        Encoder Dims:\n        [B, 3, 256, 256]\n        [B, 64, 128, 128],\n        [B, 64, 64, 64],\n        [B, 128, 32, 32],\n        [B, 256, 16, 16]\n        [B, 512, 8, 8]\n        '''\n        d4 = self.dec_blocks[0](l4, l5)\n        d3 = self.dec_blocks[1](l3, d4)\n        d2 = self.dec_blocks[2](l2, d3)\n        d1 = self.dec_blocks[3](l1, d2)\n        d0 = self.dec_blocks[4](None, d1)\n        return self.proj(d0)\n        \n\nclass BaseLineSolution(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # HYPER PARAMETERS-----------------\n        self.num_classes = 1\n        # END OF HYPER PARAMETERS ---------\n        self.encoder = EncoderBaseLine()\n        self.decoder = DecoderBaseLine(self.num_classes) \n    def forward(self, x):\n        return torch.squeeze(self.decoder(*self.encoder(x)))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DECODER","metadata":{}},{"cell_type":"code","source":"class FPN(pl.LightningModule):\n    '''\n    Feature Pyramid Network, incorporates information at all scales of the network\n    '''\n    def __init__(self, in_features, out_features, out_size, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.num_blocks = len(self.in_features)\n        self.out_features = out_features\n        self.out_size = out_size\n        self.act = act\n        self.attention_type = attention_type\n        \n        self.SABlocks = nn.ModuleList([\n            SqueezeAttend(self.in_features[i], self.out_features // 4, self.out_features, self.out_size, act = self.act, attention_type = self.attention_type) for i in range(self.num_blocks)   \n        ])\n        self.proj = ConvBlock(self.num_blocks * self.out_features, self.out_features, 3, 1, self.out_features, 1)\n        \n    def forward(self, x):\n        assert isinstance(x, list) and len(x) == self.num_blocks\n        # Process Each of the Features\n        features = []\n        for i in range(self.num_blocks):\n            features += [self.SABlocks[i](x[i])]\n        # Concatenate\n        concat = torch.cat(features, dim = 1)\n        return self.proj(concat)\n        \n        \n\nclass DecoderBlockQTPi(pl.LightningModule):\n    '''\n    Uses Pixel Shuffle, Concatenation, and Attention to Upsample Blocks(Mimic ResNet on the Way up)\n    '''\n    def increase_stochasticity(self):\n        self.stochastic_depth += self.increase_stochastic\n    def __init__(self, left_features, down_features, out_features, num_blocks, attention_type, drop_prob, use_pixel_shuffle = True, act = 'relu', stochastic_depth = 0, increase_stochastic = 0):\n        super().__init__()\n        self.left_features = left_features\n        self.out_features = out_features\n        self.stochastic_depth = stochastic_depth\n        self.increase_stochastic = increase_stochastic\n        self.act = act\n        self.num_blocks = num_blocks\n        self.down_features = down_features\n        self.use_pixel_shuffle = use_pixel_shuffle\n        self.attention_type = attention_type\n        self.drop_prob = drop_prob\n        assert self.down_features % 4 == 0\n        \n        if self.use_pixel_shuffle:\n            self.pixel_shuffle = nn.PixelShuffle(2)\n            self.att1 = Attention(self.down_features // 4, self.down_features // 16, self.device, act = self.act, attention_type = self.attention_type)\n            self.concat_dim = self.left_features + self.down_features // 4\n        else:\n            self.concat_dim = self.down_features + self.left_features\n            self.att1= Attention(self.down_features, self.down_features // 4, self.device, act = self.act, attention_type = self.attention_type)\n    \n        self.proj = ConvBlock(self.concat_dim, self.out_features, 3, 1, 1, 1)\n        \n        def add_block(x):\n            self.increase_stochasticity()\n            return x\n        self.blocks = nn.Sequential(*[\n            add_block(BottleNeckBlock(self.out_features, self.out_features // 4, self.device, attention_type = self.attention_type, act= self.act, stochastic_depth = self.stochastic_depth)) for i in range(self.num_blocks)\n        ])\n        self.dropout = nn.Dropout2d(self.drop_prob)\n        self.att2 = Attention(self.out_features, self.out_features // 4, self.device, act = self.act, attention_type = self.attention_type)\n    \n        \n    def forward(self, left_features, down_features):\n        '''\n        x: Tensor(B, C, H, W) \n        '''\n        if self.use_pixel_shuffle:\n            # Pixel Shuffle Upsample Down Features\n            upsampled = self.pixel_shuffle(down_features)\n        else:\n            upsampled = F.interpolate(down_features, scale_factor = 2, mode = 'nearest')\n        upsampled = self.att1(upsampled)\n        if left_features != None:\n            concat = torch.cat([left_features, upsampled], dim = 1)\n        else:\n            concat = upsampled # Final Layer\n        proj = self.proj(concat)\n        blocks = self.blocks(proj)\n        dropped = self.dropout(blocks)\n        return self.att2(dropped)\n        \n        \n\nclass DecoderQTPi(pl.LightningModule):\n    def increase_stochasticity(self):\n        self.stochastic_depth += self.increase_stochastic\n    def increase_dropout(self):\n        self.drop_prob += self.increase_drop\n    def __init__(self, num_classes, attention_type, act, use_pixel_shuffle = True, drop_prob = 0, increase_drop = 0, stochastic_depth = 0, increase_stochastic = 0):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.increase_drop = increase_drop\n        self.use_pixel_shuffle = use_pixel_shuffle\n        self.stochastic_depth = stochastic_depth\n        self.increase_stochastic = increase_stochastic\n        self.attention_type = attention_type\n        self.act = act\n        self.num_classes = num_classes\n    \n        self.left_features = [1024, 512, 256, 128, 64, 64, 0]\n        self.down_dims = [2048, 1024, 512, 256, 128, 64, 64, 16]\n        self.num_blocks = [5, 4, 4, 3, 3, 2, 2] # slightly Mimics ResNet's Block Structure\n        \n        def add_block(i):\n            block = DecoderBlockQTPi(self.left_features[i], self.down_dims[i], self.down_dims[i + 1], self.num_blocks[i], self.attention_type, self.drop_prob, stochastic_depth = self.stochastic_depth, increase_stochastic = self.increase_stochastic, act = self.act, use_pixel_shuffle = self.use_pixel_shuffle)\n            for x in range(self.num_blocks[i]):\n                self.increase_stochasticity()\n            self.increase_dropout()\n            return block\n        self.dec_blocks = nn.ModuleList([\n           add_block(i) for i in range(len(self.left_features))\n        ])\n        \n        # FPN layers\n        self.FPN = FPN(self.down_dims[1:-2], 64, out_size = 128, attention_type = self.attention_type, act = self.act) \n        self.proj_FPN = ConvBlock(128, 64, 3, 1, 64, 1)\n        \n        self.proj = nn.Conv2d(self.down_dims[-1], self.num_classes, kernel_size = 3, padding =1, bias = False)\n    def forward(self, l0, l1, l2, l3, l4, l5, l6, l7):\n        '''\n        Encoder Dims:\n        [B, 3, 256, 256]\n        [B, 64, 128, 128],\n        [B, 64, 64, 64],\n        [B, 128, 32, 32],\n        [B, 256, 16, 16]\n        [B, 512, 8, 8]\n        '''\n        d6 = self.dec_blocks[0](l6, l7)\n        d5 = self.dec_blocks[1](l5, d6)\n        d4 = self.dec_blocks[2](l4, d5)\n        d3 = self.dec_blocks[3](l3, d4)\n        d2 = self.dec_blocks[4](l2, d3)\n        d1 = self.dec_blocks[5](l1, d2)\n        \n        \n        fpn = self.FPN([d6, d5, d4, d3, d2])\n        # Concatenate with the d1\n        concat = torch.cat([fpn, d1], dim = 1) \n        fpn_proj = self.proj_FPN(concat)\n        \n        d0 = self.dec_blocks[6](None, fpn_proj)\n        return self.proj(d0)\n        \n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ENTIRE MODEL","metadata":{}},{"cell_type":"code","source":"class UNetQTPi(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Params\n        self.increase_drop_prob = 0.05\n        self.stochastic_depth = 0.1\n        self.num_classes = 1\n        self.attention_type = 'cbam'\n        self.model_type = 'resnet'\n        self.act = 'relu'\n        self.use_ASPP = True\n        \n        self.use_pixel_shuffle = False\n        self.decoder_stoc = 0\n        self.decoder_increase_stoc = 0.0\n        self.decoder_drop = 0\n        self.decoder_increase_drop = 0.00\n        # END OF HYPER PARAMETERS\n        \n        self.encoder = EncoderQTPi(self.increase_drop_prob, self.stochastic_depth, self.attention_type, use_ASPP = self.use_ASPP, encoder_type = self.model_type, act = self.act)\n        self.decoder = DecoderQTPi(self.num_classes, self.attention_type, self.act, use_pixel_shuffle = self.use_pixel_shuffle, stochastic_depth = self.decoder_stoc, drop_prob = self.decoder_drop, increase_drop = self.decoder_increase_drop, increase_stochastic = self.decoder_increase_stoc)\n        \n    def forward(self, x):\n        return torch.squeeze(self.decoder(*self.encoder(x)))\n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loss Functions(BCELoss, Dice Loss, Lovask Loss)","metadata":{}},{"cell_type":"code","source":"class SymmetricBCE(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.criterion = nn.BCEWithLogitsLoss()\n    def forward(self, y_pred, y_true):\n        '''\n        Symmetric BCE Loss, on both 0s and 1s\n        '''\n        ones_bools = y_true == 1\n        zeros_bools = y_true == 0\n        \n        y_pred_zeros = y_pred[zeros_bools]\n        y_pred_ones = y_pred[ones_bools]\n        \n        loss_ones = self.criterion(y_pred_ones, torch.ones_like(y_pred_ones, device = y_pred_ones.device)) * 0.5\n        loss_zeros = self.criterion(y_pred_zeros, torch.zeros_like(y_pred_zeros, device = y_pred_zeros.device)) * 0.5\n        return loss_ones + loss_zeros\nclass Symmetric_Lovask(nn.Module):\n    '''\n    Symmetric Lovask loss.\n    '''\n    def __init__(self):\n        super().__init__()\n    def forward(self, y_pred, y_true):\n        '''\n        y_pred: Logits\n        y_true: Targets\n        '''\n        return 0.5 * lovask.lovasz_hinge(y_pred, y_true) + 0.5 * lovask.lovasz_hinge(-y_pred, 1 - y_true)\nclass DiceLoss(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n    def forward(self, y_pred, y_true):\n        '''\n        Computes the Binary Cross Entropy Based Dice Loss.\n        y_pred: Binary Predictions, before sigmoid: Shape(B, H, W)  \n        y_true: Ground Truth, Shape (B, H, W)\n        '''\n        B, H, W = y_pred.shape\n        sigmoided = torch.sigmoid(y_pred)\n        smooth = 1e-6\n        \n        numerator = 2 * torch.sum(sigmoided * y_true, [1, 2]) + smooth\n        denominator = torch.sum(y_true, [1, 2]) + torch.sum(sigmoided, [1, 2]) + smooth\n        loss = 1 - numerator / denominator\n        loss = torch.sum(loss)\n        return loss / B\nclass CustomLoss(pl.LightningModule):\n    '''\n    custom Loss function, merging Lovask Loss(0.8) and Cross Entropy(0.2).\n    \n    Lovask Loss is great in terms of optimizing dice, but it needs help from just predicting 0s.\n    '''\n    def __init__(self):\n        super().__init__()\n        self.lovask = Symmetric_Lovask()\n        self.bce = SymmetricBCE()\n        \n        self.lovask_weight = 0.8\n        self.bce_weight = 1 - self.lovask_weight\n    def forward(self, y_pred, y_true):\n        lovask_loss = self.lovask(y_pred, y_true) * self.lovask_weight\n        bce_loss = self.bce(y_pred, y_true) * self.bce_weight\n        return lovask_loss + bce_loss","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Metrics","metadata":{}},{"cell_type":"code","source":"class AccuracyMetric(pl.LightningModule):\n    '''\n    Per Pixel Metric\n    '''\n    def __init__(self):\n        super().__init__()\n    def threshold(self, y_pred):\n        bools = y_pred >= 0.5\n        y_pred[:, :, :] = 0\n        y_pred[bools] = 1\n        return y_pred\n    def pos_and_neg(self, y_pred, y_true):\n        \n        y_pred = torch.sigmoid(y_pred)\n        y_pred = self.threshold(y_pred)\n        pos_bools= y_true== 1\n        neg_bools= y_true == 0\n        \n        pos_entries = y_pred[pos_bools]\n        N = pos_entries.shape[0]\n        pos_acc = torch.sum(pos_entries) / N\n        \n        neg_entries = y_pred[neg_bools]\n        N = neg_entries.shape[0]\n        neg_acc = torch.sum(1 - neg_entries) / N\n        \n        return pos_acc, neg_acc\n    def forward(self, y_pred, y_true):\n        '''\n        Logits\n        '''\n        y_pred = torch.sigmoid(y_pred) \n        y_pred = self.threshold(y_pred)\n        B, H, W = y_pred.shape\n        return torch.sum((y_pred == y_true).int()) / B / H / W\nclass DiceMetric(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n    def threshold(self, y_pred):\n        bools = y_pred >= 0.5\n        y_pred[:, :] = 0\n        y_pred[bools] = 1\n        return y_pred\n    def forward(self, y_pred, y_true):\n        '''\n        Measures Dice Metric over logits \n        '''\n        B, _, _ = y_pred.shape\n        y_pred = torch.sigmoid(y_pred)\n        y_pred = self.threshold(y_pred)\n        smooth = 1e-6\n        numerator = 2 * torch.sum(y_pred * y_true, [1, 2]) + smooth\n        denominator = torch.sum(y_pred, [1, 2]) + torch.sum(y_true, [1, 2]) + smooth\n        coef = numerator / denominator\n        metric = torch.sum(coef) / B\n        return metric","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING MODULES","metadata":{}},{"cell_type":"code","source":"class TrainingConfig:\n    model_name = 'QTPi' # QTPi = Powerful, BaseLine is Simple Transfer Learned\n    optim = 'adam'\n    criterion_type = 'lovask'\n    lr = 1e-3\n    weight_decay = 1e-3\n    \n    num_steps = 5\n    step_size = 0.9\n    eta_min = 1e-7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingSolverQTPi(pl.LightningModule):\n    def __init__(self, dev, fold_idx = 0):\n        super().__init__()\n        self.fold_idx = fold_idx\n        self.dev = dev\n        self.config = TrainingConfig\n        self.criterion_type = self.config.criterion_type\n        self.model_name = self.config.model_name\n        assert self.model_name in ['baseline', 'QTPi']\n        assert self.criterion_type in ['custom', 'dice', 'bce', 'lovask']\n        self.criterion = self.configure_loss()\n        self.model = self.configure_model()\n        self.AccuracyMetric = AccuracyMetric()\n        self.DiceMetric = DiceMetric()\n        self.initialize_states()\n        # Send Model to Device\n        self.to(self.dev)\n    def configure_loss(self):\n        if self.criterion_type == 'custom':\n            criterion = CustomLoss()\n        elif self.criterion_type == 'dice':\n            criterion = DiceLoss()\n        elif self.criterion_type == 'bce':\n            criterion = SymmetricBCE()\n        else:\n            criterion = Symmetric_Lovask()\n        return criterion\n    def initialize_states(self):\n        # Initializes Hidden States\n        self.training_loss = 0\n        self.training_pos_acc = 0\n        self.training_neg_acc = 0\n        self.training_dice = 0\n        self.training_steps = 0\n        \n        self.val_loss = 0\n        self.val_neg_acc = 0\n        self.val_pos_acc = 0\n        self.val_dice = 0\n        self.val_steps = 0\n        \n        self.NUM_EPOCHS = 0\n        \n        self.best_loss = float('inf')\n        self.best_pos_acc = 0\n        self.best_neg_acc = 0\n        self.best_dice = 0\n        \n        self.liveloss = livelossplot.PlotLosses()\n    def round_states(self):\n        # Rounds Loss Stats to 3 Decimals\n        if self.training_steps != 0:\n            self.training_loss /= self.training_steps\n            self.training_pos_acc /= self.training_steps\n            self.training_neg_acc /= self.training_steps\n            self.training_dice /= self.training_steps\n            \n            self.training_loss = round(self.training_loss, 3)\n            self.training_pos_acc = round(self.training_pos_acc, 3)\n            self.training_neg_acc = round(self.training_neg_acc, 3)\n            self.training_dice = round(self.training_dice, 3)\n        if self.val_steps != 0:\n            self.val_loss /= self.val_steps\n            self.val_pos_acc /= self.val_steps\n            self.val_neg_acc /= self.val_steps\n            self.val_dice /= self.val_steps\n            \n            self.val_loss = round(self.val_loss, 3)\n            self.val_neg_acc = round(self.val_neg_acc, 3)\n            self.val_pos_acc = round(self.val_pos_acc, 3)\n            self.val_dice = round(self.val_dice, 3)\n\n    def display_logs(self):\n        logs = {}\n        logs['loss'] = self.training_loss\n        logs['pos_acc'] = self.training_pos_acc\n        logs['neg_acc'] = self.training_neg_acc\n        logs['dice'] = self.training_dice\n        \n        logs['val_loss'] = self.val_loss\n        logs['val_pos_acc'] = self.val_pos_acc\n        logs['val_neg_acc'] = self.val_neg_acc\n        logs['val_dice']= self.val_dice\n        \n        self.liveloss.update(logs)\n        self.liveloss.send()\n    def save_states(self):\n        # Saves Loss and Dice if improved\n        if self.val_loss <= self.best_loss:\n            self.best_loss = self.val_loss\n            torch.save(self.state_dict(), f\"./fold_{self.fold_idx}_loss.pth\")\n        if self.val_dice >= self.best_dice:\n            self.best_dice = self.val_dice\n            torch.save(self.state_dict(), f\"./fold_{self.fold_idx}_dice.pth\")\n        # Optionally Save, Uncomment to do so.\n        if self.val_pos_acc >= self.best_pos_acc:\n            self.best_pos_acc = self.val_pos_acc\n            #torch.save(self.state_dict(), f\"./fold_{self.fold_idx}_pos_acc.pth\")\n        if self.val_neg_acc >= self.best_neg_acc:\n            self.best_neg_acc = self.val_neg_acc\n            #torch.save(self.state_dict(), f\"./fold_{self.fold_idx}_neg_acc.pth\")\n    def reset_states(self):\n        # Resets Epoch by Epoch Stats\n        self.training_loss = 0\n        self.training_pos_acc = 0\n        self.training_neg_acc = 0\n        self.training_dice = 0\n        self.training_steps = 0\n        \n        self.val_loss = 0\n        self.val_neg_acc = 0\n        self.val_pos_acc = 0\n        self.val_dice = 0\n        self.val_steps = 0\n        \n        self.NUM_EPOCHS += 1\n        \n    def configure_model(self):\n        '''\n        Loads in the Model\n        '''\n        if self.model_name == 'baseline':\n            model = BaseLineSolution()\n        else:\n            model = UNetQTPi()\n        return model\n    def configure_optimizers(self):\n        '''\n        Loads in LR Optims and Scheduler.\n        '''\n        if self.config.optim == 'ranger':\n            # Use Ranger(Radam + LookAhead)\n            optimizer = Ranger(self.model.parameters(), lr = self.config.lr, weight_decay = self.config.weight_decay)\n        else:\n            # Just use Adam.\n            optimizer = optim.Adam(self.model.parameters(), lr = self.config.lr, weight_decay = self.config.weight_decay) \n        # Load in Both Schedulers\n        self.lr_decay1 = optim.lr_scheduler.CosineAnnealingLR(optimizer, self.config.num_steps, eta_min = self.config.eta_min) \n        self.lr_decay2 = optim.lr_scheduler.StepLR(optimizer, self.config.num_steps, self.config.step_size) \n        return [optimizer]\n    \n    # Training and Val Logic\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.to(self.dev)\n        y = y.to(self.dev)\n        pred = self.model(x)\n        loss = self.criterion(pred, y)\n        pos_acc, neg_acc = self.AccuracyMetric.pos_and_neg(pred.detach(), y)\n        dice = self.DiceMetric(pred.detach(), y)\n        \n        # Log States\n        self.log('loss', loss)\n        self.log('pos_acc', pos_acc)\n        self.log('neg_acc', neg_acc)\n        self.log(\"dice\", dice)\n        \n        # Print States(Uncomment) \n        print(f\"STEP: {batch_idx}, L: {round(loss.item(), 3)} PA: {round(pos_acc.item(), 3)}, NA: {round(neg_acc.item(), 3)}, D: {round(dice.item(), 3)}\")\n        # Update States\n        self.training_pos_acc += pos_acc.item()\n        self.training_neg_acc += neg_acc.item()\n        self.training_dice += dice.item()\n        self.training_loss += loss.item()\n        self.training_steps += 1\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.to(self.dev) \n        y = y.to(self.dev)\n        with torch.no_grad():\n            pred = self.model(x)\n        \n            \n        loss = self.criterion(pred, y) \n        pos_acc, neg_acc = self.AccuracyMetric.pos_and_neg(pred, y)\n        dice = self.DiceMetric(pred, y)\n        \n        self.log('val_loss', loss)\n        self.log('val_pos_acc', pos_acc)\n        self.log('val_neg_acc', neg_acc)\n        self.log('val_dice', dice)\n        \n        self.val_dice += dice.item()\n        self.val_loss += loss.item()\n        self.val_neg_acc += neg_acc.item()\n        self.val_pos_acc += pos_acc.item()\n        self.val_steps += 1\n    def training_epoch_end(self, _):\n        self.lr_decay1.step()\n        self.lr_decay2.step()\n    def validation_epoch_end(self, _):\n        self.round_states()\n        self.save_states()\n        self.display_logs()\n        self.reset_states()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything()\ndef get_model(fold_idx):\n    model = TrainingSolverQTPi(Config.device, fold_idx = fold_idx)\n    # Construct Trainer\n    callbacks = []\n    #callbacks = [pl.callbacks.EarlyStopping(\n    #    monitor = 'val_dice',\n    #    mode = 'max',\n    #    patience = 100000\n    #)]\n    trainer = pl.Trainer(num_sanity_val_steps = 5, max_epochs = Config.NUM_EPOCHS, checkpoint_callback = False, logger = None, check_val_every_n_epoch = 1, precision = 16, gpus = 1, callbacks = callbacks, deterministic = True, benchmark = False)\n    return model, trainer\ndef overfit_batches(train, num_ex):\n    train_dataset = train.dataset\n    train, _ = torch.utils.data.random_split(train_dataset, [num_ex, len(train_dataset) - num_ex], generator = torch.Generator().manual_seed(42))\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size = 32, worker_init_fn = seed_worker)\n    return train_dataloader\ndef MultiFoldTraining(fold_idx, load_prev = None):\n    train, val = dataModule.get_both(fold_idx)\n    #train = overfit_batches(train, 32)\n    model, trainer = get_model(fold_idx)\n    if load_prev != None:\n        model.load_state_dict(torch.load(f\"{load_prev}fold_{fold_idx}_loss.pth\", map_location = Config.device))\n    trainer.fit(model, train, val) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MultiFoldTraining(0, load_prev = \"../input/fold0trained/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}