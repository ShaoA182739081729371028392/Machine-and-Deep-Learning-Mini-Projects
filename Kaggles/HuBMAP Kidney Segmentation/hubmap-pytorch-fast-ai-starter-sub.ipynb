{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tifffile as tiff\nimport cv2\nimport os\nimport gc\nfrom tqdm.notebook import tqdm\nimport rasterio\nfrom rasterio.windows import Window\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport sys\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master/\")\nimport timm\n\nimport cv2\nimport os\nimport random\nimport math\nimport copy\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sz = 256   #the size of tiles\nreduce = 4 #reduce the original images by 4 times\nDATA = '../input/hubmap-kidney-segmentation/test/'\nfold_0 = '../input/fold0trained/fold_0_dice.pth'\nfold_1 = '../input/fold1trained/fold_1_dice.pth'\nfold_2 = '../input/fold2trained/fold_2_dice.pth'\nfold_3 = '../input/fold3trained/fold_3_dice.pth'\n\nMODELS = [fold_0, fold_1, fold_2, fold_3]\ndf_sample = pd.read_csv('../input/hubmap-kidney-segmentation/sample_submission.csv')\nbs = 16\ntest_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"#functions to convert encoding to mask and mask to encoding\ndef enc2mask(encs, shape):\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for m,enc in enumerate(encs):\n        if isinstance(enc,np.float) and np.isnan(enc): continue\n        s = enc.split()\n        for i in range(len(s)//2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1 + m\n    return img.reshape(shape).T\n\ndef mask2enc(mask, n=1):\n    pixels = mask.T.flatten()\n    encs = []\n    for i in range(1,n+1):\n        p = (pixels == i).astype(np.int8)\n        if p.sum() == 0: encs.append(np.nan)\n        else:\n            p = np.concatenate([[0], p, [0]])\n            runs = np.where(p[1:] != p[:-1])[0] + 1\n            runs[1::2] -= runs[::2]\n            encs.append(' '.join(str(x) for x in runs))\n    return encs\n\n#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n#with transposed mask\ndef rle_encode_less_memory(img):\n    #the image should be transposed\n    pixels = img.T.flatten()\n    \n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    \n    return ' '.join(str(x) for x in runs)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/iafoss/256x256-images\n\ns_th = 40  #saturation blancking threshold\np_th = 1000*(sz//256)**2 #threshold for the minimum number of pixels\nidentity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n\nclass HuBMAPDataset(Dataset):\n    def __init__(self, idx, sz=sz, reduce=reduce):\n        self.data = rasterio.open(os.path.join(DATA,idx+'.tiff'),\n                                 num_threads='all_cpus')\n        # some images have issues with their format \n        # and must be saved correctly before reading with rasterio\n        if self.data.count != 3:\n            subdatasets = self.data.subdatasets\n            self.layers = []\n            if len(subdatasets) > 0:\n                for i, subdataset in enumerate(subdatasets, 0):\n                    self.layers.append(rasterio.open(subdataset))\n        self.shape = self.data.shape\n        self.reduce = reduce\n        self.sz = reduce*sz\n        self.pad0 = (self.sz - self.shape[0]%self.sz)%self.sz\n        self.pad1 = (self.sz - self.shape[1]%self.sz)%self.sz\n        self.n0max = (self.shape[0] + self.pad0)//self.sz\n        self.n1max = (self.shape[1] + self.pad1)//self.sz\n        \n    def __len__(self):\n        return self.n0max*self.n1max\n    \n    def __getitem__(self, idx):\n        # the code below may be a little bit difficult to understand,\n        # but the thing it does is mapping the original image to\n        # tiles created with adding padding, as done in\n        # https://www.kaggle.com/iafoss/256x256-images ,\n        # and then the tiles are loaded with rasterio\n        # n0,n1 - are the x and y index of the tile (idx = n0*self.n1max + n1)\n        n0,n1 = idx//self.n1max, idx%self.n1max\n        # x0,y0 - are the coordinates of the lower left corner of the tile in the image\n        # negative numbers correspond to padding (which must not be loaded)\n        x0,y0 = -self.pad0//2 + n0*self.sz, -self.pad1//2 + n1*self.sz\n        # make sure that the region to read is within the image\n        p00,p01 = max(0,x0), min(x0+self.sz,self.shape[0])\n        p10,p11 = max(0,y0), min(y0+self.sz,self.shape[1])\n        img = np.zeros((self.sz,self.sz,3),np.uint8)\n        # mapping the loade region to the tile\n        if self.data.count == 3:\n            img[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = np.moveaxis(self.data.read([1,2,3],\n                window=Window.from_slices((p00,p01),(p10,p11))), 0, -1)\n        else:\n            for i,layer in enumerate(self.layers):\n                img[(p00-x0):(p01-x0),(p10-y0):(p11-y0),i] =\\\n                  layer.read(1,window=Window.from_slices((p00,p01),(p10,p11)))\n        \n        if self.reduce != 1:\n            img = cv2.resize(img,(self.sz//reduce,self.sz//reduce),\n                             interpolation = cv2.INTER_AREA)\n        \n        #check for empty imges\n        hsv = cv2.cvtColor(copy.deepcopy(img), cv2.COLOR_BGR2HSV)\n        h,s,v = cv2.split(hsv)\n    \n        # Load in Image Normally\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = test_transforms(image = img)['image']\n        if (s>s_th).sum() <= p_th or img.sum() <= p_th:\n            #images with -1 will be skipped\n            del hsv\n            return img, -1\n        return img, idx\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"Conv Blocks","metadata":{}},{"cell_type":"code","source":"class ConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, act = 'relu'):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, act = \"relu\"):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.act1(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x * self.gamma + (1 - self.gamma) * x \nclass CBAMChannel(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev \n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features) \n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        max_pool, _ = torch.max(x, dim = -1) \n        max_pool, _ = torch.max(max_pool, dim = -1)\n        \n        squeeze_mean = self.act1(self.Squeeze(mean))\n        excite_mean = self.Excite(squeeze_mean)\n        \n        squeeze_max = self.act1(self.Squeeze(max_pool))\n        excite_max = self.Excite(squeeze_max)\n        \n        excite = torch.sigmoid((excite_mean + excite_max) / 2).unsqueeze(-1).unsqueeze(-1)\n        return excite * x * self.gamma + (1 - self.gamma) * x\n        \nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.attention_type = attention_type\n        assert self.attention_type in ['se', 'cbam', 'none']\n        if self.attention_type == 'se':\n            self.layer = SqueezeExcite(in_features, inner_features, dev, act = act)\n        elif self.attention_type == 'cbam':\n            self.layer = CBAMChannel(in_features, inner_features, dev, act = act)\n        else:\n            self.layer= nn.Identity()\n    def forward(self, x):\n        return self.layer(x)\n\n# Self Attention Blocks\nclass ConvPlusBatchNorm(pl.LightningModule):\n    '''\n    Conv2d + BN, no activation.\n    '''\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride)\n        self.bn1 = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn1(self.conv(x))\nclass SelfAttention(pl.LightningModule):\n    # Non Local Block.\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n        self.K = ConvPlusBatchNorm(self.in_features, self.inner_features * self.num_heads, 3, 1, 1, 1)\n        self.V = ConvPlusBatchNorm(self.in_features, self.inner_features * self.num_heads, 3, 1, 1, 1)\n        self.Q = ConvPlusBatchNorm(self.in_features, self.inner_features * self.num_heads, 3, 1, 1, 1)\n        self.Linear = ConvPlusBatchNorm(self.inner_features * self.num_heads, self.in_features, 3, 1, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        B, C, H, W = x.shape\n        Keys = self.K(x)\n        Values = self.V(x)\n        Queries = self.Q(x)\n        \n        Keys = Keys.reshape(B, self.num_heads, self.inner_features, H, W)\n        Values = Values.reshape(B, self.num_heads, self.inner_features, H, W)\n        Queries = Queries.reshape(B, self.num_heads, self.inner_features, H, W) \n        \n        Keys = Keys.reshape(B * self.num_heads, self.inner_features, H * W)\n        Values = Values.view(B * self.num_heads, self.inner_features, H * W)\n        Queries = Queries.view(B * self.num_heads, self.inner_features, H * W)\n        \n        att_mat = F.softmax(torch.bmm(Keys.transpose(1, 2), Queries) / math.sqrt(self.inner_features))\n        att_vals = torch.bmm(att_mat, Values.transpose(1, 2))\n        \n        scores = att_vals.view(B, self.num_heads, self.inner_features, H, W)\n        scores = scores.view(B, self.num_heads * self.inner_features, H, W)\n        output = self.Linear(scores) \n        return output * self.gamma + (1 - self.gamma) * x\nclass CBAMSqueezeAttend(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size, squeeze_factor = 4, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.out_size = out_size\n        self.squeeze_factor = squeeze_factor\n        self.act = act \n        \n        self.proj = ConvBlock(self.in_features, self.out_features, 3, 1, 1, 1, act = self.act)\n        self.max_pool = nn.MaxPool2d(kernel_size = 5, padding = 2, stride = self.squeeze_factor)\n        self.avg_pool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = self.squeeze_factor)\n        \n        self.Squeeze = ConvBlock(self.out_features, self.inner_features, 3, 1, 1, 1, act = self.act)\n        self.Excite = ConvPlusBatchNorm(self.inner_features, self.out_features, 3, 1, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x)\n        \n        max_pool = self.max_pool(x)\n        avg_pool = self.avg_pool(x)\n        \n        squeeze_max = self.Squeeze(max_pool)\n        squeeze_avg = self.Squeeze(avg_pool) \n        \n        excite_max = self.Excite(squeeze_max)\n        excite_avg = self.Excite(squeeze_avg)\n        \n        excite = torch.sigmoid((excite_max + excite_avg) / 2)\n        excited = avg_pool * self.gamma * excite + (1 - self.gamma) * avg_pool\n    \n        # Interpolate Upward\n        excited = F.interpolate(excited, size = (self.out_size, self.out_size), mode = 'nearest')\n        return excited\nclass SESqueezeAttend(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size, squeeze_factor = 4, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_size = out_size\n        self.squeeze_factor = squeeze_factor \n        self.act = act\n        self.avg_pool = nn.AvgPool2d(kernel_size = 5, padding = 2, stride = squeeze_factor)\n        \n        self.proj = ConvBlock(self.in_features, self.out_features, 3, 1, 1, 1, act = self.act)\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 1, act = self.act)\n        self.Excite = ConvPlusBatchNorm(self.inner_features, self.in_features, 3, 1, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        B, C, H, W = x.shape\n        x = self.proj(x)\n        pooled = self.avg_pool(x)\n        \n        squeeze = self.Squeeze(pooled)\n        excite = torch.sigmoid(self.Excite(squeeze))\n        \n        excited = self.gamma * pooled * excite + (1 - self.gamma) * pooled \n        \n        # Interpolate Back Up.\n        excited = F.interpolate(excited, size = (self.out_size, self.out_size), mode = 'nearest')\n        return excited\nclass SqueezeAttend(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, out_size, squeeze_factor = 4, act = 'relu', attention_type = 'se'):\n        super().__init__()\n        \n        self.attention_type = attention_type\n        assert self.attention_type in ['se', 'cbam', 'none']\n        if self.attention_type == 'se':\n            self.layer = SESqueezeAttend(in_features, inner_features, out_features, out_size, squeeze_factor = squeeze_factor, act = act)\n        elif self.attention_type =='cbam':\n            self.layer = CBAMSqueezeAttend(in_features, inner_features, out_features, out_size, squeeze_factor = squeeze_factor, act = act)\n        else:\n            self.layer = nn.Identity()\n    def forward(self,x):\n        return self.layer(x)\n\nclass BottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, attention_type = 'se', stochastic_depth = 0, act = 'relu'):\n        super().__init__()\n        self.stochastic_depth = stochastic_depth\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev\n        self.attention_type = attention_type\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1, act = act) \n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1, act = act)\n        self.Expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1, act = act)\n        self.SE = Attention(self.in_features, self.in_features // 4, self.dev, attention_type = self.attention_type)\n\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        if self.training and random.random() < self.stochastic_depth:\n            return x\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        return SE * self.gamma + (1 - self.gamma) * x\n        \nclass InverseBottleNeckBlock(pl.LightningModule):\n    def __init__(self, in_features, inner_features, dev, attention_type = 'se', stochastic_depth = 0, act = 'relu'):\n        super().__init__()\n        self.stochastic_depth = stochastic_depth\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.dev = dev\n        self.attention_type = attention_type\n        \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1, act = act) \n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1, act = act)\n        self.SE = Attention(self.inner_features, self.inner_features//4, self.dev, attention_type = self.attention_type, act = act)\n        self.Squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1, act = act)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        if self.training and random.random() < self.stochastic_depth:\n            return x\n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        se = self.SE(dw)\n        squeeze = self.Squeeze(se)\n        return squeeze * self.gamma + (1 - self.gamma) * x\n\nclass DownSamplerBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride, dev, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.dev = dev\n        self.attention_type = attention_type\n        \n        self.pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride =stride)\n        self.pool_conv = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1, act = act)\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1, act = act) \n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1, act = act)\n        self.Expand = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, self.stride, act = act)\n        self.SE = Attention(self.out_features, self.out_features // 4, self.dev, attention_type = self.attention_type, act = act)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.dev))\n    def forward(self, x):\n        pool = self.pool_conv(self.pool(x))\n        conv_features = self.SE(self.Expand(self.Process(self.Squeeze(x))))\n        return pool * self.gamma + pool * (1 - self.gamma)\nclass DownSamplerInverse(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride, dev, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.dev = dev\n        self.attention_type = attention_type\n    \n        self.pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.pool_conv = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1, act = act)\n        \n        self.squeeze = ConvBlock(self.in_features, self.inner_features,1, 0, 1, 1, act = act)\n        self.process = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1, act = act)\n        self.SE = Attention(self.inner_features, self.inner_features // 4, self.dev, act = act)\n        self.expand = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, self.stride, act = act)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        pool = self.pool_conv(self.pool(x))\n        conv = self.expand(self.SE(self.process(self.squeeze(x))))\n        return conv * self.gamma + (1 - self.gamma) * pool\n\nclass AstrousConvolution(pl.LightningModule):\n    '''\n    Astrous(More Properly - à trous(at holes in french)) Convolution\n    '''\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation, act = 'relu'):\n        super().__init__()\n        self.astrous = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        if act == 'relu':\n            self.act1 = nn.ReLU(inplace = True)\n        else:\n            self.act1 = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act1(self.astrous(x)))\nclass ASPP(pl.LightningModule):\n    '''\n    à trous spatial pooling pyramid block. No further Processing, this should be added later.\n    \n    5 Part:\n    - Normal Conv\n    - à trous: 3 dilation\n    - à trous: 5 dilation\n    - à trous: 7 dilation\n    '''\n    def __init__(self, in_features, inner_features, out_features, act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.conv1 = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 2, act = act)\n        self.conv2 = AstrousConvolution(self.in_features, self.inner_features, 3, 1, 1, 1, 3, act = act)\n        self.conv3 = AstrousConvolution(self.in_features, self.inner_features, 3, 3, 1, 1, 5, act = act)\n        self.conv4 = AstrousConvolution(self.in_features, self.inner_features, 3, 5, 1, 1, 7, act = act) \n        \n        self.proj = ConvBlock(4 * self.inner_features, self.out_features, 1, 0, 1, 1, act = act)\n        \n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(x)\n        conv3 = self.conv3(x)\n        conv4 = self.conv4(x)\n        # concat\n        concat = torch.cat([conv1, conv2, conv3, conv4], dim = 1)\n        proj = self.proj(concat)\n        return proj","metadata":{"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoders","metadata":{}},{"cell_type":"code","source":"# ResNet Based Complex Encoder(+ SE + dropout2d)\nclass ResNetEncoderAlpha(pl.LightningModule):\n    '''\n    ResNet34d encoder + SE and Dropout\n    \n    I would scale the model larger(ex. ResNet50), but larger models have the wrong dimensions.\n    '''\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True \n    def increase_drop(self):\n        self.drop_prob += self.increase_dropout\n    def __init__(self, increase_drop, attention_type):\n        super().__init__()\n        # HYPER PARAMETERS\n        self.increase_dropout = increase_drop\n        self.attention_type = attention_type\n        self.drop_prob = 0\n        self.model_name = 'resnet34d'\n        # END OF HYPER PARAMETERS\n        self.model = timm.create_model(self.model_name, pretrained = False) \n        # Extract Layers\n        self.conv1 = self.model.conv1 # (B, 64, 128, 128)\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.maxpool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1 # (b, 64, 64, 64)\n        self.layer2 = self.model.layer2 # (b, 128, 32, 32)\n        self.layer3 = self.model.layer3 # (b, 256, 16, 16)\n        self.layer4 = self.model.layer4 # (b, 512, 8, 8)\n        \n        self.Dropout0 = nn.Dropout2d(self.drop_prob)\n        self.Attention0 = Attention(64, 16, self.device, attention_type = self.attention_type)\n        self.increase_drop()\n        \n        self.Dropout1 = nn.Dropout2d(self.drop_prob)\n        self.increase_drop()\n        self.Attention1 = Attention(64, 16, self.device, attention_type = self.attention_type)\n        self.Dropout2 = nn.Dropout2d(self.drop_prob)\n        self.Attention2 = Attention(128, 32, self.device, attention_type = self.attention_type)\n        self.increase_drop()\n        self.Dropout3 = nn.Dropout2d(self.drop_prob)\n        self.Attention3 = Attention(256, 64, self.device, attention_type = self.attention_type)\n        self.increase_drop()\n        self.Dropout4 = nn.Dropout2d(self.drop_prob)\n        self.Attention4 = Attention(512, 128, self.device, attention_type = self.attention_type)\n        \n        del self.model\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x)))\n        features0 = self.Dropout0(features0)\n        features0 = self.Attention0(features0)\n        \n        layer1 = self.layer1(self.maxpool(features0))\n        layer1 = self.Dropout1(layer1)\n        layer1 = self.Attention1(layer1)\n        \n        layer2 = self.layer2(layer1)\n        layer2 = self.Dropout2(layer2)\n        layer2 = self.Attention2(layer2)\n        \n        layer3 = self.layer3(layer2)\n        layer3 = self.Dropout3(layer3)\n        layer3 = self.Attention3(layer3)\n        \n        layer4 = self.layer4(layer3)\n        layer4 = self.Dropout4(layer4)\n        layer4 = self.Attention4(layer4)\n        \n        return x, features0, layer1, layer2, layer3, layer4\nclass EffNetEncoderAlpha(pl.LightningModule):\n    '''\n    EfficientNet-b4 based Encoder(SE + Dropout)\n    '''\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True \n    def increase_drop(self):\n        self.drop_prob += self.increase_drop\n    def __init__(self, increase_drop, attention_type, act = 'relu'):\n        super().__init__()\n        self.drop_prob = 0\n        self.act = act \n        self.increase_drop = increase_drop\n    \n        self.attention_type = attention_type\n        \n        self.model_name = 'tf_efficientnet_b4_ns'\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        \n        self.conv1 = self.model.conv_head\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n    \n        self.block0 = self.model.blocks[0] # 24\n        self.block1 = self.model.blocks[1] # 32\n        self.block2 = self.model.blocks[2] # 56\n        self.block3 = self.model.blocks[3] # 112\n        self.block4 = self.model.blocks[4] # 160 \n        self.block5 = self.model.blocks[5] # 272\n        self.block6 = self.model.blocks[6] # 448\n        \n        # Custom Layer\n        self.Dropout0 = nn.Dropout2d(self.drop_prob)\n        self.Attention0 = Attention(24, 6, self.device, attention_type= self.attention_type, act = self.act)\n        self.increase_dropout()\n        \n        self.Dropout1 = nn.Dropout2d(self.drop_prob)\n        self.Attention1 = Attention(32, 8, self.device, attention_type = self.attention_type, act = self.act)\n        self.increase_dropout()\n        \n        self.Dropout2 = nn.Dropout2d(self.drop_prob)\n        self.Attention2 = Attention(56, 16, self.device, attention_type = self.attention_type, act = self.act)\n        self.increase_dropout()\n    \n        self.Dropout3 = nn.Dropout2d(self.drop_prob)\n        self.Attention3 = Attention(160, 48, self.device, attention_type = self.attention_type, act = self.act)\n        self.increase_dropout()\n        \n        self.Dropout4 = nn.Dropout2d(self.drop_prob)\n        self.Attention4 = Attention(448, 128, self.device, attention_type = self.attention_type, act = self.act)\n        \n        # Proj Blocks(To Match ResBlocks)\n        self.proj0 = ConvBlock(24, 64, 3, 1, 1, 1)\n        self.proj1 = ConvBlock(32, 64, 3, 1, 1, 1)\n        self.proj2 = ConvBlock(56, 128, 3, 1, 1, 1)\n        self.proj3 = ConvBlock(160, 256, 3, 1, 1, 1)\n        self.proj4 = ConvBlock(448, 512, 3, 1, 1, 1)\n    def forward(self, x):\n        '''\n        l0: (b, 3, 256, 256)\n        l1: (B, 64, 128, 128)\n        l2: (B, 64, 64, 64)\n        l3: (B, 128, 32, 32)\n        l4: (B, 256, 16, 16)\n        l5: (B, 512, 8, 8)\n        '''\n        features0 = self.bn1(self.act1(self.conv1(x))) # (B, 48, 128, 128)\n        block0 = self.block0(features0) # (B, 24, 128, 128)\n        block0 = self.Dropout0(block0)\n        block0 = self.Attention0(block0)\n        \n        block1 = self.block1(block0) # (B, 32, 64, 64)\n        block1 = self.Dropout1(block1)\n        block1 = self.Attention1(block1)\n        \n        block2 = self.block2(block1) # (B, 56, 32, 32)\n        block2 = self.Dropout2(block2)\n        block2 = self.Attention2(block2)\n        \n        block3 = self.block3(block2) # (B, 112, 16, 16)\n        block4 = self.block4(block3) # (B, 160, 16, 16)\n        block4 = self.Dropout3(block4)\n        block4 = self.Attention3(block4)\n        \n        block5 = self.block5(block4) # (B, 272, 8, 8)\n        block6 = self.block6(block5) # (B, 448, 8, 8)\n        block6 = self.Dropout4(block6)\n        block6 = self.Attention4(block6)\n        \n        # Project Block\n        l1 = self.proj0(block0)\n        l2 = self.proj1(block1)\n        l3 = self.proj2(block2)\n        l4 = self.proj3(block4)\n        l5 = self.proj4(block6)\n        return x, l1, l2, l3, l4, l5   \n        \n\nclass EncoderQTPi(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def increase_dropout(self):\n        self.drop_prob += self.increase_drop\n    def increase_stochasticity(self):\n        '''\n        Increases the Rate of Stochastic Depth Dropout(Deeper should drop more.)\n        '''\n        self.stochastic_depth += self.increase_stoc\n    def __init__(self, increase_drop, increase_stoc, attention_type, use_ASPP = False, encoder_type = 'resnet', act = 'relu'):\n        # Suggested Increase_drop = 0.05, increase_stoc = 0.1\n        super().__init__()\n        self.act = act\n        self.encoder_type = encoder_type\n        assert self.encoder_type in ['resnet', 'effnet']\n        self.increase_drop = increase_drop\n        self.drop_prob = 5 * self.increase_drop\n        \n        self.stochastic_depth = 0\n        self.increase_stoc = increase_stoc\n        \n        self.attention_type = attention_type\n        \n        self.backbone = ResNetEncoderAlpha(self.increase_drop, self.attention_type) if self.encoder_type == 'resnet' else EffNetEncoderAlpha(self.increase_drop, self.attention_type, act = self.act)\n        \n        self.use_ASPP = use_ASPP\n        \n        def add_block_stoc(x):\n            self.increase_stochasticity()\n            return x\n        def add_block(x):\n            # Adds a Block and Increases the Stochasticity of the model\n            self.increase_dropout()\n            self.increase_stochasticity()\n            return x\n        if self.use_ASPP:\n            self.ASPP = nn.Sequential(*[\n                ASPP(512, 256, 1024, act = self.act)\n            ] + [\n                add_block_stoc(BottleNeckBlock(1024, 256, self.device, attention_type = self.attention_type, stochastic_depth = self.stochastic_depth, act = self.act)) for i in range(3)\n            ])\n            \n        else:\n            self.ASPP = nn.Sequential(*[\n                DownSamplerBottleNeck(512, 256, 1024, 2, self.device, attention_type = self.attention_type, act = self.act),\n            ] + [\n                add_block_stoc(BottleNeckBlock(1024, 256, self.device, attention_type = self.attention_type, stochastic_depth = self.stochastic_depth, act = self.act)) for i in range(3)\n            ])\n        \n        self.layer7 = nn.Sequential(*[\n            DownSamplerBottleNeck(1024, 512, 2048, 2, self.device, attention_type = self.attention_type, act = self.act)\n        ] + [\n            add_block_stoc(BottleNeckBlock(2048, 512, self.device, attention_type= self.attention_type, stochastic_depth = self.stochastic_depth, act = self.act)) for i in range(2)\n        ])\n        \n        self.Dropout6 = nn.Dropout2d(self.drop_prob)\n        self.increase_dropout()\n        self.Attention6 = Attention(1024, 256, self.device, attention_type = self.attention_type, act = self.act)\n        \n        self.Dropout7 = nn.Dropout2d(self.drop_prob)\n        self.increase_dropout()\n        self.Attention7 = Attention(2048, 512, self.device, attention_type = self.attention_type, act = self.act)\n        \n    def forward(self, x):\n        l0, l1, l2, l3, l4, l5 = self.backbone(x) \n        # L5: (B, 512, 8, 8) \n        l6 = self.ASPP(l5)\n        l6 = self.Dropout6(l6)\n        l6 = self.Attention6(l6)\n        \n        l7 = self.layer7(l6)\n        l7 = self.Dropout7(l7)\n        l7 = self.Attention7(l7)\n        return l0, l1, l2, l3, l4, l5, l6, l7","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Base Line Model","metadata":{}},{"cell_type":"code","source":"class EncoderBaseLine(pl.LightningModule):\n    '''\n    ResNet34 Pretrained Model\n    '''\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'resnet34d'\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        # Extract Layers\n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        self.act1 = self.model.act1\n        self.pool = self.model.maxpool\n        \n        self.layer1 = self.model.layer1\n        self.layer2 = self.model.layer2\n        self.layer3 = self.model.layer3\n        self.layer4 = self.model.layer4\n        # Freeze Initial Layers\n        #self.freeze([self.conv1, self.bn1, self.layer1])\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x))) # (B, 64, 128, 128)\n        \n        layer1 = self.layer1(self.pool(features0)) # (B, 64, 64, 64)\n        layer2 = self.layer2(layer1) # (B, 128, 32, 32)\n        layer3 = self.layer3(layer2) # (B, 256, 16, 16)\n        layer4 = self.layer4(layer3) # (B, 512, 8, 8)\n        return x, features0, layer1, layer2, layer3, layer4\n\nclass BaseLineUNetBlock(pl.LightningModule):\n    '''\n    UNet Block, upsamples using interpolation(Transposed Convolutions are very unstable and annoying to deal with.)\n    '''\n    def __init__(self, left_features, down_features, out_features, act = 'relu'):\n        super().__init__()\n        self.act = act\n        self.left_features = left_features\n        self.down_features = down_features\n        self.out_features = out_features\n        \n        self.proj = ConvBlock(self.left_features + self.down_features, self.out_features, 3, 1, 1, 1, act = self.act) \n        \n        self.process = ConvBlock(self.out_features, self.out_features, 3, 1, 1, 1, act = self.act)\n        \n    def forward(self, left_features, down_features):\n        B, C, H, W = down_features.shape\n        upsampled = F.interpolate(down_features, scale_factor= 2, mode = 'nearest') # Upsample images\n        if left_features != None:\n            upsampled = torch.cat([left_features, upsampled], dim = 1) # Concatenate\n        features = self.proj(upsampled)\n        return self.process(features)\n\nclass DecoderBaseLine(pl.LightningModule):\n    '''\n    Decoder with nothing Fancy. For Testing and Sanity Check\n    '''\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n    \n        self.left_features = [256, 128, 64, 64, 0]\n        self.down_dims = [512, 256, 128, 64, 32, 16]\n        self.dec_blocks = nn.ModuleList([\n            BaseLineUNetBlock(self.left_features[i], self.down_dims[i], self.down_dims[i + 1]) for i in range(len(self.left_features))\n        ])\n        self.proj = nn.Conv2d(16, 1, kernel_size = 3, padding =1)\n    def forward(self, l0, l1, l2, l3, l4, l5):\n        '''\n        Encoder Dims:\n        [B, 3, 256, 256]\n        [B, 64, 128, 128],\n        [B, 64, 64, 64],\n        [B, 128, 32, 32],\n        [B, 256, 16, 16]\n        [B, 512, 8, 8]\n        '''\n        d4 = self.dec_blocks[0](l4, l5)\n        d3 = self.dec_blocks[1](l3, d4)\n        d2 = self.dec_blocks[2](l2, d3)\n        d1 = self.dec_blocks[3](l1, d2)\n        d0 = self.dec_blocks[4](None, d1)\n        return self.proj(d0)\n        \n\nclass BaseLineSolution(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # HYPER PARAMETERS-----------------\n        self.num_classes = 1\n        # END OF HYPER PARAMETERS ---------\n        self.encoder = EncoderBaseLine()\n        self.decoder = DecoderBaseLine(self.num_classes) \n    def forward(self, x):\n        return torch.squeeze(self.decoder(*self.encoder(x)))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decoder","metadata":{}},{"cell_type":"code","source":"class FPN(pl.LightningModule):\n    '''\n    Feature Pyramid Network, incorporates information at all scales of the network\n    '''\n    def __init__(self, in_features, out_features, out_size, attention_type = 'se', act = 'relu'):\n        super().__init__()\n        self.in_features = in_features\n        self.num_blocks = len(self.in_features)\n        self.out_features = out_features\n        self.out_size = out_size\n        self.act = act\n        self.attention_type = attention_type\n        \n        self.SABlocks = nn.ModuleList([\n            SqueezeAttend(self.in_features[i], self.out_features // 4, self.out_features, self.out_size, act = self.act, attention_type = self.attention_type) for i in range(self.num_blocks)   \n        ])\n        self.proj = ConvBlock(self.num_blocks * self.out_features, self.out_features, 3, 1, self.out_features, 1)\n        \n    def forward(self, x):\n        assert isinstance(x, list) and len(x) == self.num_blocks\n        # Process Each of the Features\n        features = []\n        for i in range(self.num_blocks):\n            features += [self.SABlocks[i](x[i])]\n        # Concatenate\n        concat = torch.cat(features, dim = 1)\n        return self.proj(concat)\n        \n        \n\nclass DecoderBlockQTPi(pl.LightningModule):\n    '''\n    Uses Pixel Shuffle, Concatenation, and Attention to Upsample Blocks(Mimic ResNet on the Way up)\n    '''\n    def increase_stochasticity(self):\n        self.stochastic_depth += self.increase_stochastic\n    def __init__(self, left_features, down_features, out_features, num_blocks, attention_type, drop_prob, use_pixel_shuffle = True, act = 'relu', stochastic_depth = 0, increase_stochastic = 0):\n        super().__init__()\n        self.left_features = left_features\n        self.out_features = out_features\n        self.stochastic_depth = stochastic_depth\n        self.increase_stochastic = increase_stochastic\n        self.act = act\n        self.num_blocks = num_blocks\n        self.down_features = down_features\n        self.use_pixel_shuffle = use_pixel_shuffle\n        self.attention_type = attention_type\n        self.drop_prob = drop_prob\n        assert self.down_features % 4 == 0\n        \n        if self.use_pixel_shuffle:\n            self.pixel_shuffle = nn.PixelShuffle(2)\n            self.att1 = Attention(self.down_features // 4, self.down_features // 16, self.device, act = self.act, attention_type = self.attention_type)\n            self.concat_dim = self.left_features + self.down_features // 4\n        else:\n            self.concat_dim = self.down_features + self.left_features\n            self.att1= Attention(self.down_features, self.down_features // 4, self.device, act = self.act, attention_type = self.attention_type)\n    \n        self.proj = ConvBlock(self.concat_dim, self.out_features, 3, 1, 1, 1)\n        \n        def add_block(x):\n            self.increase_stochasticity()\n            return x\n        self.blocks = nn.Sequential(*[\n            add_block(BottleNeckBlock(self.out_features, self.out_features // 4, self.device, attention_type = self.attention_type, act= self.act, stochastic_depth = self.stochastic_depth)) for i in range(self.num_blocks)\n        ])\n        self.dropout = nn.Dropout2d(self.drop_prob)\n        self.att2 = Attention(self.out_features, self.out_features // 4, self.device, act = self.act, attention_type = self.attention_type)\n    \n        \n    def forward(self, left_features, down_features):\n        '''\n        x: Tensor(B, C, H, W) \n        '''\n        if self.use_pixel_shuffle:\n            # Pixel Shuffle Upsample Down Features\n            upsampled = self.pixel_shuffle(down_features)\n        else:\n            upsampled = F.interpolate(down_features, scale_factor = 2, mode = 'nearest')\n        upsampled = self.att1(upsampled)\n        if left_features != None:\n            concat = torch.cat([left_features, upsampled], dim = 1)\n        else:\n            concat = upsampled # Final Layer\n        proj = self.proj(concat)\n        blocks = self.blocks(proj)\n        dropped = self.dropout(blocks)\n        return self.att2(dropped)\n        \n        \n\nclass DecoderQTPi(pl.LightningModule):\n    def increase_stochasticity(self):\n        self.stochastic_depth += self.increase_stochastic\n    def increase_dropout(self):\n        self.drop_prob += self.increase_drop\n    def __init__(self, num_classes, attention_type, act, use_pixel_shuffle = True, drop_prob = 0, increase_drop = 0, stochastic_depth = 0, increase_stochastic = 0):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.increase_drop = increase_drop\n        self.use_pixel_shuffle = use_pixel_shuffle\n        self.stochastic_depth = stochastic_depth\n        self.increase_stochastic = increase_stochastic\n        self.attention_type = attention_type\n        self.act = act\n        self.num_classes = num_classes\n    \n        self.left_features = [1024, 512, 256, 128, 64, 64, 0]\n        self.down_dims = [2048, 1024, 512, 256, 128, 64, 64, 16]\n        self.num_blocks = [5, 4, 4, 3, 3, 2, 2] # slightly Mimics ResNet's Block Structure\n        \n        def add_block(i):\n            block = DecoderBlockQTPi(self.left_features[i], self.down_dims[i], self.down_dims[i + 1], self.num_blocks[i], self.attention_type, self.drop_prob, stochastic_depth = self.stochastic_depth, increase_stochastic = self.increase_stochastic, act = self.act, use_pixel_shuffle = self.use_pixel_shuffle)\n            for x in range(self.num_blocks[i]):\n                self.increase_stochasticity()\n            self.increase_dropout()\n            return block\n        self.dec_blocks = nn.ModuleList([\n           add_block(i) for i in range(len(self.left_features))\n        ])\n        \n        # FPN layers\n        self.FPN = FPN(self.down_dims[1:-2], 64, out_size = 128, attention_type = self.attention_type, act = self.act) \n        self.proj_FPN = ConvBlock(128, 64, 3, 1, 64, 1)\n        \n        self.proj = nn.Conv2d(self.down_dims[-1], self.num_classes, kernel_size = 3, padding =1, bias = False)\n    def forward(self, l0, l1, l2, l3, l4, l5, l6, l7):\n        '''\n        Encoder Dims:\n        [B, 3, 256, 256]\n        [B, 64, 128, 128],\n        [B, 64, 64, 64],\n        [B, 128, 32, 32],\n        [B, 256, 16, 16]\n        [B, 512, 8, 8]\n        '''\n        d6 = self.dec_blocks[0](l6, l7)\n        d5 = self.dec_blocks[1](l5, d6)\n        d4 = self.dec_blocks[2](l4, d5)\n        d3 = self.dec_blocks[3](l3, d4)\n        d2 = self.dec_blocks[4](l2, d3)\n        d1 = self.dec_blocks[5](l1, d2)\n        \n        \n        fpn = self.FPN([d6, d5, d4, d3, d2])\n        # Concatenate with the d1\n        concat = torch.cat([fpn, d1], dim = 1) \n        fpn_proj = self.proj_FPN(concat)\n        \n        d0 = self.dec_blocks[6](None, fpn_proj)\n        return self.proj(d0)\n        \n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Full Model and Solver","metadata":{}},{"cell_type":"code","source":"class UNetQTPi(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Params\n        self.increase_drop_prob = 0.05\n        self.stochastic_depth = 0.1\n        self.num_classes = 1\n        self.attention_type = 'cbam'\n        self.model_type = 'resnet'\n        self.act = 'relu'\n        self.use_ASPP = True\n        \n        self.use_pixel_shuffle = False\n        self.decoder_stoc = 0\n        self.decoder_increase_stoc = 0.0\n        self.decoder_drop = 0\n        self.decoder_increase_drop = 0.00\n        # END OF HYPER PARAMETERS\n        \n        self.encoder = EncoderQTPi(self.increase_drop_prob, self.stochastic_depth, self.attention_type, use_ASPP = self.use_ASPP, encoder_type = self.model_type, act = self.act)\n        self.decoder = DecoderQTPi(self.num_classes, self.attention_type, self.act, use_pixel_shuffle = self.use_pixel_shuffle, stochastic_depth = self.decoder_stoc, drop_prob = self.decoder_drop, increase_drop = self.decoder_increase_drop, increase_stochastic = self.decoder_increase_stoc)\n        \n    def forward(self, x):\n        return torch.squeeze(self.decoder(*self.encoder(x)))\n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingConfig:\n    model_name = 'QTPi' # QTPi = Powerful, BaseLine is Simple Transfer Learned\n    optim = 'adam'\n    criterion_type = 'lovask'\n    lr = 1e-3\n    weight_decay = 1e-3\n    \n    num_steps = 5\n    step_size = 0.9\n    eta_min = 1e-7","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingSolverQTPi(pl.LightningModule):\n    def __init__(self, dev):\n        super().__init__()\n        self.dev = dev\n        self.config = TrainingConfig\n        self.model_name = self.config.model_name\n        assert self.model_name in ['baseline', 'QTPi']\n        self.model = self.configure_model()\n        # Send Model to Device\n        self.to(self.dev)\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            pred = torch.sigmoid(self.model(x))\n            return pred \n    def configure_model(self):\n        '''\n        Loads in the Model\n        '''\n        if self.model_name == 'baseline':\n            model = BaseLineSolution()\n        else:\n            model = UNetQTPi()\n        return model\n       ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EnsembleModel(pl.LightningModule):\n    def __init__(self, model_paths, dev):\n        super().__init__()\n        self.dev = dev\n        self.out_size = 1024\n        self.model_paths = model_paths\n        self.divide = len(self.model_paths)\n        self.models = [TrainingSolverQTPi(self.dev) for i in range(self.divide)]\n        for model_idx in range(len(self.models)):\n            self.models[model_idx].load_state_dict(torch.load(self.model_paths[model_idx], map_location = self.dev))\n        self.to(self.dev)\n    def tta(self, x):\n        im_flip1 = x.flip(-1)\n        im_flip2 = x.flip(-2)\n        im_flip3 = x.flip(-2).flip(-1)\n        return im_flip1, im_flip2, im_flip3\n    def forward_dl(self, dl):\n        pred_kidney = np.zeros((len(dl.dataset), self.out_size, self.out_size), dtype = np.uint8)\n        count_idx = 0\n        for images, idx in dl:\n            B, _, _, _ = images.shape\n            bool_0 = idx == -1\n            images = images.to(self.dev)\n            with torch.cuda.amp.autocast():\n                predictions = self(images)\n            predictions[bool_0, :, :] = 0 # Remove impossible images\n            pred_kidney[count_idx: count_idx + B, :, :] = predictions.detach().cpu().numpy().astype(np.uint8)\n            del images\n            del predictions\n            del idx\n            gc.collect()\n            count_idx += B\n            print(f\"Predicted: {count_idx}\")\n        return pred_kidney\n            \n            \n            \n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            predictions = None\n            for model in self.models:\n                if predictions is None:\n                    predictions = self.forward_one_model(model, x)\n                else:\n                    predictions = predictions + self.forward_one_model(model, x)\n            predictions = predictions / self.divide\n            \n            # Interpolate upwards\n            predictions = torch.squeeze(F.interpolate(predictions.unsqueeze(1), scale_factor = reduce))\n            # Threshold\n            ones_bools = predictions >= 0.5\n            predictions[:, :, :] = 0\n            predictions[ones_bools] = 1\n            return predictions\n    def forward_one_model(self, model, x):\n        return model(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"model = EnsembleModel(MODELS, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names,preds = [],[]\nfor idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n    idx = row['id']\n    ds = HuBMAPDataset(idx)\n    #rasterio cannot be used with multiple workers\n    dl = DataLoader(ds,bs,num_workers=0,shuffle=False,pin_memory=True)\n    # Ignore Images with Index -1(White or Black only)\n    #generate masks\n    # Predict, then resize upwards using BiLinear Interpolation to 1024x1024.\n    # Mask should be of size(DatasetLength, 1024, 1024)\n    #reshape tiled masks into a single mask and crop padding\n    mask = model.forward_dl(dl)\n    \n    mask = mask.reshape(ds.n0max,ds.n1max,ds.sz,ds.sz).\\\n        transpose(0,2,1,3).reshape(ds.n0max*ds.sz,ds.n1max*ds.sz)\n    mask = mask[ds.pad0//2:-(ds.pad0-ds.pad0//2) if ds.pad0 > 0 else ds.n0max*ds.sz,\n        ds.pad1//2:-(ds.pad1-ds.pad1//2) if ds.pad1 > 0 else ds.n1max*ds.sz]\n    \n    plt.imshow(mask)\n    plt.show()\n    #convert to rle\n    #https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n    rle = rle_encode_less_memory(mask)\n    names.append(idx)\n    preds.append(rle)\n    del mask, ds, dl\n    gc.collect()\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'id':names,'predicted':preds})\ndf.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}