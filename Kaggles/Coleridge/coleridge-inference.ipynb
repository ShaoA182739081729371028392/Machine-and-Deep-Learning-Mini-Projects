{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting","metadata":{}},{"cell_type":"markdown","source":"# 8 Models:\n- 2 Models\n    - Roberta\n    - XLNet\n    - *Electra(Only works for classification)\n        - Electra's VOCAB is weird, it's the same as BERT.\n    - No Bert needed, as RObertA is just better BERT(Same as Distill BERT)\n- 3 Models\n    - QA\n        - + ELECTRA\n    - Ner\n        - + ELECTRA\n    - LM\n        - No Electra\n        \n- For QA and NER, no need for checking. Set their logits to 0.\n       \n  ","metadata":{}},{"cell_type":"markdown","source":"# Best Models:\n- ROBERTA LM:\n    - Roberta-Base, Weird LR Scheduler\n    - 6 Decoder\n    - 94.6 CV\n    - Irreproducable?\n - XLNet LM\n ","metadata":{}},{"cell_type":"markdown","source":"# Install packages","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\n\nfrom IPython.display import clear_output\nclear_output()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-22T11:21:44.35589Z","iopub.execute_input":"2021-06-22T11:21:44.356209Z","iopub.status.idle":"2021-06-22T11:23:07.266874Z","shell.execute_reply.started":"2021-06-22T11:21:44.356177Z","shell.execute_reply":"2021-06-22T11:23:07.266054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport random\nimport glob\nimport importlib\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.autonotebook import tqdm\n\nimport tensorflow as tf\nimport tensorflow.keras as keras \nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as backend\nfrom datasets import load_dataset\nimport transformers\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nAutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n\nfrom typing import List\nimport string\nfrom functools import partial\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nsample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:23:07.269141Z","iopub.execute_input":"2021-06-22T11:23:07.269515Z","iopub.status.idle":"2021-06-22T11:23:07.284109Z","shell.execute_reply.started":"2021-06-22T11:23:07.269474Z","shell.execute_reply":"2021-06-22T11:23:07.283291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain_files_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntrain = pd.read_csv(train_path)\n\nsample_submission = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\ntest_files_path = paper_test_folder\n    \nadnl_govt_labels_path = '../input/bigger-govt-dataset-list/data_set_800.csv'","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:23:07.285435Z","iopub.execute_input":"2021-06-22T11:23:07.286315Z","iopub.status.idle":"2021-06-22T11:23:07.349122Z","shell.execute_reply.started":"2021-06-22T11:23:07.286272Z","shell.execute_reply":"2021-06-22T11:23:07.348367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"papers = {}\nfor paper_id in tqdm(sample_submission['Id']):\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:23:07.351287Z","iopub.execute_input":"2021-06-22T11:23:07.351817Z","iopub.status.idle":"2021-06-22T11:23:07.39422Z","shell.execute_reply.started":"2021-06-22T11:23:07.351778Z","shell.execute_reply":"2021-06-22T11:23:07.393374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n\n\n\"\"\"\nif not BS_CLEANING:\n    def text_cleaning(text):\n        '''\n        Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n        text - Sentence that needs to be cleaned\n        '''\n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n        text = re.sub(' +', ' ', text)\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text)\n        return text\nelse:\n    def text_cleaning(text):\n        '''\n        Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n        text - Sentence that needs to be cleaned\n        '''\n        text = ''.join([k for k in text if k not in string.punctuation])\n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n        # text = re.sub(\"/'+/g\", ' ', text)\n        return text\n\n\"\"\"\ndef read_json_pub(filename, train_data_path=train_files_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:09.770788Z","iopub.execute_input":"2021-06-22T11:30:09.771119Z","iopub.status.idle":"2021-06-22T11:30:09.78351Z","shell.execute_reply.started":"2021-06-22T11:30:09.771087Z","shell.execute_reply":"2021-06-22T11:30:09.782433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Paths and Hyperparameters","metadata":{}},{"cell_type":"code","source":"TOKENIZER_PATH = '../input/coleridge-mlm-model/model_tokenizer/'\nPRETRAINED_PATH = '../input/coleridge-mlm-model/98.8 Model.h5'\n\nMAX_LENGTH = 512\nMAX_LENGTH_LABEL = 60\nOVERLAP = 20","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.046847Z","iopub.execute_input":"2021-06-22T11:30:10.047146Z","iopub.status.idle":"2021-06-22T11:30:10.053111Z","shell.execute_reply.started":"2021-06-22T11:30:10.047115Z","shell.execute_reply":"2021-06-22T11:30:10.052073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\ndef clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n    s = re.sub(' +', ' ', s)\n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    \n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\ndef shorten_sentences_tokens(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    length = MAX_LENGTH - 2\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence\n        if len(words) > length:\n            for p in range(0, len(words), length - OVERLAP):\n                short_sentences.append(words[p:p+length])\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\n\nconnection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset'}\ndef find_mask_candidates(sentence):\n    \"\"\"\n    Extract masking candidates for Masked Dataset Modeling from a given $sentence.\n    A candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence.\n    \"\"\"\n    def candidate_qualified(words):\n        while len(words) and words[0].lower() in connection_tokens:\n            words = words[1:]\n        while len(words) and words[-1].lower() in connection_tokens:\n            words = words[:-1]\n        \n        return len(words) >= 2\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        if word[0].isupper() or word in connection_tokens:\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        else:\n            if phrase_start != -1:\n                if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n                    candidates.append((phrase_start, phrase_end))\n                phrase_start = phrase_end = -1\n    \n    if phrase_start != -1:\n        if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n            candidates.append((phrase_start, phrase_end))\n    \n    return candidates","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.106523Z","iopub.execute_input":"2021-06-22T11:30:10.106783Z","iopub.status.idle":"2021-06-22T11:30:10.122528Z","shell.execute_reply.started":"2021-06-22T11:30:10.106757Z","shell.execute_reply":"2021-06-22T11:30:10.121446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transform","metadata":{}},{"cell_type":"code","source":"class ModelConfig:\n  model_checkpoint = 'roberta-base'\n  tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True, return_token_type_ids = True, return_attention_masks = True)\n  \n  encoder_dim = 1024 if model_checkpoint == 'roberta-large' else 768\n  decoder_dim = 1024 if model_checkpoint == 'roberta-large' else 768\n  num_att_heads = 16 if model_checkpoint == 'roberta-large' else 12\n  decoder_layers = 12 if model_checkpoint == 'roberta-large' else 6\n  intermediate_dim = 4096 if model_checkpoint == 'roberta-large' else 3072\n  dropout_rate = 0.1 \n  # CONFIG:\n  model_head = 'linear'\n  label_smoothing = 0.1\n  # SPECIAL TOKS\n  values = tokenizer.encode(\"<pad>\")\n  PAD_TOKEN = tf.constant(values[1], tf.int64)\n  PAD_TOKEN_INT = PAD_TOKEN.numpy().item()\n  START_TOKEN = tf.constant(values[0], tf.int64)\n  START_TOKEN_INT = START_TOKEN.numpy().item()\n  END_TOKEN = tf.constant(values[2], tf.int64)\n  END_TOKEN_INT = END_TOKEN.numpy().item()\n  SPLIT_TOKEN = tf.constant(tokenizer.encode(\" |\")[1], tf.int64)\n  SPLIT_TOKEN_INT = SPLIT_TOKEN.numpy().item()\n  del values\n  '''\n  ROBERTA-LARGE:\n  - 1024 Decoder Dim = 1024 // num_heads\n  - 4096 Intermediate Dim - Intermediate = FFN dim\n  - 16 Att Heads = Number of Att Heads\n  - 12 Decoder Layers = Number of Decoder Layers\n\n  ROBERTA-BASE:\n  - 3072 Intermediate Dim \n  - 6 Decoder Layers\n  - 12 att Heads\n  - 768 Decoder Dim\n  '''\n  ","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.166887Z","iopub.execute_input":"2021-06-22T11:30:10.167159Z","iopub.status.idle":"2021-06-22T11:30:10.373229Z","shell.execute_reply.started":"2021-06-22T11:30:10.167133Z","shell.execute_reply":"2021-06-22T11:30:10.372276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_test_data = []\n\nfor paper_id in tqdm(sample_submission['Id']):\n    # load paper\n    paper = papers[paper_id]\n\n    # extract sentences\n    sentences = [clean_paper_sentence(sentence) for section in paper \n                     for sentence in section['text'].split('.')\n                    ]\n\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 1] # only accept sentences with length > 1 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n    sentences = [sentence.split() for sentence in sentences] # sentence = list of words\n    new_sentences = []\n    for sent in sentences:\n        new_sentences += sent\n    sentences = new_sentences\n    # mask\n    test_data = []\n    LOOKAHEAD = 100\n    cur_pos = 0\n    for phrase_start, phrase_end in find_mask_candidates(sentences):\n        if phrase_end < cur_pos:\n            continue\n        try:\n            dt_point = sentences[max(cur_pos, phrase_start - LOOKAHEAD): phrase_end + LOOKAHEAD]\n            cur_pos = phrase_end + LOOKAHEAD\n            test_data.append(\" \".join(dt_point))\n        except:\n            pass\n    test_data = '. '.join(test_data)\n    # Tokenize the Data\n    test_data = ModelConfig.tokenizer(test_data)['input_ids'][1:-1]\n    test_data = [sentence for sentence in shorten_sentences_tokens([test_data]) if len(sentence) > 10]\n    # Untokenize the data\n    test_data = ModelConfig.tokenizer.batch_decode(test_data)\n    all_test_data.append(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.374901Z","iopub.execute_input":"2021-06-22T11:30:10.375241Z","iopub.status.idle":"2021-06-22T11:30:10.547295Z","shell.execute_reply.started":"2021-06-22T11:30:10.375203Z","shell.execute_reply":"2021-06-22T11:30:10.546419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grab the Model\n- LM.","metadata":{}},{"cell_type":"code","source":"def get_model(model_name):\n  config = transformers.RobertaConfig.from_json_file(f\"{TOKENIZER_PATH}config.json\")\n  model = transformers.TFAutoModel.from_config(config) # 3 parts to a Roberta model\n  '''\n  1) Embeddings\n  2) RoBERTA Main layer\n  3) Pooler layer. - Can be skipped.\n  '''\n  # Freeze Half of the Layers in the Encoder - It's pretrained and already has decent embeddings.(Transfer Learning.)\n  model.roberta.embeddings.trainable = False \n  for i in range(len(model.roberta.encoder.layer) // 2):\n    model.roberta.encoder.layer[i].trainable = False\n  return model\n    \nclass Encoder(keras.Model):\n  def __init__(self):\n    super().__init__()\n    self.model_checkpoint = ModelConfig.model_checkpoint\n    self.frozen_backbone = get_model(self.model_checkpoint)\n    self.frozen_backbone.roberta.pooler.trainable = False\n    #self.frozen_backbone.config.use_bfloat16 = True\n  def call(self, input_ids, attention_mask, token_type_ids, training):\n    # Just grabs the Embeddings from the Roberta Model\n    embeddings = self.frozen_backbone(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, training = training)\n    return embeddings['last_hidden_state'] # (1, 512, 1024)\n\nclass EncoderMultiHeadAttention(keras.layers.Layer):\n  def __init__(self, encoder_dim, num_att_heads):\n    # x -> MAH + x -> LayerNorm -> Dropout\n    super().__init__()\n    self.encoder_dim = encoder_dim\n    self.num_att_heads = num_att_heads\n    self.drop_prob = ModelConfig.dropout_rate\n\n    self.MultiHeadAttention = keras.layers.MultiHeadAttention(num_heads = self.num_att_heads, \n      key_dim = self.encoder_dim // self.num_att_heads,\n      value_dim = self.encoder_dim // self.num_att_heads,\n      dropout = self.drop_prob\n    )\n    self.LayerNorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    self.Dropout = keras.layers.Dropout(self.drop_prob)\n  def call(self, x, attention_mask, training):\n    MAH = self.MultiHeadAttention(query = x, key = x, value = x, attention_mask = attention_mask, training = training)\n    norm = self.LayerNorm(MAH + x, training = training)\n    return self.Dropout(norm, training = training)\n\nclass DecoderMultiHeadAttention(keras.layers.Layer):\n  def __init__(self, encoder_dim, decoder_dim, num_att_heads, drop_prob):\n    super().__init__()\n    self.encoder_dim = encoder_dim\n    self.decoder_dim = decoder_dim\n    assert self.encoder_dim == self.decoder_dim\n    self.num_att_heads = num_att_heads\n    self.drop_prob = drop_prob \n    \n    \n    self.dec_enc_attention = keras.layers.MultiHeadAttention(num_heads = self.num_att_heads,\n      key_dim = self.encoder_dim // self.num_att_heads, \n      value_dim = self.encoder_dim // self.num_att_heads,\n      dropout = self.drop_prob\n    )\n    self.LayerNorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    self.Dropout = keras.layers.Dropout(self.drop_prob)\n  \n  def call(self, encoder, decoder, padding_mask, training):\n    # Encoder: Tensor(B, L, C)\n    # Decoder: Tensor(B, L, C)\n    MAH = self.dec_enc_attention(query = decoder, key = encoder, value = encoder, attention_mask = padding_mask, training = training)\n    norm = self.LayerNorm(MAH + decoder, training = training)\n    return self.Dropout(norm, training = training)\n\nclass FFN(keras.layers.Layer):\n  def __init__(self, decoder_dim, feedforward_dim, dropout_rate):\n    super().__init__()\n    self.decoder_dim = decoder_dim\n    self.feedforward_dim = feedforward_dim\n    self.drop_prob = dropout_rate\n  \n    self.FFN = keras.Sequential([\n      keras.layers.Dense(self.feedforward_dim, activation = 'relu'),\n      keras.layers.Dense(self.decoder_dim)\n    ])\n    self.LayerNorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    self.Dropout = keras.layers.Dropout(self.drop_prob)\n\n  def call(self, x, training):\n    ffn = self.FFN(x, training = training)\n    norm = self.LayerNorm(ffn + x, training = training)\n    return self.Dropout(norm, training = training)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.591029Z","iopub.execute_input":"2021-06-22T11:30:10.591426Z","iopub.status.idle":"2021-06-22T11:30:10.622093Z","shell.execute_reply.started":"2021-06-22T11:30:10.591374Z","shell.execute_reply":"2021-06-22T11:30:10.619645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderMultiHeadAttention(keras.layers.Layer):\n  def __init__(self, encoder_dim, num_att_heads):\n    # x -> MAH + x -> LayerNorm -> Dropout\n    super().__init__()\n    self.encoder_dim = encoder_dim\n    self.num_att_heads = num_att_heads\n    self.drop_prob = ModelConfig.dropout_rate\n\n    self.MultiHeadAttention = keras.layers.MultiHeadAttention(num_heads = self.num_att_heads, \n      key_dim = self.encoder_dim // self.num_att_heads,\n      value_dim = self.encoder_dim // self.num_att_heads,\n      dropout = self.drop_prob\n    )\n    self.LayerNorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    self.Dropout = keras.layers.Dropout(self.drop_prob)\n  def call(self, x, attention_mask, training):\n    MAH = self.MultiHeadAttention(query = x, key = x, value = x, attention_mask = attention_mask, training = training)\n    norm = self.LayerNorm(MAH + x, training = training)\n    return self.Dropout(norm, training = training)\n\nclass DecoderMultiHeadAttention(keras.layers.Layer):\n  def __init__(self, encoder_dim, decoder_dim, num_att_heads, drop_prob):\n    super().__init__()\n    self.encoder_dim = encoder_dim\n    self.decoder_dim = decoder_dim\n    assert self.encoder_dim == self.decoder_dim\n    self.num_att_heads = num_att_heads\n    self.drop_prob = drop_prob \n    \n    \n    self.dec_enc_attention = keras.layers.MultiHeadAttention(num_heads = self.num_att_heads,\n      key_dim = self.encoder_dim // self.num_att_heads, \n      value_dim = self.encoder_dim // self.num_att_heads,\n      dropout = self.drop_prob\n    )\n    self.LayerNorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    self.Dropout = keras.layers.Dropout(self.drop_prob)\n  \n  def call(self, encoder, decoder, padding_mask, training):\n    # Encoder: Tensor(B, L, C)\n    # Decoder: Tensor(B, L, C)\n    MAH = self.dec_enc_attention(query = decoder, key = encoder, value = encoder, attention_mask = padding_mask, training = training)\n    norm = self.LayerNorm(MAH + decoder, training = training)\n    return self.Dropout(norm, training = training)\n\nclass FFN(keras.layers.Layer):\n  def __init__(self, decoder_dim, feedforward_dim, dropout_rate):\n    super().__init__()\n    self.decoder_dim = decoder_dim\n    self.feedforward_dim = feedforward_dim\n    self.drop_prob = dropout_rate\n  \n    self.FFN = keras.Sequential([\n      keras.layers.Dense(self.feedforward_dim, activation = 'relu'),\n      keras.layers.Dense(self.decoder_dim)\n    ])\n    self.LayerNorm = keras.layers.LayerNormalization(epsilon = 1e-6)\n    self.Dropout = keras.layers.Dropout(self.drop_prob)\n\n  def call(self, x, training):\n    ffn = self.FFN(x, training = training)\n    norm = self.LayerNorm(ffn + x, training = training)\n    return self.Dropout(norm, training = training)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.645833Z","iopub.execute_input":"2021-06-22T11:30:10.646213Z","iopub.status.idle":"2021-06-22T11:30:10.674284Z","shell.execute_reply.started":"2021-06-22T11:30:10.646173Z","shell.execute_reply":"2021-06-22T11:30:10.673131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoder(keras.layers.Layer):\n  def __init__(self, encoder_dim, decoder_dim, feedforward_dim, num_att_heads, dropout_rate):\n    super().__init__()\n    self.encoder_dim = encoder_dim\n    self.decoder_dim = decoder_dim\n    self.feedforward_dim = feedforward_dim\n    self.num_att_heads = num_att_heads\n    self.dropout_rate = dropout_rate\n\n    self.DecoderAttention = EncoderMultiHeadAttention(self.decoder_dim, self.num_att_heads)\n    self.EncoderDecoderAttention = DecoderMultiHeadAttention(self.encoder_dim, self.decoder_dim, self.num_att_heads, self.dropout_rate)\n    self.FFN = FFN(self.decoder_dim, self.feedforward_dim, self.dropout_rate)\n\n  def call(self, encoder, decoder, attention_mask, padding_mask, training):\n    decoder_values = self.DecoderAttention(decoder, attention_mask = attention_mask, training = training)\n    decoder_attended = self.EncoderDecoderAttention(encoder, decoder_values, padding_mask = padding_mask, training = training)\n    ffn = self.FFN(decoder_attended, training = training)\n    return ffn\n\nclass TransformerDecoderModel(keras.Model):\n  def __init__(self):\n    super().__init__()\n    # ----------------------PROCESS---------------------\n    # 1) GET EMBEDDINGS\n    # 2) ADD POSITIONAL EMBEDDINGS\n    # 3) RUN THROUGH THE DECODERS\n    # 4) FINAL FFN\n    # ---------------------PRETRAINING PARTS------------------------------\n    # EMBEDDINGS\n    self.model_checkpoint = ModelConfig.model_checkpoint\n    # Load an Encoder Model\n    tmp_model = get_model(self.model_checkpoint)\n    # Steal embeddings(Pretrained Embeddings)\n    self.embeddings = tmp_model.roberta.embeddings # call(input_ids, token_type_ids)\n    self.vocab_size = self.embeddings.vocab_size\n    del tmp_model \n    # ------------------MODEL DEFINITIONS------------------------\n    self.max_len = MAX_LENGTH_LABEL - 1 \n    self.decoder_dim = ModelConfig.decoder_dim\n    self.encoder_dim = ModelConfig.encoder_dim\n    self.num_att_heads = ModelConfig.num_att_heads\n    self.decoder_layers = ModelConfig.decoder_layers\n    self.dropout_rate = ModelConfig.dropout_rate\n    self.feedforward_dim = ModelConfig.intermediate_dim\n\n    self.decoders = [TransformerDecoder(\n        self.encoder_dim,\n        self.decoder_dim,\n        self.feedforward_dim,\n        self.num_att_heads,\n        self.dropout_rate\n    ) for _ in range(self.decoder_layers)]\n    \n    \n    # PRECOMPUTE CAUSAL Attention MASKS\n    self.attention_mask = self.causal_attention_mask(self.max_len, self.max_len, tf.uint8)\n    # PRECOMPUTE Positional Embeddings\n    self.pos_enc = self.positional_embeddings(self.max_len, self.decoder_dim) # (1, L, C) \n    self.pos_enc = tf.expand_dims(self.pos_enc, axis = 0)\n  def positional_embeddings(self, max_length, dim):\n    L, C = (max_length, dim) \n    positional_encodings = np.zeros((L, C), np.float32)\n    for pos in range(L):\n      for i in range(0, C  + 2, 2):\n        if i >= C:\n          continue\n        positional_encodings[pos, i] = math.sin(pos / 10000 ** (i / self.decoder_dim))\n        if i + 1 >= C:\n          continue \n        positional_encodings[pos, i + 1] = math.cos(pos / 10000 ** ((i + 1) / self.decoder_dim))\n    return tf.identity(positional_encodings) \n\n\n  def causal_attention_mask(self, n_dest, n_src, dtype):\n    \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(1, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult) # Diagonal Mask \n\n  def compute_padding_mask(self, decoder_ids, dtype):\n    # Returns an Attention Mask to mask all padding tokens\n    PAD_TOKEN_ID = ModelConfig.tokenizer.pad_token_id\n    # Create a Mask over token ids.\n    mask = tf.not_equal(decoder_ids, PAD_TOKEN_ID)\n    return tf.cast(mask, dtype)\n  def call_val(self, encoder, decoder, training):\n    B, L, C = encoder.shape \n    _, Dec_Len = decoder.shape \n\n    padding_mask = self.compute_padding_mask(decoder, tf.uint8)\n    padding_mask = tf.expand_dims(padding_mask, axis = -1) # (B, Dec_Len, 1) \n\n    attention_mask = self.causal_attention_mask(Dec_Len, Dec_Len, tf.uint8) # (B, Dec_Len, Dec_Len)\n    attention_mask = attention_mask * padding_mask # (B, Dec_Len, Dec_Len) \n\n    padding_mask = tf.repeat(padding_mask, L, axis = -1) # (B, Dec_Len, L)\n\n    decoder_embeddings = self.embeddings(decoder, training = training) # (B, L, C)\n    # Pos Enc\n    pos_enc = tf.identity(self.pos_enc)[:, :Dec_Len, :] # (B, L, C)\n    decoder_embeddings = decoder_embeddings + pos_enc\n\n    for DECODER in self.decoders:\n      decoder_embeddings = DECODER(encoder, decoder_embeddings, attention_mask, padding_mask, training = training)\n    return decoder_embeddings \n  def call(self, encoder, decoder, training):\n    '''\n    Encoder: Encoder Embeddings: Tensor(B, L, C)\n    Decoder: Decoder Input Ids: Tensor(B, L')\n    training: in training mode?\n    Unfortuately, you cannot precompute attention masks, since the padding mask depends on the decoder_ids\n    '''\n    # NO NEED For TOKEN TYPE IDs, as they are always 0(Always 1 Sentence)\n    B, L, C = encoder.shape\n    _, Dec_Len = decoder.shape\n\n    # GENERATE MASKS\n    padding_mask = self.compute_padding_mask(decoder, tf.uint8) # (B, L)\n    padding_mask = tf.expand_dims(padding_mask, axis = -1)\n    attention_mask = tf.identity(self.attention_mask) # (B, L, L) - Used only in Decoder Attention\n    attention_mask = attention_mask * padding_mask # (B, L, L)\n    \n    padding_mask = tf.repeat(padding_mask, L, axis = -1)\n    \n    \n    # Convert Tokens to Embeddings\n    decoder_embeddings = self.embeddings(decoder, training = training) # (B, L', C)\n    # ----------------GET POS ENC FOR DECODER INPUTS(Encoder already got them) -------------------\n    pos_enc = tf.identity(self.pos_enc) # (1, L, C)\n\n    decoder_embeddings = decoder_embeddings + pos_enc\n    # Run through the Decoders:\n    for DECODER in self.decoders:\n      decoder_embeddings = DECODER(encoder, decoder_embeddings, attention_mask, padding_mask, training = training)\n    # FINAL HEAD\n    return decoder_embeddings","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.680233Z","iopub.execute_input":"2021-06-22T11:30:10.68061Z","iopub.status.idle":"2021-06-22T11:30:10.731379Z","shell.execute_reply.started":"2021-06-22T11:30:10.680574Z","shell.execute_reply":"2021-06-22T11:30:10.730276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DenseHead(keras.Model):\n  def __init__(self, vocab_size, pad_token):\n    super().__init__()\n    self.vocab_size = vocab_size\n    self.pad_token = pad_token\n \n    self.head = keras.layers.Dense(self.vocab_size)\n    \n    \n    self.label_smoothing = ModelConfig.label_smoothing\n    \n\n  def call(self, x, training):\n    # Does the logic of Argmax and prediction in one swoop\n    pred = self.head(x, training = training) # (B, C)\n    return pred\n  def call_val(self, x, training):\n    pred = self.head(x[:, -1], training = training)\n    return pred\n\ndef replace_index(cur_tokens, index, new_val):\n    start_tokens = cur_tokens[:, :index] # (B, L)\n    new_value = tf.expand_dims(new_val, axis = 1) # (B, 1)\n    end_tokens = cur_tokens[:, index + 1:] # (B, L) \n    \n    new_tokens = tf.concat([start_tokens, new_value, end_tokens], axis = 1) # (B, L)\n    return new_tokens\n\nclass FullModel(keras.Model):\n  def __init__(self):\n    super().__init__()\n    self.encoder = Encoder()\n    self.START_TOKEN = ModelConfig.START_TOKEN\n    self.END_TOKEN = ModelConfig.END_TOKEN # Default Special tokens for HuggingFace Tokenizers\n    self.MAX_LEN = MAX_LENGTH_LABEL\n    self.vocab_length = ModelConfig.tokenizer.vocab_size \n    self.decoder = TransformerDecoderModel()\n    self.model_head = DenseHead(self.decoder.vocab_size, ModelConfig.tokenizer.pad_token_id)\n  \n  def call(self, inputs, training):\n    input_ids = inputs[:, :, 0]\n    attention_mask = inputs[:, :, 1]\n    token_type_ids = inputs[:, :, 2]\n    \n    encoding_embeddings = self.encoder(input_ids, attention_mask, token_type_ids, training = training)\n    B = encoding_embeddings.shape[0]\n    # Predict again and again, up to MAX_LEN. \n    predicted_tokens = tf.ones((B, self.MAX_LEN), dtype = tf.int64) * ModelConfig.START_TOKEN\n    for i in range(1, self.MAX_LEN):\n        cur_tokens = predicted_tokens[:, :i] # (B, :i)\n        decoded_embeddings = self.decoder.call_val(encoding_embeddings, cur_tokens, training = training) \n        pred_tokens = self.model_head.call_val(decoded_embeddings, training = training) # (B, )\n        pred_tokens = tf.argmax(pred_tokens, axis = 1)\n    \n        predicted_tokens = replace_index(predicted_tokens, i, pred_tokens) # (B, L)\n    return predicted_tokens\n        \n  def call_train(self, input_ids, attention_mask, token_type_ids, decoder_input_ids, training):\n    # decoder_input_ids: Tensor(B, L)\n    encoded_embeddings = self.encoder(input_ids, attention_mask, token_type_ids, training = training) # (B, L, C)\n    decoded_values = self.decoder(encoded_embeddings, decoder_input_ids, training = training)\n    preds = self.model_head.call(decoded_values, training = training)\n    return preds # (B, L, C)\n  def call_val(self, input_ids, attention_mask, token_type_ids, training):\n    # Inference Loop: \n    encoded_embeddings = self.encoder(input_ids, attention_mask, token_type_ids, training = training)\n    # create Starter token\n    B, _, _ = encoded_embeddings.shape\n    sentence_tokens = tf.ones((B, 1), tf.int64) * tf.cast(self.START_TOKEN, tf.int64)\n    pred_logits = tf.ones((B, 0, self.vocab_length), encoded_embeddings.dtype)\n    for i in range(self.MAX_LEN - 1):\n      embeddings = self.decoder.call_val(encoded_embeddings, sentence_tokens, training = training) # (B, L, C)\n      pred = self.model_head.call_val(embeddings, training = training) # (B, C)\n      # Add the logits \n      TMP_LOGITS = tf.expand_dims(pred, axis = 1) # (B, 1, C)\n      \n      pred_logits = tf.concat([pred_logits, TMP_LOGITS], axis = 1) # (B, 1, C)\n\n      pred = keras.activations.softmax(pred) # (B, C)\n      pred = tf.argmax(pred, axis = -1) # (B, )\n      pred = tf.expand_dims(pred, axis = 1)\n      # Just append the values, should predict <END> and then just random garbage(We filter it out)\n      sentence_tokens = tf.concat([sentence_tokens, tf.cast(pred, sentence_tokens.dtype)], axis = 1)\n    return pred_logits, sentence_tokens","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.74254Z","iopub.execute_input":"2021-06-22T11:30:10.742897Z","iopub.status.idle":"2021-06-22T11:30:10.776167Z","shell.execute_reply.started":"2021-06-22T11:30:10.74286Z","shell.execute_reply":"2021-06-22T11:30:10.775035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def LOAD_MODEL():\n    model = FullModel()\n    input_ids = tf.ones((1, 512), dtype = tf.int64) \n    attention_mask = tf.ones((1, 512), dtype = tf.int64)\n    token_type_id = tf.zeros((1, 512), dtype = tf.int64)\n    \n    input_embeddings = tf.stack([input_ids, attention_mask, token_type_id], axis = -1)\n    \n    model(input_embeddings, training = False)\n    model.load_weights('../input/coleridge-mlm-model/acc_model.h5')\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.781329Z","iopub.execute_input":"2021-06-22T11:30:10.783344Z","iopub.status.idle":"2021-06-22T11:30:10.791202Z","shell.execute_reply.started":"2021-06-22T11:30:10.783301Z","shell.execute_reply":"2021-06-22T11:30:10.790313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LOAD_MODEL()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:10.796145Z","iopub.execute_input":"2021-06-22T11:30:10.798987Z","iopub.status.idle":"2021-06-22T11:30:25.634082Z","shell.execute_reply.started":"2021-06-22T11:30:10.798944Z","shell.execute_reply":"2021-06-22T11:30:25.633241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict","metadata":{}},{"cell_type":"code","source":"def tokenize_data(data):\n    tokenized = ModelConfig.tokenizer(data, padding = 'max_length', truncation= True, max_length = MAX_LENGTH, return_attention_mask = True, return_token_type_ids = True)\n    input_ids = np.array(tokenized['input_ids'])\n    attention_mask = np.array(tokenized['attention_mask'])\n    token_type_ids = np.array(tokenized['token_type_ids'])\n    \n\n    inputs = tf.stack([input_ids, attention_mask, token_type_ids], axis = -1)\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:25.637847Z","iopub.execute_input":"2021-06-22T11:30:25.638127Z","iopub.status.idle":"2021-06-22T11:30:25.643719Z","shell.execute_reply.started":"2021-06-22T11:30:25.638099Z","shell.execute_reply":"2021-06-22T11:30:25.642518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_values(labels):\n    # labels: Tensor(N, 60)\n    end_token = ModelConfig.tokenizer.eos_token_id\n    first_end_token = tf.argmax(tf.cast(tf.equal(labels, end_token), tf.int64), axis = -1)\n    N, _ = labels.shape\n    predicted_values = []\n    for n in range(N):\n        value = labels[n][:first_end_token[n] + 1][1:-1]\n        value = ModelConfig.tokenizer.decode(value)\n        all_datasets = value.split(\" | \")\n        all_datasets = [clean_text(ex) for ex in all_datasets]\n        if all_datasets != ['']:\n            predicted_values += all_datasets\n    \n    return '|'.join(list(set(predicted_values)))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:25.645695Z","iopub.execute_input":"2021-06-22T11:30:25.646186Z","iopub.status.idle":"2021-06-22T11:30:25.656917Z","shell.execute_reply.started":"2021-06-22T11:30:25.646145Z","shell.execute_reply":"2021-06-22T11:30:25.656001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Batched submission: Single Pred is too slow.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PREDICTION_LABELS = []\nNUM_BATCHES = 20\ncur_idx = 0\ncur_COUNT = 0\n\nwhile True:\n    min_idx = cur_idx\n    max_idx = min_idx + NUM_BATCHES \n    cur_idx = max_idx \n    \n    selected_test_data = all_test_data[min_idx: max_idx]\n    TST_DATA = []\n    indices = []\n    padded = []\n    CUR_IDX = 0 \n    all_zero = True\n    \n    for test_data in selected_test_data:\n        try:\n            if test_data == []:\n                padded += [False]\n                indices += [[]]\n            else:\n                input_values = tokenize_data(test_data)\n                \n                \n                padded += [True]\n                NEXT_IDX = len(input_values) + CUR_IDX \n                indices += [(CUR_IDX, NEXT_IDX)]\n                TST_DATA += [input_values]\n                CUR_IDX = NEXT_IDX\n                all_zero = False\n        except:\n            padded += [False]\n            indices += [[]]\n  \n    if all_zero or len(TST_DATA) == 0:\n        print(\"ALLL ZEROOOS\")\n        for _ in padded:\n            PREDICTION_LABELS += ['']\n    else:\n        # TST_DATA = [] raises an error, caught in the prev if.\n        valid = True\n        try:\n            TST_DATA = tf.concat(TST_DATA, axis = 0)\n            with tf.device(\"GPU:0\"):\n                prediction = model(TST_DATA, training = False)\n        except:\n            valid = False\n            for _ in padded:\n                PREDICTION_LABELS += ['']\n        if valid:\n            for IDX in range(len(padded)):\n                try:\n                    if not padded[IDX]:\n                        PREDICTION_LABELS += ['']\n                        continue\n                    idx1, idx2 = indices[IDX]\n                    pred = prediction[idx1: idx2] # (B, L)\n                    decoded_predictions = decode_values(pred) \n                    PREDICTION_LABELS += [decoded_predictions]\n\n                except:\n                    print(\"ERRROOROOOROOROOR\")\n                    PREDICTION_LABELS += ['']\n    \n    if cur_idx >= cur_COUNT:\n        cur_COUNT += 200\n        print(len(PREDICTION_LABELS))\n    if cur_idx >= len(all_test_data):\n        # Done.\n        break\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:25.660189Z","iopub.execute_input":"2021-06-22T11:30:25.66057Z","iopub.status.idle":"2021-06-22T11:30:30.122148Z","shell.execute_reply.started":"2021-06-22T11:30:25.660512Z","shell.execute_reply":"2021-06-22T11:30:30.121136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference isn't working\n- Fist part: getting tokens shoulf be fine\n- Assugming toen types is the issue.","metadata":{}},{"cell_type":"code","source":"sample_submission['PredictionString'] = PREDICTION_LABELS\nsample_submission.to_csv(\"./submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:30.123264Z","iopub.execute_input":"2021-06-22T11:30:30.123606Z","iopub.status.idle":"2021-06-22T11:30:30.131087Z","shell.execute_reply.started":"2021-06-22T11:30:30.123573Z","shell.execute_reply":"2021-06-22T11:30:30.130267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.PredictionString.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:44.63088Z","iopub.execute_input":"2021-06-22T11:30:44.631208Z","iopub.status.idle":"2021-06-22T11:30:44.636825Z","shell.execute_reply.started":"2021-06-22T11:30:44.631175Z","shell.execute_reply":"2021-06-22T11:30:44.635696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}