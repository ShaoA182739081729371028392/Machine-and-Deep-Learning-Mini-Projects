{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n                 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n                 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n                 'Swan Ganz Catheter Present']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.031204,"end_time":"2020-12-23T19:06:15.235441","exception":false,"start_time":"2020-12-23T19:06:15.204237","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Library"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-23T19:06:15.331972Z","iopub.status.busy":"2020-12-23T19:06:15.33012Z","iopub.status.idle":"2020-12-23T19:06:17.540544Z","shell.execute_reply":"2020-12-23T19:06:17.53983Z"},"papermill":{"duration":2.270679,"end_time":"2020-12-23T19:06:17.540705","exception":false,"start_time":"2020-12-23T19:06:15.270026","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"%%capture\n# ====================================================\n# Library\n# ====================================================\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\nimport os\nimport copy\nimport math\nimport ast\nimport time\nimport random\nimport shutil\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import check_random_state\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision\n\nimport albumentations\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose, HueSaturationValue, CoarseDropout\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\n!pip install livelossplot\nimport livelossplot\nimport matplotlib.pyplot as plt\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model(Modified ResNet200D with Attention)\nThe need for attention is so we can do layer-wise MSELoss on Attention Maps"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BatchNormBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, stride, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, stride= stride, groups = groups)\n        self.bn = nn.BatchNorm2d(out_features)\n    def forward(self, x):\n        return self.bn(F.relu(self.conv(x), inplace = True)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Squeeze and Excitation Block using CBAM(MaxPool + AvgPool)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialSqueezeExcite(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size = 7, padding = 3)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        B, C, H, W  = x.shape\n        avg_pool = torch.mean(x, dim = 1)\n        avg_pool = avg_pool.view(B, 1, H, W)\n        \n        max_pool, _ = torch.max(x, dim = 1)\n        max_pool = max_pool.view(B, 1, H, W)\n        \n        # Concat\n        concatenated = torch.cat([max_pool, avg_pool], dim = 1) # (B, 2, H, W)\n        return torch.sigmoid(self.conv(concatenated)) * x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ChannelSqueezeExcite(nn.Module):\n    def __init__(self, in_dim, squeeze_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.squeeze_dim = squeeze_dim\n        self.Squeeze = nn.Linear(self.in_dim, self.squeeze_dim)\n        self.Excite = nn.Linear(self.squeeze_dim, self.in_dim)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W) \n        '''\n        avg_pool = torch.mean(x, dim = -1)\n        avg_pool = torch.mean(avg_pool, dim = -1)\n        squeeze_avg = F.relu(self.Squeeze(avg_pool))\n        \n        max_pool, _ = torch.max(x, dim = -1)\n        max_pool, _ = torch.max(max_pool, dim = -1) # (B, C)\n        squeeze_max = F.relu(self.Squeeze(max_pool))\n        expanded = torch.sigmoid(self.Excite(squeeze_max) + self.Excite(squeeze_avg)).unsqueeze(-1).unsqueeze(-1)\n        return expanded * x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CBAMSqueezeExcite(nn.Module):\n    def __init__(self, in_dim, inner_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        self.Channel = ChannelSqueezeExcite(self.in_dim, self.inner_dim)\n        self.Spatial = SpatialSqueezeExcite()\n        self.bn = nn.BatchNorm2d(self.in_dim)\n        self.gamma = nn.Parameter(torch.zeros(1, device = device))\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W)\n        '''\n        values = self.Spatial(self.Channel(x))\n        return self.bn(values) * self.gamma + x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CBAM Attention(spatial Attention + Feature Attention)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    def __init__(self, in_dim, inner_dim, num_heads):\n        super().__init__()\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        self.num_heads = num_heads\n        self.K = nn.Conv2d(self.in_dim, self.inner_dim * self.num_heads, kernel_size = 1)\n        self.V = nn.Conv2d(self.in_dim, self.inner_dim * self.num_heads, kernel_size = 1)\n        self.Q = nn.Conv2d(self.in_dim, self.inner_dim * self.num_heads, kernel_size = 1)\n        self.Linear = nn.Conv2d(self.inner_dim * self.num_heads, self.in_dim, kernel_size = 1)\n    def forward(self, x):\n        B, C, H, W = x.shape\n        K = self.K(x)\n        V = self.V(x)\n        Q = self.Q(x) # (B, IH, H, W)\n        # Reshape Tensors\n        K = K.view(B, self.num_heads, self.inner_dim, H * W)\n        V = V.view(B, self.num_heads, self.inner_dim, H * W)\n        Q = Q.view(B, self.num_heads, self.inner_dim, H * W)\n        # Reshape Again\n        K = K.view(B * self.num_heads, self.inner_dim, H * W)\n        V = V.view(B * self.num_heads, self.inner_dim, H * W)\n        Q = Q.view(B * self.num_heads, self.inner_dim, H * W) # (BH, I, HW)\n        \n        att_mat = F.softmax(torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(self.inner_dim), dim = -1) # (BH, HW, HW)\n        attention_scores = torch.bmm(att_mat, V) # (BH, I, HW)\n        \n        # Reshape\n        attention_scores = attention_scores.view(B, self.num_heads, self.inner_dim, H, W)\n        attention_scores = attention_scores.view(B, self.num_heads * self.inner_dim, H, W)\n        return self.Linear(attention_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ChannelAttention(nn.Module):\n    def __init__(self, in_dim, inner_dim, num_heads):\n        super().__init__()\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        self.num_heads = num_heads\n        self.K = nn.Conv2d(self.in_dim, self.inner_dim * self.num_heads, kernel_size = 1)\n        self.V = nn.Conv2d(self.in_dim, self.inner_dim * self.num_heads, kernel_size = 1)\n        self.Q = nn.Conv2d(self.in_dim, self.inner_dim * self.num_heads, kernel_size = 1)\n        self.Linear = nn.Conv2d(self.inner_dim * self.num_heads, self.in_dim, kernel_size = 1)\n    def forward(self, x):\n        B, C, H, W = x.shape\n        K = self.K(x)\n        V = self.V(x)\n        Q = self.Q(x) # (B, IH, H, W)\n        # Reshape Tensor\n        K = K.view(B, self.num_heads, self.inner_dim, H, W)\n        V = V.view(B, self.num_heads, self.inner_dim, H, W)\n        Q = Q.view(B, self.num_heads, self.inner_dim, H, W)\n        # Reshape Tensors\n        K = K.view(B * self.num_heads, self.inner_dim, H * W)\n        V = V.view(B * self.num_heads, self.inner_dim, H * W)\n        Q = Q.view(B * self.num_heads, self.inner_dim, H * W) # (BH, I, HW) \n        \n        att_mat = F.softmax(torch.bmm(Q.transpose(1, 2), K) / math.sqrt(self.inner_dim), dim = -1)\n        attended = torch.bmm(att_mat, V.transpose(1, 2)) # (BH, I, HW)\n        # Reshape Tensor\n        attended = attended.view(B, self.num_heads, self.inner_dim, H, W)\n        attended = attended.view(B, self.num_heads * self.inner_dim, H, W)\n        return self.Linear(attended) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CBAMAttention(nn.Module):\n    def __init__(self, in_dim, inner_dim, num_heads):\n        super().__init__()\n        # Note: removed channel attention to reduce memory overhead\n        self.SpatialAtt = SpatialAttention(in_dim, inner_dim, num_heads)\n        self.bn = nn.BatchNorm2d(in_dim)\n        self.gamma = nn.Parameter(torch.zeros(1, device = device))\n    def forward(self, x):\n        return self.bn(self.SpatialAtt(x)) * self.gamma + x # Scale down the attention in case the NN doesn't need it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InvertedBottleNeck(nn.Module):\n    '''\n    Inverted BottleNeck Block, as proposed in MobileNetv3\n    '''\n    def __init__(self, in_dim, inner_dim):\n        # Standard Convolutional BottleNeck Block\n        super().__init__()\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        self.expand = BatchNormBlock(in_dim, inner_dim, 1, 0, 1, 1)\n        self.depthwise = BatchNormBlock(inner_dim, inner_dim, 3, 1, 1, inner_dim)\n        \n        self.se = CBAMSqueezeExcite(inner_dim, inner_dim // 8)\n        \n        self.squeeze = BatchNormBlock(inner_dim, in_dim, 1, 0, 1, 1)\n        self.bn = nn.BatchNorm2d(in_dim)\n        self.gamma = nn.Parameter(torch.zeros(1, device = device))\n    def forward(self, x):\n        expanded = self.expand(x)\n        depthwise = self.depthwise(expanded)\n        se = self.se(depthwise)\n        squeeze = self.squeeze(se)\n        return self.bn(squeeze) * self.gamma + x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BottleNeck(nn.Module):\n    def __init__(self, in_dim, inner_dim):\n        '''\n        in_features, out_features, kernel_size, padding, stride, groups\n        '''\n        super().__init__()\n        self.in_dim = in_dim\n        self.inner_dim = inner_dim\n        self.squeeze = BatchNormBlock(self.in_dim, self.inner_dim, 1, 0, 1, 1)\n        self.process = BatchNormBlock(self.inner_dim, self.inner_dim, 3, 1, 1, 1)\n        self.expand = BatchNormBlock(self.inner_dim, self.in_dim, 1, 0, 1, 1)\n        self.bn = nn.BatchNorm2d(self.in_dim)\n        self.gamma = nn.Parameter(torch.zeros(1, device = device))\n        #self.gamma.requires_grad = False\n    def forward(self, x):\n        return self.bn(self.expand(self.process(self.squeeze(x)))) * self.gamma + x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedResNet3(nn.Module):\n    def __init__(self, num_classes, drop_prob = 0.2):\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = timm.create_model(\"resnet200d_320\", pretrained = True, features_only = True)\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n            \n        self.layer2_att = CBAMAttention(512, 256, 2)\n        self.se2 = CBAMSqueezeExcite(512, 256) \n        self.layer3_att = CBAMAttention(1024,  512, 2)\n        self.se3 = CBAMSqueezeExcite(1024, 512) \n        # Custom Layer 3.5(Between layer 3 and 4)\n        \n        self.layer3_5 = nn.Sequential(*[\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            nn.AvgPool2d(kernel_size = 3, padding = 1, stride = 2),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512),\n            BottleNeck(1024, 512)\n        ])\n\n        self.layer4_att = CBAMAttention(1024, 512, 2)\n        self.se4 = CBAMSqueezeExcite(1024, 512) \n        \n        self.Dropout = nn.Dropout(drop_prob)\n        self.global_avg = nn.AvgPool2d(kernel_size = 5)\n        self.Linear = nn.Linear(2048, self.num_classes)\n    def forward(self, x):\n        B, _, _, _ = x.shape\n        base_features = self.model(x)[-3]\n        attention_2 = self.layer2_att(base_features) # (B, 512, 80, 80)\n        excited_features2 = self.se2(attention_2) # (B, 512, 80, 80)\n        # Layer 3\n        layer3 = self.model.layer3(excited_features2) # (B, 1024, 40, 40)\n        attention_3 = self.layer3_att(layer3)\n        excited_features3 = self.se3(attention_3)\n        # Layer 3.5\n        layer3_5 = self.layer3_5(excited_features3)\n        attention_4 = self.layer4_att(layer3_5)\n        excited_features4 = self.se4(attention_4)\n        # Layer 4\n        layer4 = self.model.layer4(excited_features4) # (B, 2048, 5, 5)\n        # AVGPOOL\n        global_avg = self.global_avg(layer4).view(B, -1)\n        dropped_avg = self.Dropout(global_avg)\n        logits = self.Linear(dropped_avg)\n        \n        return logits, excited_features2, excited_features3, excited_features4, layer4, global_avg # Return Layer4 and Logits for MSELoss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_bn_student(module):\n    '''\n    Recursively put desired batch norm in nn.module module\n    set module = net to start code.\n    This method used to replace parameters for the student model\n    '''\n    # go through all attributes of module nn.module (e.g. network or layer) and put batch norms if present\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.GroupNorm:\n            new_bn = nn.BatchNorm2d(target_attr.num_channels, affine = True, track_running_stats = True)\n            setattr(module, attr_str, new_bn)\n        elif type(target_attr) == torch.nn.Sequential:\n            for i in range(len(target_attr)):\n                if type(target_attr[i]) == torch.nn.GroupNorm:\n                   new_bn = nn.BatchNorm2d(target_attr[i].num_channels, affine = True, track_running_stats = True)\n                   target_attr[i] = new_bn\n                else:\n                    replace_bn_student(target_attr[i])\n\n    # iterate through immediate child modules. Note, the recursion is done by our code no need to use named_modules()\n    for name, immediate_child_module in module.named_children():\n        replace_bn_student(immediate_child_module)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Teacher Model Stage 1 Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TeacherModel(nn.Module):\n    def __init__(self, num_classes, device):\n        super().__init__()\n        self.device = device\n        self.num_classes = num_classes\n        self.model = ModifiedResNet3(11, drop_prob = 0.2)\n        for parameter in self.model.parameters():\n            parameter.requires_grad = True\n        self.optim = optim.Adam(self.model.parameters(), lr = 1e-4, weight_decay = 1e-4)\n        self.lr_decay = optim.lr_scheduler.CosineAnnealingLR(self.optim, 4, eta_min = 1e-7)\n        self.lr_decay2 = optim.lr_scheduler.ExponentialLR(self.optim, 0.95)\n        self.criterion = nn.BCEWithLogitsLoss()\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            return self.model(x)\n    def roc_auc(self, y_pred, y_true):\n        '''\n        Computes ROC_AUC\n        y_pred: tensor(B, 11)\n        y_true: Tensor(B, 11)\n        ROC AUC doesn't work for this dataset unless batch size large enough so oof.\n        '''\n        acc = 0\n        B, C = y_pred.shape\n        ones = y_pred >= 0.5\n        y_pred[:, :] = 0\n        y_pred[ones] = 1\n        for i in range(B):\n            acc += torch.sum((y_pred[i, :] == y_true[i, :]).int()) / C\n        return acc / B\n        \n    def training_loop(self, trainloader, valloader, NUM_EPOCHS, display_every = 64):\n        liveloss = livelossplot.PlotLosses()\n        best_val_acc = 0.0\n        best_val_loss = 9999\n        torch.cuda.empty_cache()\n        for EPOCH in range(NUM_EPOCHS):\n            self.train()\n            total_loss = 0.0\n            count = 0\n            logs = {}\n            '''\n            for _, annotated_images, labels in trainloader:\n                self.optim.zero_grad()\n                annotated_images = annotated_images.to(self.device).to(torch.float32)\n                labels = labels.to(self.device).to(torch.float32)\n                pred, _, _, _, _, _ = self.model(annotated_images)\n                loss = self.criterion(pred,labels) \n                total_loss += loss.item()\n                loss.backward()\n                self.optim.step()\n                del annotated_images\n                del labels\n                del pred\n                del loss\n                count += 1\n                torch.cuda.empty_cache()\n                if count == display_every:\n                    break\n            logs['loss'] = total_loss / count \n            print(f\"EPOCH: {EPOCH}, total_loss: {logs['loss']}\")\n            '''\n            self.eval()\n            with torch.no_grad():\n                logs['accuracy'] = 0\n                logs['val_loss'] = 0\n                count = 0 \n                for _, val_annotated_images, val_labels in valloader:\n                    val_annotated_images = val_annotated_images.to(self.device).to(torch.float32)\n                    val_labels = val_labels.to(self.device).to(torch.float32)\n                    pred, _, _, _, _, _ = self.model(val_annotated_images)\n                    logs['accuracy'] += self.roc_auc(torch.sigmoid(pred), val_labels)\n                    logs['val_loss'] += self.criterion(pred, val_labels).item()\n                    del val_annotated_images\n                    del val_labels\n                    del pred\n                    count += 1\n                    torch.cuda.empty_cache()\n                logs['accuracy'] /=count\n                logs['val_loss'] /=count\n            self.lr_decay.step() # Cosine Annealing\n            self.lr_decay2.step() # Slowly lower the LR\n            print(f\"Accuracy: {logs['accuracy']}, loss: {logs['val_loss']}\")\n            liveloss.update(logs)\n            liveloss.send()\n            if logs['val_loss'] < best_val_loss:\n                best_val_loss = logs['val_loss']\n                torch.save(self.state_dict(), 'teacher.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\nCOLOR_MAP = {'ETT - Abnormal': (255, 0, 0),\n             'ETT - Borderline': (0, 255, 0),\n             'ETT - Normal': (0, 0, 255),\n             'NGT - Abnormal': (255, 255, 0),\n             'NGT - Borderline': (255, 0, 255),\n             'NGT - Incompletely Imaged': (0, 255, 255),\n             'NGT - Normal': (128, 0, 0),\n             'CVC - Abnormal': (0, 128, 0),\n             'CVC - Borderline': (0, 0, 128),\n             'CVC - Normal': (128, 128, 0),\n             'Swan Ganz Catheter Present': (128, 0, 128),\n            }\n\n\nclass TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, df, df_annotations, annot_size=50, transform=None):\n        self.df = df\n        self.df_annotations = df_annotations\n        self.annot_size = annot_size\n        self.file_names = df.index.values\n        self.labels = df.iloc[:, :-1].values\n        self.transform = transform\n        self.train_path = \"../input/ranzcr-clip-catheter-line-classification/train/\"\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{self.train_path}/{file_name}.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        base_image = copy.deepcopy(image)\n        query_string = f\"StudyInstanceUID == '{file_name}'\"\n        df = self.df_annotations.query(query_string)\n        # Add in the Annotations into the image\n        for i, row in df.iterrows():\n            label = row[\"label\"]\n            data = np.array(ast.literal_eval(row[\"data\"]))\n            for d in data:\n                image[d[1]-self.annot_size//2:d[1]+self.annot_size//2,\n                      d[0]-self.annot_size//2:d[0]+self.annot_size//2,\n                      :] = COLOR_MAP[label]\n        if self.transform:\n            augmentations = self.transform(image = base_image, image1 = image)\n            orig_augmented = augmentations['image']\n            image = augmentations['image1']\n        label = torch.tensor(self.labels[idx]).float()\n        return orig_augmented, image, label","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-23T19:06:19.163947Z","iopub.status.busy":"2020-12-23T19:06:19.161721Z","iopub.status.idle":"2020-12-23T19:06:19.167297Z","shell.execute_reply":"2020-12-23T19:06:19.168067Z"},"papermill":{"duration":0.048275,"end_time":"2020-12-23T19:06:19.168252","exception":false,"start_time":"2020-12-23T19:06:19.119977","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# ====================================================\n# Transforms\n# ====================================================\nIMAGE_SIZE = 320\ntrain_transforms = Compose([\n            RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, scale = (0.9, 0.9)),\n            HorizontalFlip(p=0.5),\n            #albumentations.VerticalFlip(p = 0.2),\n            RandomBrightnessContrast(p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n            HueSaturationValue(p=0.2, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n            ShiftScaleRotate(p=0.2, shift_limit=0.0025, scale_limit=0.01, rotate_limit=10),\n            CoarseDropout(p=0.2),\n            albumentations.GaussianBlur(),\n            albumentations.GaussNoise(),\n            Cutout(p=0.2, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n            Normalize(),\n            ToTensorV2(),\n        ], additional_targets = {'image1': 'image'})\ntest_transforms = Compose([\n            Resize(IMAGE_SIZE, IMAGE_SIZE),\n            Normalize(),\n            ToTensorV2(),\n        ], additional_targets = {'image1': 'image'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv')\ntrain_annotations = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train_annotations.csv', index_col = \"StudyInstanceUID\")\ntrain_idx = train.isin(train_annotations.index)['StudyInstanceUID']\ntrain_with_annotations = train[train_idx]\ntrain_with_annotations = train_with_annotations.set_index(\"StudyInstanceUID\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nteacher_model = TeacherModel(11, device)\nteacher_model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#teacher_model.load_state_dict(torch.load(\"../input/teachermodelbn/teacher.pth\", map_location = device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#teacher_model.training_loop(train_dataloader, val_dataloader, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#torch.save(teacher_model.state_dict(), \"./TeacherModel.pth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Student Model Stage 2 Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomConvBlock(nn.Module):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size, padding = padding, groups = groups)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = nn.SiLU(inplace = True)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvSqueezeExcite(nn.Module):\n    '''\n    Standard Convolutional Based Squeeze Excitation Block \n    '''\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features= in_features\n        self.inner_features = inner_features\n        self.Squeeze = nn.Conv2d(self.in_features, self.inner_features,kernel_size = 1)\n        self.act = nn.SiLU(inplace = True)\n        self.Excite = nn.Conv2d(self.inner_features, self.in_features, kernel_size = 1)\n    def forward(self, x):\n        squeezed = self.act(self.Squeeze(x))\n        excited = torch.sigmoid(self.Excite(squeezed))\n        return excited * x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InverseBottleNeck(nn.Module):\n    def __init__(self, in_features, inner_features, device):\n        super().__init__()\n        self.device = device\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.Expand = CustomConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n        self.Depthwise = CustomConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n        self.SE = ConvSqueezeExcite(self.inner_features, self.in_features // 4)\n        self.Squeeze = CustomConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        expand = self.Expand(x)\n        depthwise = self.Depthwise(expand)\n        SE = self.SE(depthwise)\n        Squeeze = self.Squeeze(SE)\n        return Squeeze * self.gamma + x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedResNetBeta(nn.Module):\n    def __init__(self, num_classes, drop_prob = 0.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = timm.create_model(\"resnet200d_320\", pretrained = True)\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.Dropout = nn.Dropout(p = drop_prob)\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Dense = nn.Linear(2048, self.num_classes)\n    def forward(self, x):\n        '''\n        x: Image input.\n        '''\n        features = torch.squeeze(self.global_avg(self.model.forward_features(x)))\n        dropped = self.Dropout(features)\n        return self.Dense(dropped), 0, 0, 0, 0, 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModifiedResNetAlpha(nn.Module):\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def __init__(self, num_classes, device, drop_prob = 0.2):\n        # modified ResNet Student, with additional processing and one less attention head for maximal performance\n        super().__init__()\n        self.device = device\n        self.num_classes = num_classes\n        self.model = timm.create_model(\"resnet200d_320\", pretrained = True, features_only = True)\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        \n        self.conv1 = self.model.conv1\n        self.bn1 = self.model.bn1\n        \n        self.layer1 = self.model.layer1\n        \n        self.layer2 = self.model.layer2\n        \n        self.layer3 = self.model.layer3\n        \n        # Freeze Initial Layers\n        #self.freeze(self.layer1)\n        #self.freeze(self.conv1)\n        #self.freeze(self.bn1)\n        #self.freeze(self.layer2)\n        #self.freeze(self.layer3) \n        \n        self.layer4 = self.model.layer4 # Extract layers from ResNet\n        \n        self.layer2_5 = nn.Sequential(*[\n            BottleNeck(512, 128) for i in range(12) # 18 BottleNeck Blocks, Really Really Cheap Operation so use wisely.\n        ])\n        \n        self.layer2_att = nn.Identity()#CBAMAttention(512, 256, 2)\n        self.se2 = nn.Identity()#CBAMSqueezeExcite(512, 256)\n        \n        self.layer3_att = nn.Identity()#CBAMAttention(1024, 512, 2)\n        self.se3 = nn.Identity()#CBAMSqueezeExcite(1024, 256) \n        # Custom Layer 3.5(Between layer 3 and 4)\n        \n        self.layer3_5 = nn.Sequential(*[BottleNeck(1024, 256) for i in range(15)]) #+\n            #[nn.MaxPool2d(kernel_size = 3, padding = 1, stride = 2)] + \n            #[BottleNeck(1024, 256) for i in range(12)])\n\n        \n        self.layer4_att = nn.Identity()#CBAMAttention(1024, 512, 2)\n        self.se4 = nn.Identity()#CBAMSqueezeExcite(1024, 256) \n        \n        self.Dropout = nn.Dropout(drop_prob)\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Linear = nn.Linear(2048, self.num_classes)\n       \n        \n    def forward(self, x):\n        B, _, _, _ = x.shape\n        base_features = self.model(x)[-3] # (B, 512, 80, 80)\n        # Process through Layer 2.5\n        layer2_5 = self.layer2_5(base_features) # (B, 512, 80, 80)\n        \n        attention2 = self.layer2_att(layer2_5)\n        excited_features2 = self.se2(attention2)\n    \n        layer3 = self.layer3(excited_features2) # (B, 1024, 40, 40)\n        \n        attention3 = self.layer3_att(layer3)\n        excited_features3 = self.se3(attention3)\n        \n        layer3_5 = self.layer3_5(excited_features3)\n        attention4 = self.layer4_att(layer3_5)\n        excited_features4 = self.se4(attention4) # (B, 1024, 10, 10)\n        \n        layer4 = self.layer4(excited_features4) # (B, 2048, 5, 5)\n        # Pooled Features\n        avg_pooled = torch.squeeze(self.global_avg(layer4)) # (B, 2048)\n        dropped_features = self.Dropout(avg_pooled) # (B, 2048)\n        \n        logits = self.Linear(dropped_features) # (B, 11)\n        \n        return logits, excited_features2, excited_features3, excited_features4, layer4, avg_pooled ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StudentTeacher(nn.Module):\n    def __init__(self, num_classes, teacher, device):\n        super().__init__()\n        self.num_classes = num_classes\n        self.device = device\n        self.teacher = teacher\n        for parameter in self.teacher.parameters():\n            parameter.requires_grad = False\n        self.student = ModifiedResNetAlpha(self.num_classes, self.device, drop_prob = 0.2)\n        self.optim = optim.Adam(self.student.parameters(), lr = 1e-4, weight_decay = 1e-3)\n        self.lr_decay = optim.lr_scheduler.StepLR(self.optim, 5, 0.9)\n        self.lr_decay2 = optim.lr_scheduler.CosineAnnealingLR(self.optim, 5, eta_min = 1e-7)\n        self.MSELoss = nn.MSELoss()\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.BCELoss = nn.BCELoss()\n        self.copy_weight = 2 # Weight on how much student should copy teacher \n        self.logits_weight = 0.5 # Weight on how much student should get answer right\n        self.sigmoid_temp = 1.5\n    def forward(self, x):\n        '''\n        Runs Inference on the student model\n        '''\n        self.eval()\n        with torch.no_grad():\n            pred, _, _, _, _, _= self.student(x)\n            return pred\n    def sigmoid_temperature(self, teacher_logits, students_logits):\n        '''\n        Performs BCE Loss on the teacher_logits with sigmoid temperature.\n        \n        May be a bit unstable of a loss function. \n        '''\n        teacher_sigmoid = torch.sigmoid(teacher_logits / self.sigmoid_temp)\n        student_sigmoid = torch.sigmoid(students_logits / self.sigmoid_temp)\n        return self.BCELoss(student_sigmoid, teacher_sigmoid) * self.copy_weight\n    def compare_loss(self, teacher, student):\n        '''\n        Comparison loss if you need to compare features between the student and teacher.\n        '''\n        return self.MSELoss(student.view(-1), teacher.view(-1)) * self.copy_weight\n    def evaluate(self, pred, y_pred, val = False):\n        if val:\n            return self.criterion(pred, y_pred)\n        return self.criterion(pred, y_pred) * self.logits_weight\n    def accuracy(self, student_logits, y_true):\n        ones = student_logits >= 0.5\n        student_logits[:] = 0\n        student_logits[ones] = 1\n        B, C = student_logits.shape\n        return torch.sum((student_logits == y_true).int()) / B / C\n    def training_loop(self, train_dataloader, val_dataloader, NUM_EPOCHS, display_every = 64):\n        liveloss = livelossplot.PlotLosses()\n        best_val_loss = 999\n        best_val_acc = 0\n        torch.cuda.empty_cache() # Clear any wasted cuda memory\n        self.teacher.eval()\n        for EPOCH in range(NUM_EPOCHS):\n            logs = {}\n            self.student.train()\n            total_loss = 0\n            count = 0\n            logs['val_loss'] = 0\n            for images, annotated_images, labels in train_dataloader:\n                self.optim.zero_grad()\n                images = images.to(self.device)\n                annotated_images = annotated_images.to(self.device)\n                labels = labels.to(self.device)\n                \n                with torch.no_grad():\n                    teacher_logits, _, _, _, _, _ = self.teacher(annotated_images)\n                student_logits, _, _, _, _, _ = self.student(images)\n                \n                copy_loss = self.sigmoid_temperature(teacher_logits, student_logits)\n                logits_loss = self.evaluate(student_logits, labels) \n                \n                logs['val_loss'] += self.evaluate(student_logits, labels, val = True).item()\n                loss = logits_loss + copy_loss\n                loss.backward()\n                self.optim.step()\n                total_loss += loss.item()\n                count += 1\n                del student_logits\n                del teacher_logits\n                del loss\n                del labels\n                del annotated_images\n                del images\n                torch.cuda.empty_cache()\n                if count == display_every:\n                    break\n            logs['loss'] = total_loss / count\n            logs['val_loss'] /= count\n            del total_loss\n            del count\n            self.lr_decay.step()\n            \n            print(f\"EPOCH: {EPOCH}, loss: {logs['loss']}\")\n            self.student.eval()\n            with torch.no_grad():\n                count = 0\n            \n                logs['accuracy'] = 0\n                logs['val_accuracy'] = 0\n                for images, annotated_images, labels in val_dataloader:\n                    images = images.to(self.device)\n                    annotated_images = annotated_images.to(self.device)\n                    labels = labels.to(self.device)\n                    student_logits, _, _, _, _, _ = self.student(images)\n                \n                    logs['accuracy'] += self.accuracy(torch.sigmoid(student_logits), labels)\n                    logs['val_accuracy'] += self.evaluate(student_logits, labels, val = True).item()\n                    del student_logits\n                    del labels\n                    del annotated_images\n                    del images\n                    torch.cuda.empty_cache()\n                    count += 1\n                logs['accuracy'] /= count\n                logs['val_accuracy'] /= count\n            print(f\"Val_LOSS: {logs['val_loss']}, accuracy: {logs['accuracy']}\")\n    \n            liveloss.update(logs)\n            liveloss.send()\n            if logs['val_accuracy'] <= best_val_loss:\n                best_val_loss = logs['val_accuracy']\n                torch.save(self.state_dict(), './BestVal.pth')\n            if logs['accuracy'] >= best_val_acc:\n                best_val_acc = logs['accuracy']\n                torch.save(self.state_dict(), \"./BestAcc.pth\")\n            print(f\"Val_Loss: {logs['val_accuracy']}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain_split, val_split = train_test_split(train_with_annotations, train_size = 0.9999, test_size = 0.0001)\n## Filter out the Overlapping entries in Train and Val_set\ntrain_unique = set(train_split.PatientID.values)\nval_unique = set(val_split.PatientID.values)\noverlap = train_unique.intersection(val_unique)\n#\ntrain_idx = []\nfor overlapped in overlap:\n    train_idx += train_split.index[train_split.PatientID == overlapped].to_list()\nval_split = val_split.append(train_split.loc[train_idx])\ntrain_split = train_split.drop(train_idx)\n\ntrain_dataset = TrainDataset(train_split, train_annotations, transform = train_transforms) \nval_dataset = TrainDataset(val_split, train_annotations, transform = test_transforms)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True) # 14 is the maximum for ResNetStudent, 24 batch size for ResNetBeta.\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 32)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nstudentTeacher = StudentTeacher(11, teacher_model, device)\nstudentTeacher.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#studentTeacher.load_state_dict(torch.load(\"../input/simplemodel/BestVal.pth\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#studentTeacher.training_loop(train_dataloader, val_dataloader, 50, display_every = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#torch.save(studentTeacher.state_dict(), \"./LastEpoch.pth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Student Model Stage 3 Train"},{"metadata":{},"cell_type":"markdown","source":"Load in the Full Training Set, as annotations no longer needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in the full dataset\ntrain = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv', index_col = \"StudyInstanceUID\")\ntrain, val = train_test_split(train, train_size = 0.9999, test_size = 0.0001)\n# Find Leaking Patient Data\nunique_train = set(train.PatientID.values)\nunique_val = set(val.PatientID.values)\nsame_values = unique_train.intersection(unique_val)\noverlapping_train_vals = []\nfor overlap in same_values:\n    overlapped = train.index[(train.PatientID == overlap)].to_list()\n    overlapping_train_vals += overlapped\nval = val.append(train.loc[overlapping_train_vals])\ntrain = train.drop(overlapping_train_vals)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDatasetNoAnnot(torch.utils.data.Dataset):\n    def __init__(self, x, base_file, transforms):\n        self.x = x\n        self.files = x.index\n        self.transforms = transforms \n        self.base_file = base_file\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, idx):\n        file_val = str(self.files[idx])\n        file_path = self.base_file + file_val + \".jpg\"\n        GT = torch.tensor(self.x.loc[file_val])\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        transformed_image = torch.tensor(self.transforms(image = image)['image'])\n        return transformed_image, GT\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, base_file, transforms):\n        self.base_file = base_file\n        self.test_images = os.listdir(self.base_file)\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.test_images)\n    def __getitem__(self, idx):\n        file_val = self.test_images[idx].strip('.jpg')\n        file_path = self.base_file + file_val + '.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        transformed_image = torch.tensor(self.transforms(image = image)['image'])\n        return transformed_image, file_val\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntraining_dataset = TrainDatasetNoAnnot(train.iloc[:, :-1], \"../input/ranzcr-clip-catheter-line-classification/train/\", train_transforms)\ntrain_dataloader = torch.utils.data.DataLoader(training_dataset, shuffle = True, batch_size = 16)\n\nval_dataset = TrainDatasetNoAnnot(val.iloc[:, :-1], \"../input/ranzcr-clip-catheter-line-classification/train/\", test_transforms)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle = False, batch_size = 32)\n\ntest_dataset = TestDataset(\"../input/ranzcr-clip-catheter-line-classification/test/\", test_transforms)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StudentSolver(nn.Module):\n    def __init__(self, student, device):\n        super().__init__()\n        self.device = device\n        self.student = student\n        for parameter in self.student.parameters():\n            parameter.requires_grad = True # Unfreeze all Weights\n        self.optim = optim.Adam(self.student.parameters(), 1e-6, weight_decay = 1e-3)\n        self.lr_decay = optim.lr_scheduler.CosineAnnealingLR(self.optim, 5, eta_min = 1e-9)\n        self.lr_decay2 = optim.lr_scheduler.StepLR(self.optim, 5, 0.95)\n        self.criterion = nn.BCEWithLogitsLoss()\n    def forward(self, x):\n        '''\n        Runs Inference on the Student Model, performing a sigmoid on the logits\n        '''\n        self.eval()\n        with torch.no_grad():\n            logits = torch.sigmoid(self.student(x))\n            return logits\n    def accuracy(self, y_pred, y_true):\n        one = y_pred >= 0.5\n        y_pred[:] = 0\n        y_pred[one] = 1\n        B, C = y_pred.shape\n        return torch.sum((y_pred == y_true).int()) / B / C\n    def training_loop(self, trainloader, valloader, NUM_EPOCHS, display_every = 1):\n        '''\n        Trains the Model \n        '''\n        liveloss = livelossplot.PlotLosses()\n        best_val_acc = 0\n        best_val_loss = 999\n        for EPOCH in range(NUM_EPOCHS):\n            \n            self.train()\n            logs = {}\n            '''\n            count = 0\n            total_loss = 0\n            for images, labels in trainloader:\n                self.optim.zero_grad()\n                images = images.to(self.device).to(torch.float32)\n                labels = labels.to(self.device).to(torch.float32)\n                pred, _, _, _, _, _ = self.student(images)\n                loss = self.criterion(pred, labels)\n                loss.backward()\n                self.optim.step()\n                count += 1\n                total_loss += loss.item()\n                del images\n                del labels\n                del pred\n                del loss\n                torch.cuda.empty_cache()\n                if count == display_every:\n                    break\n            logs['loss'] = total_loss / count\n            print(f\"EPOCH: {EPOCH}, total_loss: {logs['loss']}\")\n            self.lr_decay.step()\n            self.lr_decay2.step()\n            '''\n            self.eval()\n            with torch.no_grad():\n                logs['val_loss'] = 0\n                logs['accuracy'] = 0\n                count = 0\n                for images, labels in valloader:\n                    images = images.to(self.device).to(torch.float32)\n                    labels = labels.to(self.device).to(torch.float32)\n                    pred, _, _, _, _, _ = self.student(images)\n                    logs['val_loss'] += self.criterion(pred, labels).item()\n                    logs['accuracy'] += self.accuracy(torch.sigmoid(pred), labels)\n                    count += 1\n                    del images\n                    del labels\n                    del pred\n                    torch.cuda.empty_cache()\n                logs['val_loss'] /= count\n                logs['accuracy'] /= count\n            print(f\"ACCURACY: {logs['accuracy']}, loss: {logs['val_loss']}\")\n            liveloss.update(logs)\n            liveloss.send()\n            if logs['val_loss'] <= best_val_loss:\n                best_val_loss = logs['val_loss']\n                torch.save(self.state_dict(), \"./BestLoss.pth\")\n            if logs['accuracy'] >= best_val_acc:\n                best_val_acc = logs['accuracy']\n                torch.save(self.state_dict(), \"./BestAcc.pth\")\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\nstudentSolver = StudentSolver(studentTeacher.student, device)\nstudentSolver.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentSolver.load_state_dict(torch.load(\"../input/stage3/FinalModel.pth\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentSolver.training_loop(train_dataloader, val_dataloader, 100, display_every = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(studentSolver.state_dict(), \"./FinalModel.pth\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}