{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word2Vec- TensorFlow",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b560acb7df664e249d8ff576a3133ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_716fc91df01349a5b04e4714c4585536",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_45def9d51034412198d45ec7f3efdd1f",
              "IPY_MODEL_2f67fbde41664a0da30c2c78431fe4c4"
            ]
          }
        },
        "716fc91df01349a5b04e4714c4585536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45def9d51034412198d45ec7f3efdd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ed4d42e9349744c0b6b5ae3591e667bb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e45ee15fad35429e9cc4de40c7c2de0f"
          }
        },
        "2f67fbde41664a0da30c2c78431fe4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8620f56813d84735adb49c6c81227890",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50000/50000 [01:45&lt;00:00, 473.92it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0ab69e264bc42f8a808c92131a306f1"
          }
        },
        "ed4d42e9349744c0b6b5ae3591e667bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e45ee15fad35429e9cc4de40c7c2de0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8620f56813d84735adb49c6c81227890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0ab69e264bc42f8a808c92131a306f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PSjCrQjhCsj"
      },
      "source": [
        "This NoteBook will detail the process in generating Word2Vec or FastText Word Embeddings. In this notebook, we will use a Word2Vec Embeddings Generator, that generates word embeddings, although this paradigm can be applied to subword models too.\n",
        "\n",
        "To do this, break down the words into trigrams(or smaller) and use a convolutional encoder to encode each word, and do the same process(FastText)\n",
        "\n",
        "What This Model will not do:\n",
        "- This will not generate contextual word encodings, such as ELMo, ULMFit, or BERT\n",
        "- To perform this, use a LM that processes all text before(or after too(ULMFIt)) and pretrain the model to predict text\n",
        "- Upon Transfer Learning, Use the encodings for the LM to concatenate to the regular embeddings(contextual + Word2Vec.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZz3gsruu_ot"
      },
      "source": [
        "# Import Dependencies and Load in SST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egurX7bQl3k6"
      },
      "source": [
        "%%capture\n",
        "!pip install kaggle\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "import collections\n",
        "from sklearn.decomposition import PCA\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5UpxASgvQ8i"
      },
      "source": [
        "%%capture\n",
        "!mkdir /root/.kaggle/\n",
        "!cp -f ./kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NERPha2vYFr"
      },
      "source": [
        "%%capture\n",
        "#!kaggle datasets download -d atulanandjha/stanford-sentiment-treebank-v2-sst2\n",
        "#!unzip ./stanford-sentiment-treebank-v2-sst2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fq5gPXL87N0"
      },
      "source": [
        "%%capture\r\n",
        "# Load in the Dataset, IMDB for more data\r\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\r\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip\r\n",
        "!rm -f imdb-dataset-of-50k-movie-reviews.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQaPeFdxTGpc"
      },
      "source": [
        "def read(file_path):\r\n",
        "  vals = []\r\n",
        "  with open(file_path, 'r') as file:\r\n",
        "    for line in tqdm.tqdm(file):\r\n",
        "      #print(line)\r\n",
        "      idx = 0\r\n",
        "      for char in line:\r\n",
        "        idx += 1\r\n",
        "        if char == ' ':\r\n",
        "          break\r\n",
        "      vals += [line[idx:-1]]\r\n",
        "  return vals"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8n-2pH_TuVj"
      },
      "source": [
        "#dataset = read(file_path)[1:-1] #SST Dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttEmYY-c9HXs"
      },
      "source": [
        "reviews_pd = pd.read_csv(\"./IMDB Dataset.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLlTX_il9dqJ"
      },
      "source": [
        "reviews = [review for review in reviews_pd['review']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqgPUKTQTwLC"
      },
      "source": [
        "def process_corpus(corpus):\r\n",
        "  '''\r\n",
        "  Processes in the corpus, lowering all words, stemming the words, and splitting it into a list\r\n",
        "  '''\r\n",
        "  corpus_processed = []\r\n",
        "  for sentence in tqdm.tqdm(corpus):\r\n",
        "    sentence_tokenized = []\r\n",
        "    for word in nltk.word_tokenize(sentence):\r\n",
        "      if str.isalnum(word):\r\n",
        "        sentence_tokenized += [str.lower(word)]\r\n",
        "    corpus_processed += [sentence_tokenized]\r\n",
        "  return corpus_processed"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXL_aSeeT9Vl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b560acb7df664e249d8ff576a3133ea8",
            "716fc91df01349a5b04e4714c4585536",
            "45def9d51034412198d45ec7f3efdd1f",
            "2f67fbde41664a0da30c2c78431fe4c4",
            "ed4d42e9349744c0b6b5ae3591e667bb",
            "e45ee15fad35429e9cc4de40c7c2de0f",
            "8620f56813d84735adb49c6c81227890",
            "e0ab69e264bc42f8a808c92131a306f1"
          ]
        },
        "outputId": "e92b95e0-6572-4594-d4b2-648abc10b937"
      },
      "source": [
        "processed_dataset = process_corpus(reviews)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b560acb7df664e249d8ff576a3133ea8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkYRMnU2-X2k"
      },
      "source": [
        "# Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BmCb3lJ-I3c"
      },
      "source": [
        "class Word2Vec(keras.Model):\r\n",
        "  def __init__(self, emb_dim, corpus):\r\n",
        "    super().__init__()\r\n",
        "    self.unique_words, self.probabilities= self._compute_unique_words(corpus)\r\n",
        "    self.idx_2_word = {i: self.unique_words[i] for i in range(len(self.unique_words))}\r\n",
        "    self.word_2_idx = {self.unique_words[i]: i for i in range(len(self.unique_words))}\r\n",
        "    self.Center_Embeddings = keras.layers.Embedding(len(self.unique_words), output_dim = emb_dim)\r\n",
        "    self.Context_Embeddings = keras.layers.Embedding(len(self.unique_words), output_dim= emb_dim)\r\n",
        "    self.probability_tensor = np.array([self.probabilities[self.idx_2_word[word_idx]] for word_idx in range(len(self.unique_words))], dtype = np.float16)\r\n",
        "  def _compute_unique_words(self, corpus):\r\n",
        "    '''\r\n",
        "    Given a Corpus, this method returns a set of all unique words\r\n",
        "    '''\r\n",
        "    words = [word for article in corpus for word in article]\r\n",
        "    total_loss = len(words)\r\n",
        "    counter = collections.Counter(words)\r\n",
        "    probabilities = {word: counter[word] / total_loss for word in counter}\r\n",
        "    return list(set(words)), probabilities\r\n",
        "  def _extract_pos(self, sentence, sentence_idx, window_size):\r\n",
        "    '''\r\n",
        "    Extracts the positive samples(if there are enough to the right)\r\n",
        "    If there isnt enough to the right, there isn't enough to batch the input, and thus it would retunr simply None\r\n",
        "    '''\r\n",
        "    # Tensorize the Sentence\r\n",
        "    tensor = sentence\r\n",
        "    if sentence_idx + window_size >= len(sentence):\r\n",
        "      return None\r\n",
        "    if sentence_idx - window_size < 0:\r\n",
        "      return tensor[0: sentence_idx + window_size]\r\n",
        "    else:\r\n",
        "      return tensor[sentence_idx - window_size: sentence_idx + window_size]\r\n",
        "  def _prepare_pos(self, sentences, sentence_idx, window_size):\r\n",
        "    '''\r\n",
        "    Prepares a positive set, pruning out the None values(to get one clean batch)\r\n",
        "    '''\r\n",
        "    tensor = [self._extract_pos(sentence, sentence_idx, window_size) for sentence in sentences]\r\n",
        "    vals = [x for x in tensor if type(x) != type(None)]\r\n",
        "    return vals if len(vals) > 0 else None\r\n",
        "  def _training_batch(self, batch, neg_samps, center_tokens):\r\n",
        "    '''\r\n",
        "    batch: (B, 2WindowSize)\r\n",
        "    neg_samps: (NumNeg)\r\n",
        "    center_tokens: (B)\r\n",
        "    '''\r\n",
        "    B, N = batch.shape\r\n",
        "    batched_neg_samps = tf.repeat(tf.expand_dims(neg_samps, 0), B, axis = 0) # (B, NumNeg)\r\n",
        "    # Lookup embeddings for positive and negative samples\r\n",
        "    center_embeddings = self.Center_Embeddings(np.reshape(center_tokens, (B, 1))) # (B, 1, 300)\r\n",
        "    pos_embeddings = self.Context_Embeddings(batch) # (B, 2Window, 300)\r\n",
        "    neg_embeddings = self.Context_Embeddings(batched_neg_samps) # (B, NumNeg, 300)\r\n",
        "    # BatchMM\r\n",
        "    pos_similarity = tf.matmul(center_embeddings, tf.transpose(pos_embeddings,perm = (0, 2, 1)))\r\n",
        "    neg_similarity = tf.matmul(center_embeddings, tf.transpose(neg_embeddings, perm = (0, 2, 1)))\r\n",
        "\r\n",
        "    pos_scores = tf.squeeze(pos_similarity) # (B, 2Window)\r\n",
        "    neg_scores = tf.squeeze(neg_similarity) # (B, NumNeg)\r\n",
        "    pos_loss = tf.losses.binary_crossentropy(tf.ones_like(pos_scores), pos_scores, from_logits = True)\r\n",
        "    neg_loss = tf.losses.binary_crossentropy(tf.zeros_like(neg_scores), neg_scores, from_logits = True)\r\n",
        "    return tf.reduce_mean(pos_loss + neg_loss)\r\n",
        "  def batched_word2vec(self, sentences, window_size = 2, num_neg_samp = 5):\r\n",
        "    # Tokenize All Sentences\r\n",
        "    tokenized_sentences = []\r\n",
        "    longest_sent = 0\r\n",
        "    for sentence in sentences:\r\n",
        "      tokenized_sentences += [[self.word_2_idx[word] for word in sentence]]\r\n",
        "      longest_sent = max(len(sentence), longest_sent)\r\n",
        "    # Iterate over all indices\r\n",
        "    total_loss = tf.zeros((1))\r\n",
        "    for idx in range(longest_sent):\r\n",
        "      center_tokens= []\r\n",
        "      for sentence in tokenized_sentences:\r\n",
        "        if idx + window_size < len(sentence):\r\n",
        "           center_tokens += [sentence[idx]]\r\n",
        "      center_tokens = np.array(center_tokens)\r\n",
        "      tokenized = self._prepare_pos(tokenized_sentences, idx, window_size)\r\n",
        "      if type(tokenized) == type(None):\r\n",
        "        break\r\n",
        "      # Generate negative samples\r\n",
        "      neg_samps = []\r\n",
        "      while len(neg_samps) < num_neg_samp:\r\n",
        "        neg_samp = np.random.choice(len(self.unique_words), p = self.probability_tensor)\r\n",
        "        if neg_samp not in tokenized and neg_samp not in neg_samps:\r\n",
        "          neg_samps += [neg_samp]\r\n",
        "      neg_samps = np.array(neg_samps)\r\n",
        "      np_tokenized = np.array(tokenized)\r\n",
        "      total_loss = total_loss + self._training_batch(np_tokenized, neg_samps, center_tokens)\r\n",
        "    return total_loss\r\n",
        "  def call(self, idx):\r\n",
        "    array = np.zeros((1, 1))\r\n",
        "    array[:, :] = idx\r\n",
        "    return np.squeeze(self.Center_Embeddings(array))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HOdztYwCF1f"
      },
      "source": [
        "word2vec = Word2Vec(300, processed_dataset)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b9FVEcKCKZD"
      },
      "source": [
        "def training_fn(corpus, NUM_EPOCHS, batch_size, display_every = 24):\r\n",
        "  optim = tf.optimizers.Adam(learning_rate = 1e-3)\r\n",
        "  cur_idx = 0\r\n",
        "  corpus_length = len(corpus) // batch_size\r\n",
        "  for EPOCH in range(NUM_EPOCHS):\r\n",
        "    total_loss = 0\r\n",
        "    for i in tqdm.tqdm(range(display_every)):\r\n",
        "      cur_idx += 1\r\n",
        "      if cur_idx >= corpus_length:\r\n",
        "        cur_idx = 0\r\n",
        "      batch = corpus[cur_idx * batch_size: (cur_idx + 1) * batch_size]\r\n",
        "      with tf.GradientTape() as tape:\r\n",
        "        loss = word2vec.batched_word2vec(batch)\r\n",
        "      grads = tape.gradient(loss, word2vec.trainable_weights)\r\n",
        "      optim.apply_gradients(zip(grads, word2vec.trainable_weights))\r\n",
        "      total_loss += loss.numpy().item()\r\n",
        "    print(f'EPOCH: {EPOCH}, total_loss: {total_loss / display_every}')      "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmRx0ZqzTcM"
      },
      "source": [
        "def unique_words(num_words, corpus):\r\n",
        "  '''\r\n",
        "  corpus: sentences\r\n",
        "  Computes the N most frequent words, not including stopwords\r\n",
        "  '''\r\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "  words = []\r\n",
        "  for article in corpus:\r\n",
        "    for word in article:\r\n",
        "      if word not in stopwords:\r\n",
        "        words += [word]\r\n",
        "  counter = collections.Counter(words)\r\n",
        "  mapped_values = {word: counter[word] for word in counter}\r\n",
        "  reversed_orders = sorted(mapped_values.items(), key = lambda x: x[1], reverse = True)\r\n",
        "  \r\n",
        "  top_n_words = []\r\n",
        "  count = 0\r\n",
        "  for word, _ in reversed_orders:\r\n",
        "    top_n_words += [word]\r\n",
        "    count += 1\r\n",
        "    if count == num_words:\r\n",
        "      break\r\n",
        "  return top_n_words\r\n",
        "def visualize_embeddings(num_words, corpus):\r\n",
        "  '''\r\n",
        "  Visualizes the embeddings for the n most frequent words \r\n",
        "  '''\r\n",
        "  top_n_words = unique_words(num_words, corpus)\r\n",
        "  embeddings = []\r\n",
        "  #indices = []\r\n",
        "  for word in top_n_words:\r\n",
        "    embeddings += [word2vec(word2vec.word_2_idx[word])]\r\n",
        "    #indices += [word2vec.word_2_idx[word]]\r\n",
        "  embeddings = np.stack(embeddings, axis = 0) # (N, 300)\r\n",
        "  # Reduce Dimensionality of the Model\r\n",
        "  pca = PCA(n_components = 2)\r\n",
        "  reduced = pca.fit_transform(embeddings) # (N, 2)\r\n",
        "  for tensor in range(len(reduced)):\r\n",
        "    plt.scatter(reduced[tensor][0], reduced[tensor][1])\r\n",
        "    plt.text(reduced[tensor][0], reduced[tensor][1], top_n_words[tensor])\r\n",
        "  plt.show()\r\n",
        "def visualize_words(words):\r\n",
        "  embeddings = []\r\n",
        "  for word in words:\r\n",
        "    embeddings += [word2vec(word2vec.word_2_idx[word])]\r\n",
        "  embeddings = np.stack(embeddings, axis = 0) # (N, 300)\r\n",
        "  # Reduce Dimensionality of the Model\r\n",
        "  pca = PCA(n_components = 2)\r\n",
        "  reduced = pca.fit_transform(embeddings) # (N, 2)\r\n",
        "  for tensor in range(len(reduced)):\r\n",
        "    plt.scatter(reduced[tensor][0], reduced[tensor][1])\r\n",
        "    plt.text(reduced[tensor][0], reduced[tensor][1], words[tensor])\r\n",
        "  plt.show() \r\n",
        "def distances(center_vector, matrix):\r\n",
        "  '''\r\n",
        "  Computes Euclidean Distance \r\n",
        "  '''\r\n",
        "  vals = matrix - center_vector\r\n",
        "  vals = vals ** 2\r\n",
        "  vals = np.sum(vals, axis = -1) # (N)\r\n",
        "  return vals\r\n",
        "def closest(word, k, farthest = False):\r\n",
        "  '''\r\n",
        "  finds the k nearest words to this word.\r\n",
        "  '''\r\n",
        "  center_embeddings = word2vec(word2vec.word_2_idx[word])\r\n",
        "  embeddings = word2vec.Center_Embeddings.weights[0]\r\n",
        "  distance_matrix = distances(center_embeddings, embeddings)\r\n",
        "  distance_map = {idx: distance_matrix[idx] for idx in range(len(distance_matrix))}\r\n",
        "  sorted_map = sorted(distance_map.items(), key = lambda x: x[1], reverse = farthest)\r\n",
        "  return sorted_map[:k ]\r\n",
        "def closest_embeddings(center_embeddings, k, farthest = False):\r\n",
        "  embeddings = word2vec.Center_Embeddings.weights[0]\r\n",
        "  distance_matrix = distances(center_embeddings, embeddings)\r\n",
        "  distance_map = {idx: distance_matrix[idx] for idx in range(len(distance_matrix))}\r\n",
        "  sorted_map = sorted(distance_map.items(), key = lambda x: x[1], reverse = farthest)\r\n",
        "  return sorted_map[:k]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUa4gFCXxVdh"
      },
      "source": [
        "with tf.device(\"GPU:0\"):\r\n",
        "  training_fn(processed_dataset, 150, 512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT6bdC_XUy-z"
      },
      "source": [
        "word2vec.save_weights(\"./model/model\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WXzXrs9bZJt"
      },
      "source": [
        "!zip -r ./model.zip ./model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8zeIaCWyuj6"
      },
      "source": [
        "visualize_embeddings(100, processed_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6o39ffzYtHk"
      },
      "source": [
        "def analogy(first, second, third):\r\n",
        "  first_embeddings = word2vec(word2vec.word_2_idx[first])\r\n",
        "  second_embeddings = word2vec(word2vec.word_2_idx[second])\r\n",
        "  third_embeddings = word2vec(word2vec.word_2_idx[third])\r\n",
        "  return closest_embeddings(first_embeddings - second_embeddings + third_embeddings, 5)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDJl84gbZkjh"
      },
      "source": [
        "def read(indices):\r\n",
        "  for word, _ in indices:\r\n",
        "    print(word2vec.idx_2_word[word])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgPvzi4ooomU"
      },
      "source": [
        "read(analogy('england', 'english', 'france'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc3AKcCmqqk-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}