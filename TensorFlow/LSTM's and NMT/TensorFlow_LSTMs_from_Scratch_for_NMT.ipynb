{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TensorFlow - LSTMs from Scratch for NMT ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f2cd7e6d2ee4cf59598d760ddde5cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd7698d517144fc5a138157a7377eb89",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_998f74dca0944c80ae6c17d0d6f17760",
              "IPY_MODEL_5d32fd836a034099a0550155653ae115"
            ]
          }
        },
        "bd7698d517144fc5a138157a7377eb89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "998f74dca0944c80ae6c17d0d6f17760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a827a147ccbd4711b8edf596344d9966",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01655f6a557d437b8c7212740eed97fe"
          }
        },
        "5d32fd836a034099a0550155653ae115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9edb841462fa4b079a34c1c1fd844308",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [00:12&lt;00:00,  8.20it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ba3ae7d395b426da784a4431b2665fe"
          }
        },
        "a827a147ccbd4711b8edf596344d9966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01655f6a557d437b8c7212740eed97fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9edb841462fa4b079a34c1c1fd844308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ba3ae7d395b426da784a4431b2665fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GdeImBuagSe"
      },
      "source": [
        "# Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz0T627vl61x"
      },
      "source": [
        "%%capture\r\n",
        "!pip install kaggle\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras as keras\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import math\r\n",
        "import copy\r\n",
        "import tqdm.notebook as tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "nltk.download(\"punkt\")\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJjhQ7zOtISO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8532e3b5-b2ec-4e75-d6ec-1de951f276c6"
      },
      "source": [
        "!mkdir /root/.kaggle/\r\n",
        "!cp -f ./kaggle.json /root/.kaggle/kaggle.json\r\n",
        "!chmod 600 /root/.kaggle/kaggle.json\r\n",
        "!rm -f kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜/root/.kaggle/â€™: File exists\n",
            "cp: cannot stat './kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-2iK-af1e4k",
        "outputId": "784f026b-9b8b-4c5a-efa0-8154eea7e3e1"
      },
      "source": [
        "!kaggle datasets download -d dhruvildave/en-fr-translation-dataset\r\n",
        "!unzip en-fr-translation-dataset.zip\r\n",
        "!rm -f en-fr-translation-dataset.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading en-fr-translation-dataset.zip to /content\n",
            "100% 2.54G/2.54G [00:37<00:00, 54.2MB/s]\n",
            "100% 2.54G/2.54G [00:37<00:00, 72.8MB/s]\n",
            "Archive:  en-fr-translation-dataset.zip\n",
            "  inflating: en-fr.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehqQulXfJ9zF"
      },
      "source": [
        "# Load Dataset From Kaggle  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebeJVWR3EIfV"
      },
      "source": [
        "# Load in the CSV File\r\n",
        "en_fr_csv = pd.read_csv(\"en-fr.csv\", nrows =  100)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asfuoViSFLbp"
      },
      "source": [
        "english_corpus = list(en_fr_csv['en'])\r\n",
        "french_corpus = list(en_fr_csv['fr'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKjG2x3_IUwx"
      },
      "source": [
        "def process_corpus(english, french, min_length = 15):\r\n",
        "  corpus_english_tokenized = []\r\n",
        "  corpus_french_tokenized = []\r\n",
        "  for sentence_idx in tqdm.tqdm(range(len(english))):\r\n",
        "    english_sentence = english[sentence_idx]\r\n",
        "    french_sentence = french[sentence_idx]\r\n",
        "    english_tokenized = []\r\n",
        "    french_tokenized = []\r\n",
        "    try:\r\n",
        "      for english_word in nltk.word_tokenize(english_sentence, language = 'english'):\r\n",
        "        processed_english = re.sub(r'[^\\w\\s]', \"\", str.lower(english_word))\r\n",
        "        if processed_english != \"\":\r\n",
        "          english_tokenized += [processed_english]\r\n",
        "      for french_word in nltk.word_tokenize(french_sentence, language = 'french'):\r\n",
        "        processed_french = re.sub(r'[^\\w\\s]', \"\", str.lower(french_word))\r\n",
        "        if processed_french != \"\":\r\n",
        "          french_tokenized += [processed_french]\r\n",
        "      if len(english_tokenized) >= min_length:\r\n",
        "        corpus_english_tokenized += [english_tokenized]\r\n",
        "        corpus_french_tokenized += [french_tokenized]\r\n",
        "    except:\r\n",
        "      continue\r\n",
        "  return corpus_english_tokenized, corpus_french_tokenized"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5f2cd7e6d2ee4cf59598d760ddde5cef",
            "bd7698d517144fc5a138157a7377eb89",
            "998f74dca0944c80ae6c17d0d6f17760",
            "5d32fd836a034099a0550155653ae115",
            "a827a147ccbd4711b8edf596344d9966",
            "01655f6a557d437b8c7212740eed97fe",
            "9edb841462fa4b079a34c1c1fd844308",
            "3ba3ae7d395b426da784a4431b2665fe"
          ]
        },
        "id": "ScsvyaiWvH4H",
        "outputId": "c11d9980-80ec-45d2-b0a8-726d60cf6d15"
      },
      "source": [
        "english_processed, french_processed = process_corpus(english_corpus, french_corpus)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f2cd7e6d2ee4cf59598d760ddde5cef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYzl_-bGHQF"
      },
      "source": [
        "class EnFrDataset(keras.utils.Sequence):\r\n",
        "  def __init__(self, english, french, batch_size):\r\n",
        "    self.english = english\r\n",
        "    self.french = french\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.cur_idx = 0\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.english) // self.batch_size\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    english_corpus = self.english[self.cur_idx * self.batch_size: (self.cur_idx + 1) * self.batch_size]\r\n",
        "    french_corpus = self.french[self.cur_idx * self.batch_size: (self.cur_idx + 1) * self.batch_size]\r\n",
        "    self.cur_idx += 1\r\n",
        "    if self.cur_idx >= self.__len__():\r\n",
        "      self.cur_idx = 0\r\n",
        "    return english_corpus, french_corpus\r\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuLpXkrsL59R"
      },
      "source": [
        "TranslationDataset = EnFrDataset(copy.deepcopy(english_processed), copy.deepcopy(french_processed), 32)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aX8zgx3NIsg"
      },
      "source": [
        "Word Embeddings and loading Pretrained GLoVE vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faAqSaWZNORO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339f3a27-8975-4e83-e39e-e11bad350b9f"
      },
      "source": [
        "!kaggle datasets download -d rtatman/glove-global-vectors-for-word-representation\r\n",
        "!unzip glove-global-vectors-for-word-representation.zip\r\n",
        "!rm -f glove.6B.100d.txt\r\n",
        "!rm -f glove.6B.50d.txt\r\n",
        "!rm -f glove-global-vectors-for-word-representation.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove-global-vectors-for-word-representation.zip to /content\n",
            " 98% 450M/458M [00:05<00:00, 103MB/s]\n",
            "100% 458M/458M [00:05<00:00, 88.4MB/s]\n",
            "Archive:  glove-global-vectors-for-word-representation.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg8bASEnNqX4"
      },
      "source": [
        "glove_path = \"./glove.6B.200d.txt\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0hNLimkM6Ug"
      },
      "source": [
        "class WordEmbeddings(keras.layers.Layer):\r\n",
        "  def __init__(self, corpus, glove_path, french = False):\r\n",
        "    super().__init__()\r\n",
        "    self.glove_path = glove_path\r\n",
        "    self.french = french\r\n",
        "    self.unique_words = self._compute_unique_words(corpus)\r\n",
        "    self.word_2_idx = {self.unique_words[idx]: idx + 4 for idx in range(len(self.unique_words))}\r\n",
        "    self.word_2_idx['<PAD>'] = 0\r\n",
        "    self.word_2_idx['<UNK>'] = 1\r\n",
        "    self.word_2_idx[\"<START>\"] = 2\r\n",
        "    self.word_2_idx['<END>'] = 3\r\n",
        "\r\n",
        "    self.idx_2_word = {idx + 4: self.unique_words[idx] for idx in range(len(self.unique_words))}\r\n",
        "    self.idx_2_word[0] = \"<PAD>\"\r\n",
        "    self.idx_2_word[1] = \"<UNK>\"\r\n",
        "    self.idx_2_word[2] = \"<START>\"\r\n",
        "    self.idx_2_word[3] = \"<END>\"\r\n",
        "\r\n",
        "    self.len_vocab = len(self.unique_words) + 4\r\n",
        "    \r\n",
        "    self.embedding_dim = 200\r\n",
        "    if not self.french:\r\n",
        "      self.glove_embeddings = self.load_glove(self.glove_path)\r\n",
        "      self.word_embeddings = keras.layers.Embedding(self.len_vocab, self.embedding_dim, weights = [self.glove_embeddings])\r\n",
        "    else:\r\n",
        "      self.word_embeddings = keras.layers.Embedding(self.len_vocab, self.embedding_dim)\r\n",
        "  def _compute_unique_words(self, corpus):\r\n",
        "    '''\r\n",
        "    Computes the Unique Words inside of the corpus\r\n",
        "    corpus: the corpus to be trained on.\r\n",
        "    '''\r\n",
        "    words = [word for sentence in corpus for word in sentence]\r\n",
        "    return list(set(words))\r\n",
        "  def load_glove(self, glove_path):\r\n",
        "    '''\r\n",
        "    Loads in the GLoVE vectors \r\n",
        "    '''\r\n",
        "    initializer = tf.initializers.GlorotUniform()\r\n",
        "    embeddings = initializer(shape = (self.len_vocab, self.embedding_dim)).numpy()\r\n",
        "    with open(glove_path, 'r') as file:\r\n",
        "      for line in file:\r\n",
        "        array = line.split()\r\n",
        "        word = array[0]\r\n",
        "        if word in self.word_2_idx:\r\n",
        "          word_embed = np.array([float(val) for val in array[1:]])\r\n",
        "          embeddings[self.word_2_idx[word]] = word_embed\r\n",
        "    return embeddings\r\n",
        "  def _max_length(self, x):\r\n",
        "    '''\r\n",
        "    Computes the maximum length of a corpus\r\n",
        "    '''\r\n",
        "    maximum_length = 0\r\n",
        "    for sentence in x:\r\n",
        "      maximum_length = max(len(sentence), maximum_length)\r\n",
        "    return maximum_length\r\n",
        "  def _tokenize(self, x, max_sent_length):\r\n",
        "    '''\r\n",
        "    Pads(or Truncates) and tokenizes every word in the corpus\r\n",
        "    '''\r\n",
        "    tokenized_sentences = []\r\n",
        "    for sentence in x:\r\n",
        "      tokenized_sent = [self.word_2_idx[\"<PAD>\"]] * max_sent_length\r\n",
        "      for word_idx in range(max_sent_length):\r\n",
        "        if word_idx >= len(sentence):\r\n",
        "          break\r\n",
        "        word = sentence[word_idx]\r\n",
        "        if word in self.word_2_idx:\r\n",
        "          tokenized_sent[word_idx] = self.word_2_idx[word]\r\n",
        "        else:\r\n",
        "          tokenized_sent[word_idx] = self.word_2_idx['<UNK>']\r\n",
        "      tokenized_sentences += [tokenized_sent]\r\n",
        "    return tokenized_sentences\r\n",
        "  def grab_embeddings(self, x):\r\n",
        "    '''\r\n",
        "    x: Tensor(B, L)\r\n",
        "    '''\r\n",
        "    return self.word_embeddings(x)\r\n",
        "  def call(self, x, max_sent_length = None):\r\n",
        "    '''\r\n",
        "    Tokenizes and Grabs the Embeddings of a given sentence\r\n",
        "    x: List of Sentences(List of Words)\r\n",
        "    padded up to max_sent_length\r\n",
        "    '''\r\n",
        "    x = copy.deepcopy(x)\r\n",
        "    if not max_sent_length:\r\n",
        "      max_sent_length = self._max_length(x)\r\n",
        "    for sentence_idx in range(len(x)):\r\n",
        "      assert len(x[sentence_idx]) > 0, x\r\n",
        "      x[sentence_idx] = ['<START>'] + x[sentence_idx]\r\n",
        "\r\n",
        "      if len(x[sentence_idx]) > max_sent_length:\r\n",
        "        x[sentence_idx][max_sent_length] = '<END>'\r\n",
        "      else:\r\n",
        "        x[sentence_idx][-1] = \"<END>\"\r\n",
        "    tokens = np.array(self._tokenize(x, max_sent_length))\r\n",
        "    #B, L = tokens.shape\r\n",
        "    #for l in range(L):\r\n",
        "    #  vals = tokens[0, l]\r\n",
        "    #  print(self.idx_2_word[vals.item()])\r\n",
        "    \r\n",
        "    del x\r\n",
        "    if not self.french:\r\n",
        "      return self.word_embeddings(tokens)\r\n",
        "    return tokens    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrnE0OY7XvLR"
      },
      "source": [
        "class EnglishFrenchEmbeddings(keras.layers.Layer):\r\n",
        "  '''\r\n",
        "  Stores the English and French Embeddings, which is handy during training\r\n",
        "  '''\r\n",
        "  def __init__(self, en_corpus, fr_corpus, glove_path):\r\n",
        "    super().__init__()\r\n",
        "    self.english = WordEmbeddings(en_corpus, glove_path)\r\n",
        "    self.french = WordEmbeddings(fr_corpus, None, french = True)\r\n",
        "  def call(self, text, max_length = None, french = False):\r\n",
        "    if not french:\r\n",
        "      return self.english(text, max_sent_length = max_length)\r\n",
        "    else:\r\n",
        "      return self.french(text, max_sent_length = max_length)\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2nRyD31FSH8"
      },
      "source": [
        "#LSTM From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBsWhrJdFVHN"
      },
      "source": [
        "Step 1: One LSTM Cell "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyaL6B3uzabJ"
      },
      "source": [
        "class LSTMCell(keras.layers.Layer):\r\n",
        "  def __init__(self, cell_state):\r\n",
        "    super().__init__()\r\n",
        "    self.cell_state = cell_state\r\n",
        "    self.i = keras.layers.Dense(self.cell_state, activation = 'sigmoid')\r\n",
        "    self.f = keras.layers.Dense(self.cell_state, activation = 'sigmoid')\r\n",
        "    self.o = keras.layers.Dense(self.cell_state, activation = 'sigmoid')\r\n",
        "    self.g = keras.layers.Dense(self.cell_state, activation = 'tanh')\r\n",
        "  def call(self, x, hidden_state, cell_state):\r\n",
        "    '''\r\n",
        "    x: Tensor(B, hidden_size)\r\n",
        "    hidden_state: Tensor(B, hidden_size)\r\n",
        "    cell_state: (B, cell_size)\r\n",
        "    '''\r\n",
        "    concat = tf.concat([x, hidden_state], axis = 1) # (B, 2 * hidden_size)\r\n",
        "    i = self.i(concat) # (B, cell_state)\r\n",
        "    f = self.f(concat) # (B, cell_state)\r\n",
        "    o = self.o(concat) # (B, cell_state)\r\n",
        "    g = self.g(concat) # (B, cell_state)\r\n",
        "    forgotten = cell_state * f\r\n",
        "    remember = i * g\r\n",
        "    new_cell = remember + forgotten\r\n",
        "    hidden_state = o * tf.tanh(new_cell)\r\n",
        "    return hidden_state, new_cell"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEV0Uq1dFcK1"
      },
      "source": [
        "Step 2: Bidirection LSTM Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBGRvWC5zfh3"
      },
      "source": [
        "class SingleDirectionLSTM(keras.layers.Layer):\r\n",
        "  def __init__(self, hidden_size, cell_size):\r\n",
        "    super().__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.cell_size = cell_size\r\n",
        "    # Learned Hidden and Cell State \r\n",
        "    initializer = tf.initializers.GlorotUniform()\r\n",
        "    self.initial_hidden = tf.Variable(initial_value = initializer(shape = (1, self.hidden_size), dtype = \"float32\"), trainable = True)\r\n",
        "    self.initial_cell = tf.Variable(initial_value = initializer(shape = (1, self.cell_size), dtype = 'float32'), trainable = True)\r\n",
        "    self.LSTM_Cell = LSTMCell(self.cell_size)\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    '''\r\n",
        "    Encodes a Given Input:\r\n",
        "    x: Tensor(B, L, C)\r\n",
        "    Returns all Hidden States\r\n",
        "    '''\r\n",
        "    B, L, _ = x.shape\r\n",
        "    hidden_state = tf.repeat(tf.identity(self.initial_hidden), B, axis = 0)\r\n",
        "    cell_state = tf.repeat(tf.identity(self.initial_cell), B, axis = 0)\r\n",
        "    hidden_states = []\r\n",
        "    for i in range(L):\r\n",
        "      hidden_state, cell_state = self.LSTM_Cell(x[:, i, :], hidden_state,cell_state)\r\n",
        "      hidden_states += [copy.deepcopy(hidden_state)]\r\n",
        "    return tf.stack(hidden_states, axis = 1)\r\n",
        "\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IMFx_yXM7Gw"
      },
      "source": [
        "class BidirectionalLSTMEncoder(keras.layers.Layer):\r\n",
        "  def __init__(self, hidden_size, cell_size):\r\n",
        "    super().__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.cell_size = cell_size\r\n",
        "    self.LSTMCell = SingleDirectionLSTM(self.hidden_size, self.cell_size)\r\n",
        "  def call(self, x):\r\n",
        "    '''\r\n",
        "    x: Tensor(B, L, C)\r\n",
        "    '''\r\n",
        "    reversed_x = []\r\n",
        "    for i in range(x.shape[1] - 1, -1, -1):\r\n",
        "      reversed_x += [x[:, i, :]]\r\n",
        "    reversed_x = tf.stack(reversed_x, axis = 1) # (B, L, C)\r\n",
        "    # Run Both the Reversed and Forward versions\r\n",
        "    forward_hidden_states = self.LSTMCell(x) # (B, L, C)\r\n",
        "    reversed_hidden_states = self.LSTMCell(reversed_x) # (B, L, C)\r\n",
        "\r\n",
        "    output_hidden_state = tf.concat([forward_hidden_states[:, -1, :], forward_hidden_states[:, -1, :]], axis = 1) # (B, 2 * self.hidden_size)\r\n",
        "    hidden_states = tf.concat([forward_hidden_states, reversed_hidden_states], axis = 2)\r\n",
        "    \r\n",
        "    return hidden_states, output_hidden_state"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_vhYRtyFfnS"
      },
      "source": [
        "Step 3: Attention LSTM Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jylKi6ktzuGy"
      },
      "source": [
        "class LuongAttention(keras.layers.Layer):\r\n",
        "  '''\r\n",
        "  Computes Multiplicative(Or Luong) Attention across hidden states \r\n",
        "  '''\r\n",
        "  def __init__(self, decoder_size):\r\n",
        "    super().__init__()\r\n",
        "    self.decoder_size = decoder_size\r\n",
        "    self.encoder_linear = keras.layers.Dense(self.decoder_size, activation = None)\r\n",
        "  def call(self, decoder_state, encoder_states):\r\n",
        "    '''\r\n",
        "    decoder_state: Shape(B, decoder_size)\r\n",
        "    encoder_states: Shape(B, L, encoder_size)\r\n",
        "    '''\r\n",
        "    # Expand Decoder State Across middle axis\r\n",
        "    decoder_expanded = tf.expand_dims(decoder_state, 1) # (B, 1, decoder_size)\r\n",
        "    encoder_linear = self.encoder_linear(decoder_expanded) # (B, L, decoder_size)\r\n",
        "    # Dot Product\r\n",
        "    att_mat = keras.activations.softmax(tf.matmul(decoder_expanded, tf.transpose(encoder_linear, perm = (0, 2, 1)))) # (B, 1, L)\r\n",
        "    att_mat = tf.transpose(att_mat, perm = (0, 2, 1)) # (B, L, 1)\r\n",
        "    att_scores = att_mat * encoder_linear\r\n",
        "    return att_scores \r\n",
        "    "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcCpxNwezoF5"
      },
      "source": [
        "class LSTMDecoder(keras.layers.Layer):\r\n",
        "  '''\r\n",
        "  LSTM Decoder with no trainable hidden_state\r\n",
        "  '''\r\n",
        "  def __init__(self, hidden_size, cell_size, embeddings):\r\n",
        "    super().__init__()\r\n",
        "    self.embeddings = embeddings # This embeddings layer is used to extract special characters.\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.cell_size = cell_size\r\n",
        "    initializer = tf.initializers.GlorotUniform()\r\n",
        "    self.initial_cell_state = tf.Variable(initializer(shape = (1, self.cell_size), dtype = 'float32'), trainable = True)\r\n",
        "    self.attention_fn = LuongAttention(self.hidden_size)\r\n",
        "    self.LSTMCell = LSTMCell(self.cell_size)\r\n",
        "    self.Dense = keras.layers.Dense(self.embeddings.french.len_vocab)\r\n",
        "    self.loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits = True)\r\n",
        "  def call(self, last_hidden_state, encoder_states, y = None, max_length = 25, print_states = False):\r\n",
        "    '''\r\n",
        "    x: Tensor(B, 2 * hidden_size)\r\n",
        "    encoder_states: Tensor(B, L_encoder, 2 * hidden_size)\r\n",
        "    y: Tensor(B, L_Decoder), returns loss if y is provided(using teacher-student forcing.)\r\n",
        "    Runs(AutoRegressively to predict the outputs)\r\n",
        "    '''\r\n",
        "    B, _ = last_hidden_state.shape\r\n",
        "    _, L_encoder, _ = encoder_states.shape\r\n",
        "    if type(y) != type(None):\r\n",
        "      _, L_decoder = y.shape\r\n",
        "    cell_state = tf.repeat(tf.identity(self.initial_cell_state), B, axis = 0) # (B, Cell_size)\r\n",
        "    hidden_state = last_hidden_state\r\n",
        "    if type(y) == type(None):\r\n",
        "      # decode using no y values(final predictions), This should only be used at eval time\r\n",
        "      # Initialize Start Keys as beginning inputs\r\n",
        "      START_TOKEN = self.embeddings.french.word_2_idx[\"<START>\"]\r\n",
        "      END_TOKEN = self.embeddings.french.word_2_idx[\"<END>\"]\r\n",
        "      \r\n",
        "      current_input = tf.expand_dims(tf.repeat(np.array(START_TOKEN), B, axis = 0), 1)\r\n",
        "      current_input = tf.squeeze(self.embeddings.french.grab_embeddings(current_input))\r\n",
        "\r\n",
        "      output_sentences =[[\"<START>\"] for i in range(B)]\r\n",
        "      ended_sentences = [False] * B\r\n",
        "      # ended_sentences asks if the END token has been generated\r\n",
        "      for i in range(max_length):\r\n",
        "        hidden_state, cell_state = self.LSTMCell(current_input, hidden_state, cell_state)\r\n",
        "        # Get the Hidden State and Use this for Attention Logits\r\n",
        "        attention_logits = tf.squeeze(self.attention_fn(hidden_state, encoder_states))\r\n",
        "        # Concatenate the Attention Logits and Hidden States\r\n",
        "        attended = tf.concat([hidden_state, attention_logits], 1)\r\n",
        "        logits = tf.keras.activations.softmax(self.Dense(attended))\r\n",
        "        selected_words = tf.argmax(logits, axis = -1)\r\n",
        "        B = selected_words.shape[0]\r\n",
        "        #print('---------------------')\r\n",
        "        #print(f\"{self.embeddings.french.idx_2_word[selected_words[0].numpy().item()]}\")\r\n",
        "        #print('---------------------')\r\n",
        "        for word_idx in range(B):\r\n",
        "          if ended_sentences[word_idx]:\r\n",
        "            continue\r\n",
        "          if selected_words[word_idx].numpy().item() == END_TOKEN:\r\n",
        "            # Sentence Finished\r\n",
        "            ended_sentences[word_idx] = True\r\n",
        "            output_sentences[word_idx] += [self.embeddings.french.idx_2_word[selected_words[word_idx].numpy().item()]]\r\n",
        "          else:\r\n",
        "            output_sentences[word_idx] += [self.embeddings.french.idx_2_word[selected_words[word_idx].numpy().item()]]\r\n",
        "        # Create next input\r\n",
        "        current_input = tf.expand_dims(selected_words, axis = 1)\r\n",
        "        current_input = tf.squeeze(self.embeddings.french.grab_embeddings(current_input))\r\n",
        "      return output_sentences\r\n",
        "    else:\r\n",
        "      total_loss = tf.identity(np.zeros((1))) # Create empty loss tensor\r\n",
        "      for i in range(L_decoder - 1):\r\n",
        "        # Take the input from ground truth, use to predict next word.\r\n",
        "        y_input = y[:, i]\r\n",
        "        not_pad = y_input != self.embeddings.french.word_2_idx[\"<PAD>\"]\r\n",
        "        y_input = tf.expand_dims(y_input, axis = 1)\r\n",
        "        y_input = tf.squeeze(self.embeddings.french.grab_embeddings(y_input)) \r\n",
        "        # Grab embeddings\r\n",
        "        y_GT = y[:, i + 1]\r\n",
        "        hidden_state, cell_state = self.LSTMCell(y_input, hidden_state, cell_state)\r\n",
        "        # Attention logits\r\n",
        "        attention_logits = tf.squeeze(self.attention_fn(hidden_state, encoder_states))\r\n",
        "        # Concatenate\r\n",
        "        concatenated = tf.concat([hidden_state, attention_logits], 1)\r\n",
        "        logits = self.Dense(concatenated)\r\n",
        "      \r\n",
        "        logits = tf.cast(logits, tf.double)\r\n",
        "        # TEST\r\n",
        "        if print_states:\r\n",
        "          B, C = logits.shape\r\n",
        "          test_logits = tf.keras.activations.softmax(logits)\r\n",
        "          digit_predicted = tf.argmax(test_logits, axis = -1)\r\n",
        "          #print('--------------------')\r\n",
        "          #print(f\"pred: {self.embeddings.french.idx_2_word[digit_predicted[0].numpy().item()]}\")\r\n",
        "          #print(f\"GT: {self.embeddings.french.idx_2_word[y_GT[0].item()]}\")\r\n",
        "          #print('-------------------')\r\n",
        "        logits = logits[not_pad] # Remove Sentences from the pad dimension, to not pollute the loss\r\n",
        "        y_GT = y_GT[not_pad]\r\n",
        "\r\n",
        "        \r\n",
        "        # Compute loss\r\n",
        "        total_loss = total_loss + self.loss_function(y_GT, logits)\r\n",
        "      return total_loss / B\r\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgcBvi5DFjB4"
      },
      "source": [
        "Step 4: LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNsWNCBMacqe"
      },
      "source": [
        "class NMTLSTM(keras.Model):\r\n",
        "  def __init__(self, en_corpus, fr_corpus, glove_path, hidden_size, cell_size):\r\n",
        "    super().__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.cell_size = cell_size\r\n",
        "    self.embeddings = EnglishFrenchEmbeddings(en_corpus, fr_corpus, glove_path)\r\n",
        "    self.LSTMEncoder = BidirectionalLSTMEncoder(self.hidden_size, self.cell_size)\r\n",
        "    self.Decoder = LSTMDecoder(2 * self.hidden_size, 2 * self.cell_size, self.embeddings)\r\n",
        "  def call(self, x, y = None, print_states = False):\r\n",
        "    '''\r\n",
        "    Encodes and Decodes the text.\r\n",
        "    x: List of sentences(list of words) in english\r\n",
        "    y: List of sentences(list of words) in french \r\n",
        "    '''\r\n",
        "    x_english = self.embeddings.call(x) # (B, L_encoder, 200)\r\n",
        "    #print('---------------')\r\n",
        "    if type(y) != type(None):\r\n",
        "      y_french = self.embeddings.call(y, french = True) # (B, L_decoder)\r\n",
        "    #print('------------------')\r\n",
        "    # Encode the english words\r\n",
        "    hidden_states, beginning_hidden_state = self.LSTMEncoder(x_english)\r\n",
        "    # Shapes hidden_state: Tensor(B, L, 2 * hidden_size), beginning_hidden_state = Tensor(B, 2 * hidden_size)\r\n",
        "    if type(y) != type(None):\r\n",
        "      #B, L = y_french.shape\r\n",
        "      #for l in range(L):\r\n",
        "      #  vals = y_french[0, l]\r\n",
        "      #  print(self.embeddings.french.idx_2_word[vals])\r\n",
        "      #raise Exception()\r\n",
        "      loss = self.Decoder(beginning_hidden_state, hidden_states, y = y_french, print_states = print_states)\r\n",
        "      return loss\r\n",
        "    else:\r\n",
        "      output_sentence = self.Decoder(beginning_hidden_state, hidden_states)\r\n",
        "      return output_sentence"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjTybL7Ua2TO"
      },
      "source": [
        "model = NMTLSTM(english_processed, french_processed, glove_path, 256, 256)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OjMowNKR2B2"
      },
      "source": [
        "model.load_weights(\"./model/model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG5QWGrbFlYb"
      },
      "source": [
        "Step 5: Train the Model with Gradient Tape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDGMZ5rrz0Yq"
      },
      "source": [
        "def training_fn(NUM_EPOCHS, display_every = 16):\r\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate = keras.optimizers.schedules.ExponentialDecay(1e-3, display_every, 0.9))\r\n",
        "  for EPOCH in range(NUM_EPOCHS):\r\n",
        "    total_loss = 0\r\n",
        "    for i in tqdm.tqdm(range(display_every)):\r\n",
        "      for english, french in TranslationDataset:\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "          loss = model(english, y = french, training = True)\r\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\r\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
        "        del grads\r\n",
        "        total_loss += loss.numpy().item()\r\n",
        "        break\r\n",
        "    # Visualize Trained Translations\r\n",
        "    print(f\"English: {TranslationDataset.english[0:2]}\")\r\n",
        "    print(f\"French_pred: {model(TranslationDataset.english[0:2], training = False, print_states = True)}\")\r\n",
        "    print(f\"French: {TranslationDataset.french[0:2]}\")\r\n",
        "    print(f\"EPOCH: {EPOCH}, total_loss: {total_loss / display_every}\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SANSsfny4ENk"
      },
      "source": [
        "def test_fn(english):\n",
        "  '''\n",
        "  Tests the model on a batch of english sentences \n",
        "  '''\n",
        "  return model(english, training = False)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfncsUBogqpi"
      },
      "source": [
        "with tf.device(\"GPU:0\"):\n",
        "  training_fn(15, display_every = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4a5W3UhRORe"
      },
      "source": [
        "model.save_weights(\"./model/model\")"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjVHpT9LC9ow"
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": []
    }
  ]
}