{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning with Heroku and PyTorch",
      "provenance": [],
      "collapsed_sections": [
        "edQ67ZqB5KXl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edQ67ZqB5KXl"
      },
      "source": [
        "# Model Structure: Modified SEResNet152 with DistilBERT Captioning System(Both Transfer Learned)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeuoPKvD5Tzt"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3syn9XO4c73y"
      },
      "source": [
        "%%capture\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm.notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "import copy\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install livelossplot\n",
        "import livelossplot\n",
        "\n",
        "!pip install timm\n",
        "import timm\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "\n",
        "!pip install kaggle\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T14OzESlDjSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feead3a9-bfef-474f-c431-3ef9a7bbd094"
      },
      "source": [
        "!mkdir /root/.kaggle/\n",
        "!cp -f ./kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKVGScmXC_YU"
      },
      "source": [
        "Load in the Dataset(FlicKR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMMWMxuoDUw6"
      },
      "source": [
        "%%capture\n",
        "#!kaggle datasets download -d hsankesara/flickr-image-dataset\n",
        "#!unzip -a ./flickr-image-dataset.zip\n",
        "#!rm -f flickr-image-dataset.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZZ86KooHM0O"
      },
      "source": [
        "# Extract Unique Images(So dataset is only 30K images long)\n",
        "images_dataframe = pd.read_csv(\"./flickr30k_images/results.csv\", error_bad_lines=False, sep = \"|\", index_col = 'image_name')\n",
        "images_dataframe = images_dataframe.iloc[images_dataframe[' comment_number'].values == ' 0']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-YGKYLt4DEP"
      },
      "source": [
        "Helpers for Reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgaONB-q4FK8"
      },
      "source": [
        "import os\n",
        "import random\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euazJsGS4F5O"
      },
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    numpy.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srujoVaYI84q"
      },
      "source": [
        "# Custom Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUMW3lkSIKqW"
      },
      "source": [
        "# HYPER PARAMETERS \n",
        "BASE_IMAGE_PATH = \"./flickr30k_images/flickr30k_images/\"\n",
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 32\n",
        "TRAIN_SIZE = 0.9995"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X61uaT8hI09t"
      },
      "source": [
        "Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zNnYyJIJCk"
      },
      "source": [
        "# Transforms Used\n",
        "TRAIN_TRANSFORMS = transforms.Compose([\n",
        "  transforms.RandomGrayscale(),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.RandomRotation(10),\n",
        "  transforms.ColorJitter(),\n",
        "  transforms.RandomRotation(15),\n",
        "  transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "TEST_TRANSFORMS = transforms.Compose([\n",
        "  transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ParS9jI8NE"
      },
      "source": [
        "class ImageCaptioningDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, transforms, base_path, dataframe, device):\n",
        "    self.dataframe = dataframe\n",
        "    self.transforms = transforms \n",
        "    self.base_path = base_path\n",
        "    self.device = device\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "  def __getitem__(self, idx):\n",
        "    index = self.dataframe.index[idx]\n",
        "    caption = str.strip(self.dataframe.iloc[idx][1])[:-2]\n",
        "    lower_caption = str.lower(caption)\n",
        "    # Load in Image\n",
        "    image_path = self.base_path + index\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = torch.tensor(image, device = self.device).to(torch.float32) / 255.0\n",
        "    image = image.transpose(1, 2).transpose(0, 1)\n",
        "    image = self.transforms(image)\n",
        "    \n",
        "    return image, caption"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST_5c-v6bjpO"
      },
      "source": [
        "Train_dataframe, Val_dataframe = train_test_split(images_dataframe, train_size = TRAIN_SIZE, test_size = 1 - TRAIN_SIZE, random_state = 42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR7iXRQh3JVs"
      },
      "source": [
        "Train_Dataset = ImageCaptioningDataset(TRAIN_TRANSFORMS, BASE_IMAGE_PATH, Train_dataframe, device)\n",
        "Val_Dataset = ImageCaptioningDataset(TEST_TRANSFORMS, BASE_IMAGE_PATH, Val_dataframe, device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0kluKsuXFIP"
      },
      "source": [
        "def custom_collate(vals):\n",
        "  images = []\n",
        "  labels = []\n",
        "  for val in vals:\n",
        "    images += [val[0]]\n",
        "    labels += [val[1]]\n",
        "  return images, labels"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv312FGq4QGw"
      },
      "source": [
        "Train/Val Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF2YqLhy4uKY"
      },
      "source": [
        "TrainDataloader = torch.utils.data.DataLoader(Train_Dataset, batch_size = BATCH_SIZE, shuffle = True, worker_init_fn = seed_worker, collate_fn = custom_collate)\n",
        "ValDataloader = torch.utils.data.DataLoader(Val_Dataset, batch_size = TEST_BATCH_SIZE, collate_fn= custom_collate)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ape6taPgqUKH"
      },
      "source": [
        "# Image Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvlHtlL5uEaD"
      },
      "source": [
        "Custom Resizer: Using a Bilinear Model to scale down images to 224 x 224\n",
        "\n",
        "Using a CNN to resize down images: as proposed recently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW2pgnFBcYnW"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_features, out_features, kernel_size, padding, groups):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups)\n",
        "    self.bn = nn.BatchNorm2d(out_features)\n",
        "    self.act1 = nn.SiLU(inplace = True)\n",
        "  def forward(self, x):\n",
        "    return self.bn(self.act1(self.conv(x)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSmmDDsKT6XJ"
      },
      "source": [
        "class DownSampleConvBlock(nn.Module):\n",
        "  def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_features, out_features, kernel_size, stride, padding = padding, groups = groups)\n",
        "    self.bn = nn.BatchNorm2d(out_features)\n",
        "    self.act1 = nn.SiLU(inplace = True)\n",
        "  def forward(self, x):\n",
        "    return self.bn(self.act1(self.conv(x)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4kYD9rfIpK1"
      },
      "source": [
        "class RegularSE(nn.Module):\n",
        "  '''\n",
        "  Regular Squeeze and Excitation Block \n",
        "  '''\n",
        "  def __init__(self, in_dim, inner_dim):\n",
        "    super().__init__()\n",
        "    self.in_dim = in_dim\n",
        "    self.inner_dim = inner_dim\n",
        "    self.Squeeze = nn.Linear(self.in_dim, self.inner_dim)\n",
        "    self.SiLU = nn.SiLU(inplace = True)\n",
        "    self.Excite = nn.Linear(self.inner_dim, self.in_dim)\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Performs Channel-Wise Attention using Squeeze Excite Blocks\n",
        "    '''\n",
        "    avg_pool = torch.mean(x, dim = -1)\n",
        "    avg_pool = torch.mean(avg_pool, dim = -1) \n",
        "    squeeze = self.SiLU(self.Squeeze(avg_pool))\n",
        "    excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n",
        "    return excite * x \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y1eH7cIEAJW"
      },
      "source": [
        "class ResBottleNeck(nn.Module):\n",
        "  def __init__(self, in_features, inner_features, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.in_features = in_features\n",
        "    self.inner_features = inner_features\n",
        "    \n",
        "    self.squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n",
        "    self.process = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n",
        "    self.SE = RegularSE(self.inner_features, self.inner_features // 4)\n",
        "    self.expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n",
        "    self.gamma = nn.Parameter(torch.zeros(1, device = self.device))\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: Tensor(B, C, H, W)\n",
        "    '''\n",
        "    squeeze = self.squeeze(x)\n",
        "    process= self.process(squeeze)\n",
        "    SE = self.SE(process)\n",
        "    expand = self.expand(SE) \n",
        "    return expand * self.gamma + x\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9GwG-pIydeE"
      },
      "source": [
        "class CNNDownSampler(nn.Module):\n",
        "  '''\n",
        "  Leverages Bilinear interpolation and a CNN to resize and enhance image features\n",
        "  '''\n",
        "  def __init__(self, out_size, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.out_size = out_size\n",
        "    self.initial_processing = nn.Sequential(*[\n",
        "      ConvBlock(3, 16, 7, 3, 1),\n",
        "      ResBottleNeck(16, 4, self.device)\n",
        "    ])\n",
        "    self.num_res = 2\n",
        "    self.middle_processing = nn.Sequential(*[\n",
        "      ResBottleNeck(16, 4, self.device) for i in range(self.num_res) \n",
        "    ])\n",
        "    \n",
        "    self.proj = ConvBlock(16, 3, 1, 0, 1)\n",
        "    self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: Tensor(3, H, W)\n",
        "    H and W can be anything:\n",
        "    output: Tensor(B, 3, out_size, out_size), enhanced resized image features\n",
        "    '''\n",
        "    x = x.unsqueeze(0) # (1, 3, H, W)\n",
        "    # Interpolate to desired size(One pathway)\n",
        "    resized_base = F.interpolate(x, size = (self.out_size, self.out_size), mode= 'bilinear')\n",
        "    # Use Convolutions\n",
        "    initial_processed = self.initial_processing(x) # (B, 16, H, W) \n",
        "    # Interpolate again\n",
        "    resized_processed = F.interpolate(initial_processed, (self.out_size, self.out_size), mode = 'bilinear')\n",
        "    middle_processed = self.middle_processing(resized_processed)\n",
        "    proj = self.proj(middle_processed)\n",
        "    return proj * self.gamma + resized_base"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ris2HvwPasRu"
      },
      "source": [
        "class DownSampler(nn.Module):\n",
        "  def __init__(self, out_size, device):\n",
        "    super().__init__()\n",
        "    self.out_size = out_size\n",
        "    self.device = device\n",
        "    self.downsampler = CNNDownSampler(self.out_size, self.device)\n",
        "  def forward(self, images):\n",
        "    '''\n",
        "    Resizes and processes a batch\n",
        "    '''\n",
        "    resized_images = []\n",
        "    for image in images:\n",
        "      resized_images += [torch.squeeze(self.downsampler(image))]\n",
        "    return torch.stack(resized_images)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYKkIpADbon0"
      },
      "source": [
        "class InvertedResidualBlock(nn.Module):\n",
        "  def __init__(self, in_features, inner_features, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.in_features = in_features\n",
        "    self.inner_features = inner_features\n",
        "    self.squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n",
        "    self.depthwise = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n",
        "    self.SE = RegularSE(self.inner_features, self.in_features // 16)\n",
        "    self.expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n",
        "    self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n",
        "  def forward(self, x):\n",
        "    squeeze = self.squeeze(x)\n",
        "    depthwise = self.depthwise(squeeze)\n",
        "    SE = self.SE(depthwise)\n",
        "    expand = self.expand(SE)\n",
        "    return expand * self.gamma + x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCaG5XZyuNaL"
      },
      "source": [
        "EfficientNetb0 Transfer Learned:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_18pkeqdTKT"
      },
      "source": [
        "class EfficientNet(nn.Module):\n",
        "  '''\n",
        "  Modified EfficientNetb0\n",
        "  '''\n",
        "  def freeze(self, layer):\n",
        "    for parameter in layer.parameters():\n",
        "      parameter.requires_grad = False\n",
        "  def unfreeze(self, layer):\n",
        "    for parameter in layer.parameters():\n",
        "      parameter.requires_grad = True\n",
        "  def __init__(self, out_features, device):\n",
        "    super().__init__()\n",
        "    self.out_features = out_features\n",
        "    self.device = device\n",
        "    # Download Pretrained model and Weights\n",
        "    self.model = timm.create_model(\"efficientnet_b0\", pretrained = True)\n",
        "    # Extract Layers\n",
        "    self.conv1 = self.model.conv_stem\n",
        "    self.bn1 = self.model.bn1\n",
        "    self.act1 = self.model.act1\n",
        "    # Pretrained Blocks\n",
        "    self.block0 = self.model.blocks[0]\n",
        "    self.block1 = self.model.blocks[1]\n",
        "    self.block2 = self.model.blocks[2] # (B, 40, 32, 32)\n",
        "    self.block3 = self.model.blocks[3]\n",
        "    self.block4 = self.model.blocks[4] # (B, 112, 16, 16)\n",
        "    self.block5 = self.model.blocks[5]\n",
        "    self.block6 = self.model.blocks[6] # (B, 320, 8, 8)\n",
        "    # Freeze a Few Initial Layers\n",
        "    self.freeze(self.block0)\n",
        "    self.freeze(self.block1)\n",
        "    self.freeze(self.bn1)\n",
        "    self.freeze(self.conv1)\n",
        "    # Custom Layers\n",
        "    self.Attention1 = RegularSE(40, 8)\n",
        "    self.Attention2 = RegularSE(112, 32)\n",
        "    self.Attention3 = RegularSE(320, 64)\n",
        "\n",
        "    self.features_extract =nn.Identity() #nn.Sequential(*\n",
        "    #  [DownSampleConvBlock(320, 320, 5, 2, 320, 2), ConvBlock(320, 512, 1, 0, 1)] + \n",
        "    #  [InvertedResidualBlock(512, 1024, self.device) for i in range(5)])\n",
        "    self.proj = ConvBlock(320, self.out_features, 1, 0, 1) # Not Enough for Compute\n",
        "  def forward(self, x):\n",
        "    B, _, _, _ = x.shape\n",
        "    conv1 = self.bn1(self.act1(self.conv1(x))) # (B, 32, 128, 128)\n",
        "    \n",
        "    block0 = self.block0(conv1)\n",
        "    block1 = self.block1(block0)\n",
        "    block2 = self.block2(block1)\n",
        "    attention1 = self.Attention1(block2)\n",
        "    block3 = self.block3(attention1)\n",
        "    block4 = self.block4(block3)\n",
        "    attention2 = self.Attention2(block4)\n",
        "    block5 = self.block5(attention2)\n",
        "    block6 = self.block6(block5)\n",
        "    attention3 = self.Attention3(block6)\n",
        "    \n",
        "    extract_features = self.proj(self.features_extract(attention3)) # (B, 768, 4, 4)\n",
        "    return extract_features.view(B, self.out_features, -1).transpose(1, 2) # (B, 16, 768)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V5W_mqzuYdU"
      },
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "  def __init__(self, in_dim, out_size, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.in_dim = in_dim\n",
        "    self.out_size = out_size\n",
        "    self.downsampler = DownSampler(self.out_size, self.device)\n",
        "    self.encoder = EfficientNet(self.in_dim, self.device)\n",
        "  def forward(self, images):\n",
        "    downsized_images = self.downsampler(images) # (B, 3, out_size, out_size)\n",
        "    encoded = self.encoder(downsized_images)\n",
        "    return encoded"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d9Cbb-ccfoU"
      },
      "source": [
        "# Custom Transformer(QANET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UiWNvlnZJ3r"
      },
      "source": [
        "class EncoderDecoderAttention(nn.Module):\n",
        "  def __init__(self, in_features, inner_features, num_heads):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.inner_features = inner_features\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.K = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n",
        "    self.V = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n",
        "    self.Q = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n",
        "    self.Linear = nn.Linear(self.inner_features * self.num_heads, self.in_features)\n",
        "  def forward(self, encoder, decoder):\n",
        "    '''\n",
        "    Encoder: Tensor(B, L_enc, C)\n",
        "    Decoder: Tensor(B, L_dec, C)\n",
        "    '''    \n",
        "    B, L_enc, _ = encoder.shape\n",
        "    B, L_dec, _ = decoder.shape \n",
        "    Keys = self.K(encoder)\n",
        "    Values = self.V(encoder) # (B, L_enc, HI)\n",
        "    Queries = self.Q(decoder) # (B, L_dec, HI)\n",
        "\n",
        "    Keys = Keys.reshape(B, L_enc, self.num_heads, self.inner_features)\n",
        "    Values = Values.reshape(B, L_enc, self.num_heads, self.inner_features) \n",
        "    Queries = Queries.reshape(B, L_dec, self.num_heads, self.inner_features)\n",
        "\n",
        "    Keys = Keys.transpose(1, 2)\n",
        "    Values = Values.transpose(1, 2)\n",
        "    Queries = Queries.transpose(1, 2)\n",
        "\n",
        "    Keys = Keys.reshape(B * self.num_heads, L_enc, self.inner_features)\n",
        "    Values = Values.reshape(B * self.num_heads, L_enc, self.inner_features)\n",
        "    Queries = Queries.reshape(B * self.num_heads, L_dec, self.inner_features)\n",
        "\n",
        "    att_mat = F.softmax(torch.bmm(Keys, Queries.transpose(1, 2)) / math.sqrt(self.inner_features)) # (B, L_enc, L_dec)\n",
        "    att_scores = torch.bmm(Values.transpose(1, 2), att_mat) # (B, L_dec, I)\n",
        "    att_scores = att_scores.reshape(B, self.num_heads, L_dec, self.inner_features)\n",
        "    att_scores = att_scores.transpose(1, 2)\n",
        "    att_scores = att_scores.reshape(B, L_dec, -1)\n",
        "    return self.Linear(att_scores)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq8zKI92nN_c"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''\n",
        "  Computes MultiHeadAttention\n",
        "  '''\n",
        "  def __init__(self, in_features, inner_features, num_heads):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.inner_features = inner_features\n",
        "    self.num_heads = num_heads\n",
        "    \n",
        "    self.K = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n",
        "    self.V = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n",
        "    self.Q = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n",
        "    self.Linear = nn.Linear(self.inner_features * self.num_heads, self.in_features)\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: Tensor(B, L, C)\n",
        "    '''\n",
        "    B, L, _ = x.shape\n",
        "    Keys = self.K(x) # (B, L, HI)\n",
        "    Values = self.V(x)\n",
        "    Queries = self.Q(x) # (B, L, HI)\n",
        "\n",
        "    Keys = Keys.reshape(B, L, self.num_heads, self.inner_features)\n",
        "    Values = Values.reshape(B, L, self.num_heads, self.inner_features)\n",
        "    Queries = Queries.reshape(B, L, self.num_heads, self.inner_features)\n",
        "\n",
        "    Keys = Keys.transpose(1, 2) \n",
        "    Values = Values.transpose(1, 2)\n",
        "    Queries = Queries.transpose(1, 2)\n",
        "\n",
        "    Keys = Keys.reshape(B * self.num_heads, L, self.inner_features)\n",
        "    Values = Values.reshape(B * self.num_heads, L, self.inner_features)\n",
        "    Queries = Queries.reshape(B * self.num_heads, L, self.inner_features)\n",
        "\n",
        "    att_mat = F.softmax(torch.bmm(Keys, Values.transpose(1, 2)) / math.sqrt(self.inner_features))\n",
        "    att_scores = torch.bmm(att_mat, Queries) # (BH, L, I)\n",
        "\n",
        "    att_scores = att_scores.reshape(B, self.num_heads, L, self.inner_features)\n",
        "    att_scores = att_scores.transpose(1, 2)\n",
        "    att_scores = att_scores.reshape(B, L, -1)\n",
        "    return self.Linear(att_scores)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PehTI7afn2_U"
      },
      "source": [
        "class QAEncoder(nn.Module):\n",
        "  '''\n",
        "  One QA Net Encoder Module\n",
        "  '''\n",
        "  def __init__(self, in_dim, inner_dim, num_heads, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.in_dim = in_dim\n",
        "    self.inner_dim = inner_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.MAH = MultiHeadAttention(self.in_dim, self.inner_dim, self.num_heads)\n",
        "    self.MAHNorm = nn.GroupNorm(1, self.in_dim) # Equivalent to LayerNorm\n",
        "\n",
        "    self.Linear = nn.Linear(self.in_dim, self.in_dim)\n",
        "    self.LinearNorm = nn.GroupNorm(1, self.in_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: Tensor(B, L, C)\n",
        "    '''\n",
        "    pos_enc = self.add_pos_enc(x)\n",
        "\n",
        "    attended = self.MAH(self.MAHNorm(pos_enc.transpose(1, 2)).transpose(1, 2)) + pos_enc\n",
        "\n",
        "    linear = self.Linear(self.LinearNorm(attended.transpose(1, 2)).transpose(1, 2)) + attended\n",
        "    return linear\n",
        "  def add_pos_enc(self, x):\n",
        "    '''\n",
        "    Inserts positional encodings into the features(sinusoidal functions)\n",
        "    x: Tensor(B, L, C)\n",
        "    '''\n",
        "    B, L, C = x.shape\n",
        "    pos_enc = torch.zeros((L, C), device = self.device) \n",
        "    for pos in range(L):\n",
        "      for i in range(0, C, 2):\n",
        "        pos_enc[pos,i] = math.sin(pos / 10000 ** (2 * i / self.in_dim))\n",
        "        pos_enc[pos, i + 1] = math.cos(pos / 10000 ** (2 * (i + 1) / self.in_dim))\n",
        "    batched_pos= []\n",
        "    for b in range(B):\n",
        "      batched_pos += [pos_enc]\n",
        "    batched_pos = torch.stack(batched_pos)\n",
        "    return batched_pos + x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iGiLeRVubOL"
      },
      "source": [
        "class QADecoder(nn.Module):\n",
        "  def __init__(self, in_features, inner_features, num_heads, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.in_features = in_features\n",
        "    self.inner_features = inner_features\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.MHA = MultiHeadAttention(self.in_features, self.inner_features, self.num_heads)\n",
        "    self.MHANorm = nn.GroupNorm(1, self.in_features)\n",
        "\n",
        "    self.DecoderAtt = EncoderDecoderAttention(self.in_features, self.inner_features, self.num_heads)\n",
        "    self.DecoderNorm = nn.GroupNorm(1, self.in_features)\n",
        "    \n",
        "    self.Linear = nn.Linear(self.in_features, self.in_features)\n",
        "    self.LinearNorm = nn.GroupNorm(1, self.in_features)\n",
        "  def forward(self, decoder, encoder):\n",
        "    decoder = self.add_positional_encodings(decoder)\n",
        "    attended_decoder = self.MHA(self.MHANorm(decoder.transpose(1, 2)).transpose(1, 2)) + decoder\n",
        "    attended = self.DecoderAtt(encoder, self.DecoderNorm(attended_decoder.transpose(1, 2)).transpose(1, 2)) + attended_decoder\n",
        "    linear = self.Linear(self.LinearNorm(attended.transpose(1, 2)).transpose(1, 2)) + attended\n",
        "    return linear\n",
        "  def add_positional_encodings(self, decoder):\n",
        "    '''\n",
        "    decoder: Tensor(B, L, C)\n",
        "    '''\n",
        "    B, L, C = decoder.shape\n",
        "    position_enc = torch.zeros((L, C), device = self.device)\n",
        "    for pos in range(L):\n",
        "      for i in range(0, C, 2):\n",
        "        position_enc[pos, i] = math.sin(pos / 10000 ** (2 * i / self.inner_features))\n",
        "        position_enc[pos, i + 1] = math.cos(pos / 10000 ** (2 * (i + 1) / self.inner_features))\n",
        "    batched = []\n",
        "    for b in range(B):\n",
        "      batched += [position_enc]\n",
        "    return torch.stack(batched) + decoder"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q_gjaCM1ZjI"
      },
      "source": [
        "Model Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZL2hNP1Y5A"
      },
      "source": [
        "class Tokenizer(nn.Module):\n",
        "  def __init__(self, model_name):\n",
        "    super().__init__()\n",
        "    self.model_name = model_name\n",
        "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
        "    self.num_words = self.tokenizer.vocab_size\n",
        "    self.START = self.tokenizer.cls_token\n",
        "    self.END = self.tokenizer.sep_token\n",
        "    self.START_ID = self.tokenizer.cls_token_id\n",
        "    self.END_ID = self.tokenizer.sep_token_id \n",
        "    self.max_length = 40\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Tokenizes sentences using HuggingFace \n",
        "    '''\n",
        "    tokenized_sentences = self.tokenizer(x, add_special_tokens = True, padding = 'max_length', truncation = True, max_length = self.max_length, return_tensors = 'pt')\n",
        "    return tokenized_sentences"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5ZOMqNduVPC"
      },
      "source": [
        "class QANet(nn.Module):\n",
        "  def __init__(self, in_dim, device):\n",
        "    super().__init__()\n",
        "    # Tokenizer \n",
        "    self.in_dim = in_dim\n",
        "    self.model_name = 'distilbert-base-uncased'\n",
        "    self.tokenizer = Tokenizer(self.model_name)\n",
        "    self.num_classes = self.tokenizer.tokenizer.vocab_size\n",
        "    self.device = device\n",
        "    # HYPER PARAMETERS\n",
        "    \n",
        "    self.inner_dim = self.in_dim // 4\n",
        "    self.num_att_heads = 2\n",
        "\n",
        "    self.num_im_encoder = 1\n",
        "    self.num_encoder = 2\n",
        "    self.num_decoder = 3\n",
        "    # Embeddings\n",
        "    self.Embeddings = nn.Embedding(self.num_classes, self.in_dim)\n",
        "    # Encoder\n",
        "    self.image_encoder = nn.Sequential(*[\n",
        "      QAEncoder(self.in_dim, self.inner_dim, self.num_att_heads, self.device) for i in range(self.num_im_encoder)\n",
        "    ])\n",
        "    self.encoder = nn.Sequential(*[\n",
        "      QAEncoder(self.in_dim, self.inner_dim, self.num_att_heads, self.device) for i in range(self.num_encoder)\n",
        "    ])\n",
        "    # Decoder\n",
        "    self.decoder = nn.ModuleList([\n",
        "      QADecoder(self.in_dim, self.inner_dim, self.num_att_heads, self.device) for i in range(self.num_decoder)\n",
        "    ])\n",
        "    # Dense Decoder\n",
        "    self.linear = nn.Linear(self.in_dim, self.num_classes)\n",
        "\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "  def forward_train(self, x, GT):\n",
        "    '''\n",
        "    x: Tensor(B, L, C), the encoded image features\n",
        "    GT: List of Sentences\n",
        "    Returns Loss\n",
        "    '''\n",
        "    tokenized_GT = self.tokenizer(GT) # (B, L)\n",
        "    GT_input_ids = tokenized_GT['input_ids'].to(self.device)\n",
        "    GT_attention_mask = tokenized_GT['attention_mask'].to(self.device)\n",
        "    B, L = GT_input_ids.shape\n",
        "    encoded_image = self.image_encoder(x) # (B, L, C)\n",
        "    # Begin Teacher forcing.\n",
        "    total_loss = torch.zeros((1), device = self.device)\n",
        "\n",
        "    for i in range(1, L):\n",
        "      input_IDs = GT_input_ids[:, :i]\n",
        "      if len(input_IDs.shape) == 1:\n",
        "        input_IDs = input_IDs.unsqueeze(-1)\n",
        "      # Encode using Embeddings\n",
        "      encodings = self.Embeddings(input_IDs)\n",
        "      # Process with Custom Encoder Layers\n",
        "      encodings = self.encoder(encodings)\n",
        "      # Encode using custom QA Encoders \n",
        "      for decoder in self.decoder:\n",
        "        encodings = decoder(encodings, encoded_image) # (B, L, 768)\n",
        "      # Average Logits\n",
        "      logits = torch.mean(encodings, dim = 1) # (B, 768)\n",
        "      predicted = self.linear(logits) # (B, C)\n",
        "      GT_id = GT_input_ids[:, i] # GT Targets: (B)\n",
        "\n",
        "      # Compute Loss between predicted\n",
        "      loss = self.criterion(predicted, GT_id)\n",
        "      total_loss = total_loss + loss\n",
        "    return total_loss / L\n",
        "\n",
        "  def forward(self, x, max_length = 40):\n",
        "    '''\n",
        "    x: Tensor(B, L, C), the encoded image features\n",
        "    '''\n",
        "    # Encode Image Features \n",
        "    encoded_images = self.image_encoder(x) # (B, L, C)\n",
        "    B, _, _ = encoded_images.shape\n",
        "    # Create Starter Tokens \n",
        "    sentences = [self.tokenizer.START for i in range(B)]\n",
        "    # Finished Array\n",
        "    finished = [False for i in range(B)]\n",
        "    for i in tqdm.tqdm(range(max_length)):\n",
        "      tokenized = self.tokenizer(sentences)\n",
        "      # encode using hugging face\n",
        "      input_ids = tokenized['input_ids'].to(self.device)\n",
        "      encoded = self.Embeddings(input_ids)\n",
        "      # Encode using custom encoders\n",
        "      encoded = self.encoder(encoded)\n",
        "      # Decode using QADecoders\n",
        "      for decoder in self.decoder:\n",
        "        encoded = decoder(encoded, encoded_images) \n",
        "      # Average the Logits\n",
        "      logits = torch.mean(encoded, dim = 1) # (B, C)\n",
        "      # Linear Layers\n",
        "      scores = F.softmax(self.linear(logits), dim = -1) # (B, C)\n",
        "      # Argmax over scores\n",
        "      _, indices = torch.max(scores, dim = -1) # (B) \n",
        "      for b in range(B):\n",
        "        if finished[b]:\n",
        "          continue # Already Completed Sentences\n",
        "        if indices[b].item() == self.tokenizer.END_ID:\n",
        "          # Sentence Terminated\n",
        "          finished[b] = True\n",
        "          sentences[b] += f\" {self.tokenizer.tokenizer.decode(indices[b].item())}\"\n",
        "        else:\n",
        "          sentences[b] +=f\" {self.tokenizer.tokenizer.decode(indices[b].item())}\"\n",
        "    return sentences"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5ihVK4MA1CF"
      },
      "source": [
        "class FullModel(nn.Module):\n",
        "  '''\n",
        "  Houses the Full Image Captioning Model, powerful, but slow. Faster Implementation using HuggingFace Transformers is below.\n",
        "  '''\n",
        "  def __init__(self, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.in_dim = 256\n",
        "    self.out_size = 256 \n",
        "    self.image_encoder = ImageEncoder(self.in_dim, self.out_size, self.device)\n",
        "    self.QANet = QANet(self.in_dim, self.device)\n",
        "  def forward_train(self, images, captioning):\n",
        "    image_encoded = self.image_encoder(images)\n",
        "    return self.QANet.forward_train(image_encoded, captioning)\n",
        "  def forward(self, images):\n",
        "    image_encoded = self.image_encoder(images)\n",
        "    return self.QANet(image_encoded)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnrHalJWyLv_"
      },
      "source": [
        "# Custom Training Loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsfkEklBuWlP"
      },
      "source": [
        "class ImageCaptioningSolver(nn.Module):\n",
        "  def __init__(self, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.model = FullModel(self.device)\n",
        "    self.optim = optim.Adam(self.model.parameters(), lr= 5e-4, weight_decay = 1e-4)\n",
        "    self.decay = optim.lr_scheduler.StepLR(self.optim, 5, 0.9)\n",
        "    self.decay2 = optim.lr_scheduler.CosineAnnealingLR(self.optim, 5, eta_min = 1e-7)\n",
        "  def forward(self, x):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      return self.model(x)\n",
        "  def evaluate(self, valloader):\n",
        "    '''\n",
        "    Evaluates the Model's BLEU score.\n",
        "    '''\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      total_bleu = 0\n",
        "      count = 0\n",
        "      for images, labels in tqdm.tqdm(valloader):\n",
        "        predicted = self.model(images)\n",
        "        bleu = nltk.translate.bleu_score.corpus_bleu(labels, predicted)\n",
        "        total_bleu += bleu\n",
        "        count += 1\n",
        "        del images\n",
        "        del labels\n",
        "        del predicted\n",
        "        torch.cuda.empty_cache()\n",
        "    return total_bleu / count\n",
        "  def training_loop(self, trainloader, valloader, NUM_EPOCHS, display_every = 64):\n",
        "    liveloss = livelossplot.PlotLosses()\n",
        "    best_val_bleu  = 0\n",
        "    best_val_loss = 999\n",
        "    for EPOCH in range(NUM_EPOCHS):\n",
        "      self.train()\n",
        "      logs = {}\n",
        "      total_loss = 0\n",
        "      count = 0\n",
        "  \n",
        "      for images, labels in trainloader:\n",
        "        self.optim.zero_grad()\n",
        "        loss = self.model.forward_train(images, labels)\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "        print(f\"Step: {count}, loss: {loss.item()}\")\n",
        "        total_loss += loss.item()\n",
        "        count += 1\n",
        "        del images\n",
        "        del labels\n",
        "        torch.cuda.empty_cache()\n",
        "        if count == display_every:\n",
        "          break\n",
        "      logs['loss'] = total_loss / count\n",
        "      print(f\"EPOCH: {EPOCH}, total_loss: {logs['loss']}\")\n",
        "      self.decay.step()\n",
        "      self.decay2.step()\n",
        "    \n",
        "      self.eval()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        logs['val_loss'] = 0\n",
        "        #logs['accuracy'] = 0\n",
        "        count = 0\n",
        "        for images, labels in valloader:\n",
        "          loss = self.model.forward_train(images, labels)\n",
        "          #pred_sentences = self.model(images)\n",
        "          # Compute BLEU between GT and Predicted sentences\n",
        "          #bleu = nltk.translate.bleu_score.corpus_bleu(labels, pred_sentences)\n",
        "          logs['val_loss'] += loss.item()\n",
        "          #logs['accuracy'] += bleu\n",
        "          count += 1\n",
        "          del images\n",
        "          del labels\n",
        "          torch.cuda.empty_cache()\n",
        "      \n",
        "      liveloss.update(logs)\n",
        "      liveloss.send()\n",
        "      '''\n",
        "      if logs['val_loss'] < best_val_loss:\n",
        "        best_val_loss = logs['val_loss']\n",
        "        torch.save(self.model.state_dict(), \"./BestVal.pth\")\n",
        "      if logs['accuracy'] > best_val_bleu:\n",
        "        best_val_bleu = logs['accuracy']\n",
        "        torch.save(self.model.state_dict(), \"./BestAcc.pth\")\n",
        "      print(f\"EPOCH: {EPOCH}, train_loss: {logs['loss']}, val_loss: {logs['val_loss']}, val_accuracy: {logs['accuracy']}\")\n",
        "      '''"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yzM1bXRyohy"
      },
      "source": [
        "%%capture \n",
        "solver = ImageCaptioningSolver(device)\n",
        "solver.to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf7pT4FGAn2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "outputId": "925cebd0-50da-4103-f7ec-6e3e6ad605bf"
      },
      "source": [
        "solver.training_loop(TrainDataloader, ValDataloader, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss\n",
            "\ttraining         \t (min:    2.295, max:    3.749, cur:    2.304)\n",
            "\tvalidation       \t (min:    2.876, max:    3.241, cur:    2.881)\n",
            "Step: 0, loss: 2.413353681564331\n",
            "Step: 1, loss: 2.4330203533172607\n",
            "Step: 2, loss: 2.351206064224243\n",
            "Step: 3, loss: 2.3386082649230957\n",
            "Step: 4, loss: 2.4527840614318848\n",
            "Step: 5, loss: 1.8198908567428589\n",
            "Step: 6, loss: 2.1196682453155518\n",
            "Step: 7, loss: 2.1131539344787598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rZGycubCctM"
      },
      "source": [
        "from google.colab import files\n",
        "torch.save(solver.state_dict(), \"./model.pth\")\n",
        "files.download(\"./model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}