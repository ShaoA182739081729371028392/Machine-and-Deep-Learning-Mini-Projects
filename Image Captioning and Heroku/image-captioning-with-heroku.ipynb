{"cells":[{"metadata":{"id":"edQ67ZqB5KXl"},"cell_type":"markdown","source":"# Model Structure: Transfer Learned ResNet200D + DistilBert."},{"metadata":{"id":"GeuoPKvD5Tzt"},"cell_type":"markdown","source":"# Import Dependencies"},{"metadata":{"id":"3syn9XO4c73y","trusted":true,"scrolled":true},"cell_type":"code","source":"%%capture\nimport PIL\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport tqdm.notebook as tqdm\nfrom sklearn.model_selection import ShuffleSplit\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport random\nimport copy\nimport cv2\nimport math\n\nimport numpy as np\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n!pip install livelossplot\nimport livelossplot\n\n!pip install timm\nimport timm\n\n!pip install transformers\nimport transformers\n\nimport nltk \nnltk.download('punkt')\nimport re\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"id":"tKVGScmXC_YU"},"cell_type":"markdown","source":"Load in the Dataset(FlicKR)"},{"metadata":{"trusted":true},"cell_type":"code","source":"images_dataframe = pd.read_csv('../input/flickr8k/captions.txt')\nimages_dataframe = images_dataframe.drop_duplicates(subset = 'image')\nimages_dataframe = images_dataframe.set_index(\"image\")","execution_count":null,"outputs":[]},{"metadata":{"id":"L-YGKYLt4DEP"},"cell_type":"markdown","source":"Helpers for Reproducibility:"},{"metadata":{"id":"lgaONB-q4FK8","trusted":true,"scrolled":true},"cell_type":"code","source":"import os\nimport random\nseed = 42\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"id":"euazJsGS4F5O","trusted":true,"scrolled":true},"cell_type":"code","source":"def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Append COCO Captions to Train Dataset(8k is too little, 30k full of duplicates)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!wget http://images.cocodataset.org/zips/val2014.zip\n!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n!unzip val2014.zip\n!unzip annotations_trainval2014.zip\n!rm -f val2014.zip\n!rm -f annotations_trainval2014.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Captions_Path = './annotations/captions_val2014.json'\nCOCO_Path = \"./val2014/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_coco():\n    '''\n    Converts the COCO dataset to the Flickr8k Format\n    '''\n    dataset = json.load(open(Captions_Path))\n    images = dataset['images']\n    labels = dataset['annotations']\n    ID2Image = {}\n    for i in images:\n        ID2Image[i['id']] = i['file_name']\n    ID2Label = {}\n    for i in labels:\n        ID2Label[i['image_id']] = i['caption']\n    Image2Label = {}\n    for key in ID2Image:\n        Image2Label[ID2Image[key]] = ID2Label[key]\n    return Image2Label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def construct_dataframe(Image2Label):\n    images = []\n    labels = []\n    for image in Image2Label:\n        images += [image]\n        labels += [Image2Label[image]]\n    dictionary = {'image': images, 'caption': labels}\n    dataframe = pd.DataFrame(dictionary)\n    dataframe = dataframe.set_index('image')\n    return dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image2Label = process_coco()\nCOCO_dataframe = construct_dataframe(Image2Label)\nimages_dataframe = images_dataframe.append(COCO_dataframe)","execution_count":null,"outputs":[]},{"metadata":{"id":"srujoVaYI84q"},"cell_type":"markdown","source":"# Custom Dataloader"},{"metadata":{"id":"eUMW3lkSIKqW","trusted":true,"scrolled":true},"cell_type":"code","source":"# HYPER PARAMETERS \nBASE_IMAGE_PATH = \"../input/flickr8k/Images/\"\nGLOVE_PATH = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\nTRAIN_BATCH_SIZE = 128\nTEST_BATCH_SIZE = 128\nTRAIN_SIZE = 0.9995\nIMAGE_SIZE = 256","execution_count":null,"outputs":[]},{"metadata":{"id":"X61uaT8hI09t"},"cell_type":"markdown","source":"Transforms"},{"metadata":{"id":"g2zNnYyJIJCk","trusted":true,"scrolled":true},"cell_type":"code","source":"train_transforms = A.Compose([\n    A.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, scale=(0.6, 0.6), p=1),\n    A.Flip(p = 0.7),\n    A.OneOf([\n        A.MotionBlur(blur_limit=(3, 5)),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=(3, 5)),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n        A.MultiplicativeNoise(),\n    ], p=0.7),\n    A.OneOf([\n        A.OpticalDistortion(distort_limit=1.0),\n        A.GridDistortion(num_steps=5, distort_limit=1.),\n        A.ElasticTransform(alpha=3),\n    #    A.GridDropout()\n    ], p=0.7),\n    A.CLAHE(clip_limit=4.0, p=0.7),\n    #A.IAAPiecewiseAffine(p=0.5),\n    A.IAASharpen(p=0.5),\n    #A.RandomGamma(gamma_limit=(70, 130), p=0.3),\n    A.ColorJitter(p = 0.7),\n    A.OneOf([\n        A.ImageCompression(),\n        A.Downscale(scale_min=0.7, scale_max=0.95),\n        #A.RandomGridShuffle(),\n    ], p=0.2),\n    A.OneOf([\n        A.ToGray(),\n        A.ToSepia()\n    ]),\n    #A.OneOf([\n    #    A.RandomRain(),\n    #    A.RandomFog(),\n    #    A.RandomShadow(),\n    #    A.RandomSnow(),\n    #    A.RandomSunFlare()\n    #]),\n    A.CoarseDropout(max_holes=8, max_height=int(IMAGE_SIZE * 0.1),\n                       max_width=int(IMAGE_SIZE* 0.1), p=0.5),\n    A.Cutout(num_holes = 32),\n    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=45, border_mode=0, p=0.85),\n    A.Normalize(mean=0.482288, std=0.22085),\n    ToTensorV2()\n])\n\ntest_transforms = A.Compose([\n    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n    A.Normalize(),\n    ToTensorV2()\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_vocab(dataframe):\n    '''\n    Loads in the Vocabulary for Flickr8K\n    '''\n    captions = [nltk.word_tokenize(str.lower(caption)) for caption in dataframe.caption.values]\n    captions_word = [word for sentence in captions for word in sentence]\n    return sorted(list(set(captions_word)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = process_vocab(images_dataframe)","execution_count":null,"outputs":[]},{"metadata":{"id":"s2ParS9jI8NE","trusted":true,"scrolled":true},"cell_type":"code","source":"class ImageCaptioningDataset(torch.utils.data.Dataset):\n  def __init__(self, transforms, dataframe, device):\n    self.dataframe = dataframe\n    self.transforms = transforms\n    self.device = device\n    self.COCO_Base = COCO_Path\n    self.Flickr_base = BASE_IMAGE_PATH\n  def __len__(self):\n    return len(self.dataframe)\n  def __getitem__(self, idx):\n    index = str.strip(self.dataframe.index[idx])\n    caption = str.strip(self.dataframe.iloc[idx]['caption'])\n    caption = str.lower(caption)\n    caption = re.sub(r'[^\\w\\s]','',caption)\n    lower_caption = str.lower(caption)\n    # Load in Image\n    if index[:4] == 'COCO':\n        image_path = self.COCO_Base + index\n    else:\n        image_path = self.Flickr_base + index\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = self.transforms(image = image)['image']\n    image = image.to(self.device)\n    \n    return image, caption","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splitter = ShuffleSplit(1, test_size = 1 - TRAIN_SIZE, train_size = TRAIN_SIZE, random_state = 42)\nfor train_idx, test_idx in splitter.split(images_dataframe):\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataframe = images_dataframe.iloc[train_idx]\nval_dataframe = images_dataframe.iloc[test_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataset = ImageCaptioningDataset(train_transforms,  train_dataframe, device)\nValDataset = ImageCaptioningDataset(test_transforms, val_dataframe, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataloader = torch.utils.data.DataLoader(TrainDataset, batch_size = TRAIN_BATCH_SIZE, shuffle = True, worker_init_fn = seed_worker)\nValDataloader = torch.utils.data.DataLoader(ValDataset, batch_size = TEST_BATCH_SIZE, shuffle = False, worker_init_fn = seed_worker)","execution_count":null,"outputs":[]},{"metadata":{"id":"ape6taPgqUKH"},"cell_type":"markdown","source":"# Image Encoding"},{"metadata":{"id":"OW2pgnFBcYnW","trusted":true,"scrolled":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n  def __init__(self, in_features, out_features, kernel_size, padding, groups):\n    super().__init__()\n    self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups)\n    self.bn = nn.BatchNorm2d(out_features)\n    self.act1 = nn.SiLU(inplace = True)\n  def forward(self, x):\n    return self.bn(self.act1(self.conv(x)))","execution_count":null,"outputs":[]},{"metadata":{"id":"eSmmDDsKT6XJ","trusted":true,"scrolled":true},"cell_type":"code","source":"class DownSampleConvBlock(nn.Module):\n  def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n    super().__init__()\n    self.conv = nn.Conv2d(in_features, out_features, kernel_size, stride, padding = padding, groups = groups)\n    self.bn = nn.BatchNorm2d(out_features)\n    self.act1 = nn.SiLU(inplace = True)\n  def forward(self, x):\n    return self.bn(self.act1(self.conv(x)))","execution_count":null,"outputs":[]},{"metadata":{"id":"u4kYD9rfIpK1","trusted":true,"scrolled":true},"cell_type":"code","source":"class RegularSE(nn.Module):\n  '''\n  Regular Squeeze and Excitation Block \n  '''\n  def __init__(self, in_dim, inner_dim):\n    super().__init__()\n    self.in_dim = in_dim\n    self.inner_dim = inner_dim\n    self.Squeeze = nn.Linear(self.in_dim, self.inner_dim)\n    self.SiLU = nn.SiLU(inplace = True)\n    self.Excite = nn.Linear(self.inner_dim, self.in_dim)\n  def forward(self, x):\n    '''\n    Performs Channel-Wise Attention using Squeeze Excite Blocks\n    '''\n    avg_pool = torch.mean(x, dim = -1)\n    avg_pool = torch.mean(avg_pool, dim = -1) \n    squeeze = self.SiLU(self.Squeeze(avg_pool))\n    excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n    return excite * x \n","execution_count":null,"outputs":[]},{"metadata":{"id":"0Y1eH7cIEAJW","trusted":true,"scrolled":true},"cell_type":"code","source":"class ResBottleNeck(nn.Module):\n  def __init__(self, in_features, inner_features, device):\n    super().__init__()\n    self.device = device\n    self.in_features = in_features\n    self.inner_features = inner_features\n    \n    self.squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n    self.process = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n    self.SE = RegularSE(self.inner_features, self.inner_features // 4)\n    self.expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n    self.gamma = nn.Parameter(torch.zeros(1, device = self.device))\n  def forward(self, x):\n    '''\n    x: Tensor(B, C, H, W)\n    '''\n    squeeze = self.squeeze(x)\n    process= self.process(squeeze)\n    SE = self.SE(process)\n    expand = self.expand(SE) \n    return expand * self.gamma + x\n","execution_count":null,"outputs":[]},{"metadata":{"id":"FYKkIpADbon0","trusted":true,"scrolled":true},"cell_type":"code","source":"class InvertedResidualBlock(nn.Module):\n  def __init__(self, in_features, inner_features, device):\n    super().__init__()\n    self.device = device\n    self.in_features = in_features\n    self.inner_features = inner_features\n    self.squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1)\n    self.depthwise = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features)\n    self.SE = RegularSE(self.inner_features, self.in_features // 16)\n    self.expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1)\n    self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n  def forward(self, x):\n    squeeze = self.squeeze(x)\n    depthwise = self.depthwise(squeeze)\n    SE = self.SE(depthwise)\n    expand = self.expand(SE)\n    return expand * self.gamma + x","execution_count":null,"outputs":[]},{"metadata":{"id":"rCaG5XZyuNaL"},"cell_type":"markdown","source":"ResNet200D Transfer Learned"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNetBase(nn.Module):\n    '''\n    Tiny ResNet Pretrained as a baseline\n    '''\n    def freeze(self, layer):\n        for parameter in layer.parameters():\n            parameter.requires_grad = False\n    def __init__(self, in_dim, device):\n        super().__init__()\n        self.in_dim = in_dim\n        self.device = device\n        self.model_name = 'resnet200d'\n        self.model = timm.create_model(self.model_name, pretrained = True)\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.freeze(self.model)\n        self.proj = ConvBlock(2048, self.in_dim, 1, 0, 1)\n    def forward(self, x):\n        features = self.proj(self.model(x))\n        B, C, H, W = features.shape\n        return features.view(B, C, H * W).transpose(1, 2)","execution_count":null,"outputs":[]},{"metadata":{"id":"5q_gjaCM1ZjI"},"cell_type":"markdown","source":"Model Blocks"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomTokens(nn.Module):\n    def __init__(self, vocabulary, glove_path, max_length, device):\n        super().__init__()\n        self.device = device\n        self.glove_path = glove_path\n        self.vocab = vocabulary\n        self.idx2word = {idx + 4: self.vocab[idx] for idx in range(len(self.vocab))}\n        self.START = \"<START>\"\n        self.END = \"<END>\"\n        self.PAD = \"<PAD>\"\n        self.UNK = \"<UNK>\"\n        self.START_ID = 0\n        self.END_ID = 1\n        self.PAD_ID = 2\n        self.UNK_ID = 3\n    \n        \n        self.idx2word[self.START_ID]=self.START\n        self.idx2word[self.END_ID] = self.END\n        self.idx2word[self.PAD_ID] = self.PAD\n        self.idx2word[self.UNK_ID] = self.UNK\n        \n        self.word2idx = {self.vocab[idx]: idx + 4 for idx in range(len(self.vocab))}\n        self.word2idx[self.START] = self.START_ID\n        self.word2idx[self.END] = self.END_ID\n        self.word2idx[self.PAD] = self.PAD_ID\n        self.word2idx[self.UNK] = self.UNK_ID\n        self.dim = 200\n        self.max_length = max_length\n        if self.glove_path:\n            self.embeddings = self.load_glove(self.glove_path)\n        else:\n            self.embeddings = nn.init.xavier_uniform(torch.zeros((len(self.wordx2idx), self.dim), device = self.device))\n        self.embeddings = nn.Embedding(len(self.word2idx), self.dim, _weight = self.embeddings)\n    def load_glove(self, glove_path):\n        '''\n        glove_path: path to the glove file.\n        '''\n        embeddings = nn.init.xavier_uniform(torch.zeros((len(self.word2idx), self.dim), device = self.device))\n        with open(glove_path, 'r') as file:\n            for line in tqdm.tqdm(file):\n                vals = line.split()\n                word = vals[0]\n                if word in self.word2idx:\n                    embedding = torch.tensor([float(val) for val in vals[1:]], device = self.device)\n                    embeddings[self.word2idx[word], :] = embedding\n        return embeddings\n    def decode(self, x):\n        return self.idx2word[x]\n    def pad_sents(self, x):\n        '''\n        Pads and Tokenizes Inputs\n        '''\n        tokenized_sents = []\n        for sent in x:\n            tok_sent = [self.PAD_ID for i in range(self.max_length)]\n            for word_idx in range(self.max_length):\n                if word_idx >= len(sent):\n                    break\n                if sent[word_idx] in self.word2idx:\n                    tok_sent[word_idx] = self.word2idx[sent[word_idx]]\n                else:\n                    tok_sent[word_idx] = self.word2idx[self.UNK]\n            tokenized_sents += [tok_sent]\n        return torch.tensor(tokenized_sents, device = self.device)\n            \n    def forward(self, x):\n        '''\n        Tokenizes a Given Input\n        '''\n        tokenized = [nltk.word_tokenize(sent) for sent in x]\n        padded = self.pad_sents(tokenized)\n        return padded\n    def embed(self, x):\n        return self.embeddings(x)","execution_count":null,"outputs":[]},{"metadata":{"id":"LSZL2hNP1Y5A","trusted":true,"scrolled":true},"cell_type":"code","source":"class Tokenizer(nn.Module):\n  def __init__(self, model_name, max_length):\n    super().__init__()\n    self.model_name = model_name\n    self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n    self.num_words = self.tokenizer.vocab_size\n    self.START = self.tokenizer.cls_token\n    self.END = self.tokenizer.sep_token\n    self.START_ID = self.tokenizer.cls_token_id\n    self.END_ID = self.tokenizer.sep_token_id \n    self.max_length = max_length\n  def forward(self, x):\n    '''\n    Tokenizes sentences using HuggingFace \n    '''\n    tokenized_sentences = self.tokenizer(x, add_special_tokens = True, padding = 'max_length', truncation = True, max_length = self.max_length, return_tensors = 'pt')\n    return tokenized_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    '''\n    Decoder Attention for the LSTM\n    '''\n    def __init__(self, enc_dim, dec_dim, inner_dim):\n        super().__init__()\n        self.enc_dim = enc_dim\n        self.dec_dim = dec_dim\n        self.inner_dim = inner_dim\n        \n        self.dec_proj = nn.Linear(self.dec_dim, self.inner_dim)\n        self.enc_proj = nn.Linear(self.enc_dim, self.inner_dim)\n    def forward(self, encoder, decoder):\n        '''\n        encoder: output from LSTM(B, enc)\n        decoder: output from CNN(B, L, dec)\n        '''\n        decoder = self.dec_proj(decoder) # (B, L, dim)\n        encoder = self.enc_proj(encoder).unsqueeze(1) # (B, 1, dim) \n        att_mat = F.softmax(torch.squeeze(torch.bmm(encoder, decoder.transpose(1, 2)), dim = 1)).unsqueeze(1) # (B, 1, L)\n        attended = torch.squeeze(torch.bmm(att_mat, decoder))\n        return attended \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n    '''\n    Uses LSTMS to Caption Images\n    '''\n    def __init__(self, in_dim, im_dim, max_length, device, drop_prob = 0.2):\n        super().__init__()\n        self.im_dim = im_dim\n        self.in_dim = in_dim\n        self.drop_prob = drop_prob\n        self.proj_hidden = nn.Linear(self.im_dim, self.im_dim)\n        self.proj_cell = nn.Linear(self.im_dim, self.im_dim)\n        self.max_length = max_length\n        self.device = device\n        \n        #self.Attention = Attention(self.im_dim, self.im_dim, self.im_dim)\n        self.LSTMCell = nn.LSTMCell(self.in_dim, self.im_dim)\n        \n        #self.model_name = 'distilbert-base-uncased'\n        self.tokenizer = CustomTokens(vocabulary, GLOVE_PATH, self.max_length, self.device)\n        self.num_classes = len(self.tokenizer.word2idx)\n        self.Dropout = nn.Dropout(self.drop_prob)\n        self.Linear = nn.Linear(self.im_dim, self.num_classes)\n        self.criterion = nn.CrossEntropyLoss()\n    def forward_train(self, x, GT):\n        '''\n        Uses LSTMs to Generate Captions\n        '''\n        # Tokenize Ground Truth\n        tokenized_GT = self.tokenizer(GT)\n        B, L = tokenized_GT.shape\n        # Project the Image down to One LSTM Cell\n        hidden_state = self.proj_hidden(torch.mean(x, dim = 1))\n        cell_state = self.proj_cell(torch.mean(x, dim = 1))\n        total_loss = torch.zeros((1), device = self.device)\n        # Begin Decoding\n        for l in range(0, L - 1):\n            input_id = tokenized_GT[:, l]\n            GT_id = tokenized_GT[:, l + 1]\n            \n            embeddings = self.tokenizer.embed(input_id)\n            hidden_state, cell_state = self.LSTMCell(embeddings, (hidden_state, cell_state))\n            copied_h = hidden_state.clone()\n            #attended = self.Attention(copied_h, x)\n            #concat = torch.cat([attended, copied_h], dim = -1)\n            concat = self.Dropout(copied_h)\n            pred = self.Linear(concat)\n            # Mask Out PAD tokens\n            keep = GT_id != self.tokenizer.PAD_ID\n            pred = pred[keep]\n            GT_id = GT_id[keep]\n            if pred.shape[0] != 0:\n                loss = self.criterion(pred, GT_id)\n                total_loss = total_loss + loss\n        return total_loss\n    def forward(self, x):\n        # Project Down the X to the hidden_state\n        hidden_state = self.proj_hidden(torch.mean(x, dim = 1))\n        cell_state = self.proj_cell(torch.mean(x, dim = 1) )\n        B, L = cell_state.shape\n        # Starter Sentences\n        current_tokens = torch.tensor([self.tokenizer.START_ID for i in range(B)], device = self.device) \n        pred_sentences = [self.tokenizer.START for i in range(B)]\n        finished = [False for i in range(B)]\n        # Begin Decoding\n        for i in range(self.max_length):\n            embeddings = self.tokenizer.embed(current_tokens)\n            hidden_state, cell_state = self.LSTMCell(embeddings, (hidden_state, cell_state))\n            copied_h = hidden_state.clone()\n            #attended = self.Attention(copied_h, x)\n            #concat = torch.cat([attended, copied_h], dim = -1)\n            pred = F.softmax(self.Linear(copied_h))\n            _, indices = torch.max(pred, dim = -1)\n            for b in range(B):\n                if finished[b]:\n                    continue # Finished Already\n                elif indices[b].item() == self.tokenizer.END_ID:\n                    finished[b] = True\n                    pred_sentences[b] += f\" {self.tokenizer.decode(indices[b].item())}\"\n                else:\n                    pred_sentences[b] += f\" {self.tokenizer.decode(indices[b].item())}\"\n                    current_tokens[b] = indices[b].item()\n        return pred_sentences","execution_count":null,"outputs":[]},{"metadata":{"id":"Q5ihVK4MA1CF","trusted":true,"scrolled":true},"cell_type":"code","source":"class FullModel(nn.Module):\n  '''\n  Houses the Full Image Captioning Model\n  '''\n  def __init__(self, device):\n    super().__init__()\n    self.device = device\n    self.in_dim = 200\n    self.im_dim = 2048\n    self.max_length = 20\n    self.image_encoder = ResNetBase(self.im_dim, self.device)\n    self.LSTM = LSTM(self.in_dim, self.im_dim, self.max_length, self.device)\n  def forward_train(self, images, captioning):\n    image_encoded = self.image_encoder(images)\n    return self.LSTM.forward_train(image_encoded, list(captioning))\n  def forward(self, images):\n    image_encoded = self.image_encoder(images)\n    return self.LSTM(image_encoded)","execution_count":null,"outputs":[]},{"metadata":{"id":"NnrHalJWyLv_"},"cell_type":"markdown","source":"# Custom Training Loop:"},{"metadata":{"id":"tsfkEklBuWlP","trusted":true,"scrolled":true},"cell_type":"code","source":"class ImageCaptioningSolver(nn.Module):\n  def __init__(self, device):\n    super().__init__()\n    self.device = device\n    self.model = FullModel(self.device)\n    self.optim = optim.Adam(self.model.parameters(), lr= 1e-3, weight_decay = 1e-3)\n    self.decay = optim.lr_scheduler.StepLR(self.optim, 1, 0.97)\n  def forward(self, x):\n    self.eval()\n    with torch.no_grad():\n      return self.model(x)\n  def evaluate(self, valloader):\n    '''\n    Evaluates the Model's BLEU score.\n    '''\n    self.eval()\n    with torch.no_grad():\n      total_bleu = 0\n      count = 0\n      for images, labels in tqdm.tqdm(valloader):\n        predicted = self.model(images)\n        bleu = nltk.translate.bleu_score.corpus_bleu(labels, predicted)\n        total_bleu += bleu\n        count += 1\n        del images\n        del labels\n        del predicted\n        torch.cuda.empty_cache()\n    return total_bleu / count\n  def training_loop(self, trainloader, valloader, NUM_EPOCHS, display_every = 64):\n    liveloss = livelossplot.PlotLosses()\n    best_val_bleu  = 0\n    best_val_loss = 999\n    torch.cuda.empty_cache()\n    for EPOCH in range(NUM_EPOCHS):\n      self.train()\n      logs = {}\n      total_loss = 0\n      count = 0\n      for images, labels in trainloader:\n        self.optim.zero_grad()\n        loss = self.model.forward_train(images, labels)\n        loss.backward()\n        self.optim.step()\n        print(f\"Step: {count}, loss: {loss.item()}\")\n        total_loss += loss.item()\n        count += 1\n        del images\n        del labels\n        torch.cuda.empty_cache()\n        if count == display_every:\n          break\n      logs['loss'] = total_loss / count\n      self.decay.step()\n      print(f\"EPOCH: {EPOCH}, total_loss: {logs['loss']}\")\n      \n      self.eval()\n      sample_caption = \"\"\n      sample_image = None\n      with torch.no_grad():\n        logs['val_loss'] = 0\n        logs['accuracy'] = 0\n        count = 0\n    \n        for images, labels in valloader:\n          loss = self.model.forward_train(images, labels)\n          pred_sentences = self.model(images)\n            \n          sample_caption = pred_sentences[0]\n          sample_image = images[0].cpu()\n          # Compute BLEU between GT and Predicted sentences\n          bleu = nltk.translate.bleu_score.corpus_bleu(labels, pred_sentences)\n          logs['val_loss'] += loss.item()\n          logs['accuracy'] += bleu\n          count += 1\n          del images\n          del labels\n          torch.cuda.empty_cache()\n          break\n        logs['val_loss'] /= count\n        logs['accuracy'] /= count\n      \n      liveloss.update(logs)\n      liveloss.send()\n      \n      if logs['val_loss'] < best_val_loss:\n        best_val_loss = logs['val_loss']\n        torch.save(self.model.state_dict(), \"./BestVal.pth\")\n      if logs['accuracy'] > best_val_bleu:\n        best_val_bleu = logs['accuracy']\n        torch.save(self.model.state_dict(), \"./BestAcc.pth\")\n      plt.imshow(sample_image.transpose(0, 1).transpose(1, 2))\n      plt.show()\n      print(f\"E:{EPOCH}VL{round(logs['val_loss'], 3)}VA{round(logs['accuracy'], 3)}loss:{round(logs['loss'], 3)}, Sample Caption Created: {sample_caption}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"2yzM1bXRyohy","trusted":true},"cell_type":"code","source":"%%capture\nsolver = ImageCaptioningSolver(device)\nsolver.to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"Xf7pT4FGAn2Z","outputId":"925cebd0-50da-4103-f7ec-6e3e6ad605bf","trusted":true,"scrolled":false},"cell_type":"code","source":"#solver.training_loop(TrainDataloader, ValDataloader, 120, display_every = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"solver.load_state_dict(torch.load(\"../input/image-captioned-trained/BestVal.pth\", map_location = device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"torch.save(solver.state_dict(), \"./Captioner.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(vocabulary, \"./vocab.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf val2014\n!rm -rf annotations","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}