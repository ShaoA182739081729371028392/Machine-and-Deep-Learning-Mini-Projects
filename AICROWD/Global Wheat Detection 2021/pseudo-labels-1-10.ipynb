{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Augment the Dataset for 30 EPOCHS(30 Sets of data is enough Augmentations)\n- This Speds up computation and training(30 minutes -> 20 Minutes/Epoch)\n","metadata":{}},{"cell_type":"markdown","source":"# Datasets Used:\n- 2800 Psuedolabels","metadata":{}},{"cell_type":"markdown","source":"Import Dependencies ","metadata":{}},{"cell_type":"code","source":"%%capture\n# Machine Learning and Data Science Imports\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as backend\nimport collections\nfrom tensorflow.keras import *\nimport tensorflow_addons as tfa\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np\nimport string\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom glob import glob\nimport random\nimport math\nfrom tqdm.notebook import tqdm\nimport os\n\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nAUTO = tf.data.experimental.AUTOTUNE\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:11.033503Z","iopub.execute_input":"2021-07-04T02:51:11.033894Z","iopub.status.idle":"2021-07-04T02:51:17.502263Z","shell.execute_reply.started":"2021-07-04T02:51:11.033798Z","shell.execute_reply":"2021-07-04T02:51:17.501241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation and Data Loading Logic","metadata":{}},{"cell_type":"code","source":"def compute_area(bbox):\n    # Computes Areas of a Bbox\n    ones = bbox[:, :2] # (N, 2)\n    twos = bbox[:, 2:4] # (N, 2)\n    diff = tf.maximum(twos - ones, tf.cast(0.0, dtype = twos.dtype)) # (N, 2)\n    return diff[:, 0] * diff[:, 1] # (N,)\ndef filter_bboxes(box):\n    # Filters out Bboxes if their area is < 0\n    indices_for_object = tf.where(tf.math.reduce_all([box[:,4] >= 0, \n                                                          box[:, 2] >= box[:, 0], \n                                                          box[:, 3] >= box[:, 1]], axis=0))\n    box = tf.cond(tf.shape(indices_for_object)[0] == 0, \n                    lambda: tf.reshape([0., 0., -1., -1., 0.], (-1, 1, box.shape[-1])),\n                    lambda: tf.reshape(tf.gather(box, indices_for_object), (-1, 1, box.shape[-1]))) # (?, 1, 5)\n    return box\ndef compute_centers(bbox):\n    return 0.5 * (bbox[:, :2] + bbox[:, 2:4])\ndef compute_iou(bbox, anchors):\n    # Computes IOU Between Bbox and Anchors\n    # Bbox: tensor(N, min(4)) - As long as the first 4 values are (x1, y1, x2, y2)\n    # Anchors: (x1, y1, x2, y2)\n    \n    # Expand bbox and anchor for broadcasting\n    area_bbox = compute_area(bbox) # (N, )\n    area_anchor = compute_area(anchors) # (M,)\n    \n    area_bbox = tf.expand_dims(area_bbox, axis = 0) # (1, N)\n    area_anchor = tf.expand_dims(area_anchor, axis = 1) # (M, 1)\n    \n    \n    bbox = tf.expand_dims(bbox, axis = 0) # (1, N, 5)\n    anchors = tf.expand_dims(anchors, axis = 1) # (M, 1, 5)\n    top_left_bboxes = tf.maximum(bbox[:, :, :2], anchors[:, :, :2]) # (M, N, 2)\n    bottom_right_bbox = tf.minimum(bbox[:, :, 2:4], anchors[:, :, 2:4]) # (M, N, 2)\n    \n    \n    \n    differences = bottom_right_bbox - top_left_bboxes# (M, N, 2)\n    differences = tf.maximum(differences, 0.0)\n    \n    inter = tf.maximum(differences[:, :, 0] * differences[:, :, 1], 0.0) # (M, N)\n    union = area_bbox + area_anchor - inter # (M, N)\n\n    eps = 1e-6\n    return (inter + eps) / (union + eps) # (M, N)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.503676Z","iopub.execute_input":"2021-07-04T02:51:17.503964Z","iopub.status.idle":"2021-07-04T02:51:17.520305Z","shell.execute_reply.started":"2021-07-04T02:51:17.503935Z","shell.execute_reply":"2021-07-04T02:51:17.519341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper FN\ndef filter_normal_bbox(orig_area, bboxes, thresh, min_x_value, max_x_value, min_y_value, max_y_value):\n    # orig_area: (N, )\n    # Bbox: ((N, 5), (N, C))\n    \n    # If the area changes too much when clamping the values(Scale, Shift, Rotate, etc,), remove the values\n    \n    x1 = tf.clip_by_value(bboxes[:, 0], min_x_value, max_x_value) \n    y1 = tf.clip_by_value(bboxes[:, 1], min_y_value, max_y_value)\n    x2 = tf.clip_by_value(bboxes[:, 2], min_x_value, max_x_value)\n    y2 = tf.clip_by_value(bboxes[:, 3], min_y_value, max_y_value)\n    \n\n    x1 = tf.expand_dims(x1, axis = -1)\n    y1 = tf.expand_dims(y1, axis = -1)\n    x2 = tf.expand_dims(x2, axis = -1)\n    y2 = tf.expand_dims(y2, axis = -1)\n\n    bboxes = tf.concat([x1, y1, x2, y2, bboxes[:, 4:]], axis = 1) # (B, 5)\n    new_area =  compute_area(bboxes) # (N, )\n  \n    # Compute the Change in area\n    eps = 1e-10\n    cng_in_area = (new_area) / (orig_area + eps) # (N, ) - Percentage shrink\n    # Threshold\n    keep = tf.where(tf.greater_equal(cng_in_area, thresh))\n    kept_bboxes = tf.gather(bboxes, keep, axis = 0) # (M, 5)\n    return kept_bboxes # (N, 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.522008Z","iopub.execute_input":"2021-07-04T02:51:17.522385Z","iopub.status.idle":"2021-07-04T02:51:17.537411Z","shell.execute_reply.started":"2021-07-04T02:51:17.522355Z","shell.execute_reply":"2021-07-04T02:51:17.536428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CROP ","metadata":{}},{"cell_type":"code","source":"class RandomCrop(object):\n    '''\n    Random Square Crop -> Resize(to avoid distortion: distortion is devastating to bounding boxes)\n    '''\n    def __init__(self, min_height, max_height, thresh = 0.25, p = 0.5):\n        self.min_height = min_height\n        self.p = p \n        self.max_height = max_height\n        self.resizer = Resize(image_size = self.max_height) # Resize After Crops\n        self.thresh = thresh # If the bbox decreases in area too much, drop the area\n        self.random = tf.data.experimental.RandomDataset(seed = 5)\n        self.iterator = iter(self.random)\n    def __call__(self, image, bboxes, seeds):\n        # Image: Tensor(H, W, C)\n        # H and W should be the same.\n        # Bboxes: Tensor(N, 5) \n        # classification: Tensor(N, C)\n        # Extract values\n        #seed1, seed2, seed3, seed4 = seeds\n        seed1 = next(self.iterator)\n        seed2 = next(self.iterator)\n        seed3 = next(self.iterator)\n        seed4 = next(self.iterator)\n        if len(bboxes.shape) > 2:\n          bboxes = tf.squeeze(bboxes, axis = 1)\n        \n        seed = next(self.iterator)\n        seed = seed % int(1e5)\n        seed = seed / int(1e5)\n        if seed > self.p:\n            return tf.cast(image, dtype = tf.float32), tf.cast(bboxes, dtype = tf.float32)\n        # compute Original Area\n        orig_area = compute_area(bboxes) # (N, )\n        # ------------------\n        # Find a Random Crop:\n\n\n        width = seed1 % (self.max_height - self.min_height) + self.min_height\n        height = seed2 % (self.max_height - self.min_height) + self.min_height # My version of random sampling.\n        # Clip the Width and Height\n\n\n        starting_x = seed3 % (self.max_height - width)\n        starting_y = seed4 % (self.max_height - height)\n        \n       \n        starting_x = tf.cast(starting_x, tf.int32)\n        starting_y = tf.cast(starting_y, tf.int32)\n        width = tf.cast(width, tf.int32)\n        height = tf.cast(height, tf.int32)\n        # Shift Bbox Coords by the starting_x and starting_y, to create relative x, and y values.\n        \n        x1 = bboxes[:, 0] - tf.cast(starting_x, bboxes.dtype)\n        y1 = bboxes[:, 1] - tf.cast(starting_y, bboxes.dtype)\n        x2 = bboxes[:, 2] - tf.cast(starting_x, bboxes.dtype)\n        y2 = bboxes[:, 3] - tf.cast(starting_y, bboxes.dtype)\n        obj = bboxes[:, 4:]\n\n        x1 = tf.expand_dims(x1, axis = -1)\n        x2 = tf.expand_dims(x2, axis = -1)\n        y1 = tf.expand_dims(y1, axis = -1)\n        y2 = tf.expand_dims(y2, axis = -1)\n\n        new_bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1) # (N, 5)\n        \n        # Crop the Images\n        new_image = image[starting_y: starting_y + height, starting_x: starting_x + width, :]\n        \n        kept_bboxes = tf.reshape(new_bboxes, (-1, 5))\n        kept_bboxes = filter_normal_bbox(orig_area, kept_bboxes, self.thresh, tf.cast(tf.constant(0.0), kept_bboxes.dtype), tf.cast(width, kept_bboxes.dtype), tf.cast(tf.constant(0.0), kept_bboxes.dtype), tf.cast(height, kept_bboxes.dtype))\n        kept_bboxes = tf.reshape(new_bboxes, (-1, 5))\n        \n        # Resize the Bounding Boxes and Image\n        new_image, kept_bboxes = self.resizer(new_image, kept_bboxes, height, width)\n        \n        return tf.cast(new_image, dtype = tf.float32), tf.cast(kept_bboxes, dtype = tf.float32)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.539365Z","iopub.execute_input":"2021-07-04T02:51:17.539748Z","iopub.status.idle":"2021-07-04T02:51:17.562014Z","shell.execute_reply.started":"2021-07-04T02:51:17.539718Z","shell.execute_reply":"2021-07-04T02:51:17.561177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transpose","metadata":{}},{"cell_type":"code","source":"def transpose(bboxes):\n  x1 = bboxes[:, 0]\n  y1 = bboxes[:, 1]\n  x2 = bboxes[:, 2]\n  y2 = bboxes[:, 3]\n  obj = bboxes[:, 4:]\n\n  x1, y1 = (y1, x1)\n  x2, y2 = (y2, x2)\n\n  x1 = tf.expand_dims(x1, axis = 1)\n  x2 = tf.expand_dims(x2, axis = 1)\n  y1 = tf.expand_dims(y1, axis = 1)\n  y2 = tf.expand_dims(y2, axis = 1) \n\n  bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n  return bboxes\nclass Transpose(object):\n  def __init__(self, p = 0.5):\n    self.p = p\n    self.random = tf.data.experimental.RandomDataset(seed = 3)\n    self.iterator = iter(self.random)\n  def __call__(self, images, bboxes, seeds):\n    seed = next(self.iterator)\n    seed = seed % int(1e5)\n    seed = seed / int(1e5)\n    if seed > self.p:\n      return images, bboxes\n    images = tf.squeeze(tf.image.transpose(tf.expand_dims(images, axis = 0)), axis = 0)\n    x1 = bboxes[:, 0]\n    y1 = bboxes[:, 1]\n    x2 = bboxes[:, 2]\n    y2 = bboxes[:, 3]\n    obj = bboxes[:, 4:]\n\n    x1, y1 = (y1, x1)\n    x2, y2 = (y2, x2)\n\n    x1 = tf.expand_dims(x1, axis = 1)\n    x2 = tf.expand_dims(x2, axis = 1)\n    y1 = tf.expand_dims(y1, axis = 1)\n    y2 = tf.expand_dims(y2, axis = 1) \n\n    bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n    return images, bboxes ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.562969Z","iopub.execute_input":"2021-07-04T02:51:17.563235Z","iopub.status.idle":"2021-07-04T02:51:17.578072Z","shell.execute_reply.started":"2021-07-04T02:51:17.563209Z","shell.execute_reply":"2021-07-04T02:51:17.577373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rotate 90","metadata":{}},{"cell_type":"code","source":"class Rotate90(object):\n    def __init__(self, image_size, p = 0.5, thresh = 0.25):\n        self.image_size = image_size\n        self.p = p\n        self.thresh = thresh\n        self.random = tf.data.experimental.RandomDataset()\n        self.iterator = iter(self.random)\n    \n    \n    def __call__(self, images, bboxes, seed):\n        random_seed = next(self.iterator)\n        random_seed = random_seed % int(1e5)\n        random_seed = random_seed / int(1e5)\n        if random_seed > self.p:\n            return images, bboxes\n        #seed1, seed2, seed3, seed4 = seed\n        seed1 = next(self.iterator)\n        seed2 = next(self.iterator)\n        seed3 = next(self.iterator)\n        seed4 = next(self.iterator)\n        \n        seed1 = (seed1 % 3) + 1\n        k = seed1\n    \n        images = tf.image.rot90(images, k = tf.cast(k, tf.int32))\n        orig_area = compute_area(bboxes)\n\n        if k == 1:\n          bboxes = horizontal_flip(images, bboxes)\n          bboxes = transpose(bboxes)\n        elif k == 2:\n          bboxes = horizontal_flip(images, bboxes)\n          bboxes = vertical_flip(images, bboxes)\n        else:\n          bboxes = vertical_flip(images, bboxes)\n          bboxes = transpose(bboxes)\n      \n        bboxes = filter_normal_bbox(orig_area, bboxes, self.thresh, tf.cast(tf.constant(0.), bboxes.dtype), tf.cast(tf.constant(self.image_size), bboxes.dtype), tf.cast(tf.constant(0.0), bboxes.dtype), tf.cast(tf.constant(self.image_size),bboxes.dtype))\n        bboxes = tf.reshape(bboxes, (-1, 5))\n        return images, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.579072Z","iopub.execute_input":"2021-07-04T02:51:17.579355Z","iopub.status.idle":"2021-07-04T02:51:17.59806Z","shell.execute_reply.started":"2021-07-04T02:51:17.579326Z","shell.execute_reply":"2021-07-04T02:51:17.597352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sharpen","metadata":{}},{"cell_type":"code","source":"class Sharpen(object):\n    def __init__(self, min_alpha = 0.2, max_alpha = 0.5, min_sharpness = 0.5, max_sharpness = 1.0, p = 0.5):\n        self.min_alpha = min_alpha\n        self.max_alpha = max_alpha\n        self.p = p\n        self.min_sharpness = min_sharpness\n        self.max_sharpness = max_sharpness\n\n        self.random = tf.data.experimental.RandomDataset()\n        self.iterator = iter(self.random)\n    def __call__(self, images, bboxes, seeds):\n        random_seed = next(self.iterator)\n        random_seed = random_seed % int(1e5)\n        random_seed = random_seed / int(1e5)\n\n        if random_seed > self.p:\n            return images, bboxes\n        #seed1, seed2, seed3, seed4 = seeds\n        seed1 = next(self.iterator)\n        seed2 = next(self.iterator)\n        seed3 = next(self.iterator)\n        seed4 = next(self.iterator)\n        \n        seed1 = seed1 % int(1e5) # Number of Buckets\n        seed1 = seed1 / int(1e5) # convert to uniform number\n\n        seed2 = seed2 % int(1e5)\n        seed2 = seed2 / int(1e5)\n        alpha = seed1\n        sharpness = seed2\n        \n        sharpened = tfa.image.sharpness(images, factor = tf.cast(sharpness, images.dtype)) \n        return images * tf.cast(1 - alpha, images.dtype) + tf.cast(alpha, sharpened.dtype) * sharpened, bboxes\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.599235Z","iopub.execute_input":"2021-07-04T02:51:17.599547Z","iopub.status.idle":"2021-07-04T02:51:17.617305Z","shell.execute_reply.started":"2021-07-04T02:51:17.599519Z","shell.execute_reply":"2021-07-04T02:51:17.616305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mosaic ","metadata":{}},{"cell_type":"code","source":"def crop_range(bboxes, h_min_bound, h_max_bound, w_min_bound, w_max_bound, thresh = 0.05):\n  # Clips Bounding Boxes and Turns invalid bounding boxes to 0(obj = 0)\n  \n  orig_area = compute_area(bboxes)\n\n  x1 = bboxes[:, 0]\n  y1 = bboxes[:, 1]\n  x2 = bboxes[:, 2]\n  y2 = bboxes[:, 3]\n  obj = bboxes[:, 4]\n\n  x1 = tf.clip_by_value(x1, w_min_bound, w_max_bound)\n  y1 = tf.clip_by_value(y1, h_min_bound, h_max_bound)\n  x2 = tf.clip_by_value(x2, w_min_bound, w_max_bound)\n  y2 = tf.clip_by_value(y2, h_min_bound, h_max_bound)\n\n  x1 = tf.expand_dims(x1, axis = 1)\n  y1 = tf.expand_dims(y1, axis = 1)\n  x2 = tf.expand_dims(x2, axis = 1)\n  y2 = tf.expand_dims(y2, axis = 1)\n\n  bboxes = tf.concat([x1, y1, x2, y2, obj[:, None]], axis = 1)\n  # Compute the Area\n  area = compute_area(bboxes) # (N, )\n  # Filter out the invalid bounding boxes\n  cng_in_area = area / orig_area\n  keep = tf.cast(tf.greater(cng_in_area, thresh), area.dtype)\n  obj = obj * keep\n\n  bboxes = tf.concat([x1, y1, x2, y2, obj[:, None]], axis = 1)\n  \n  return bboxes\n\ndef shift_bounding_boxes(bboxes, h_shift, w_shift):\n  x1 = tf.expand_dims(bboxes[:, 0] - w_shift, axis = 1)\n  y1 = tf.expand_dims(bboxes[:, 1] - h_shift, axis = 1)\n  x2 = tf.expand_dims(bboxes[:, 2] - w_shift, axis = 1)\n  y2 = bboxes[:, 3] - h_shift \n  obj = bboxes[:, 4:]\n\n  boxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n  return boxes\n\nclass CustomMosaic(object):\n    # Corners Mosaic.\n    def __init__(self, p = 0.5, thresh = 0.05, border = 40):\n        self.p = p\n        self.border = border\n        self.thresh = thresh\n        # So many random seeds that a generator is needed\n        self.generator = tf.data.experimental.RandomDataset()\n        self.iterator = iter(self.generator)\n    \n    def __call__(self, images, bboxes, seeds):\n        B, H, W, C = images.shape\n        shift = H // 2\n        # Reshape the Bboxes\n        bboxes = tf.reshape(bboxes, (B, -1, 5))\n        B, N, _ = bboxes.shape\n        IMAGES = tf.ones((0, H, W, C))\n        BBOXES = tf.ones((0, N * 4, 5))\n        # bboxes: (B, N, 5) \n        for b in range(B):\n          value = next(self.iterator) % int(1e5)\n          value = value / int(1e5)\n          #tf.print('---------ITERATION--------------')\n          if value > self.p:\n            # Stack bboxes 4x\n            #tf.print(\"---------Skip-----------\")\n            image1 = images[b]\n            bboxes1 = bboxes[b]\n            # Bounding Boxes set to 0\n            bbox0 = tf.zeros_like(bboxes1, dtype = bboxes1.dtype) \n            #tf.print(bboxes1.shape, bbox0.shape)\n            boxes = tf.concat([bboxes1, bbox0, bbox0, bbox0], axis = 0)\n            \n            #tf.print(boxes.shape, BBOXES.shape)\n            #tf.print(image1.shape, IMAGES.shape)\n            IMAGES = tf.concat([IMAGES, tf.expand_dims(image1, axis = 0)], axis = 0)\n            BBOXES = tf.concat([BBOXES, tf.expand_dims(boxes, axis = 0)], axis = 0)\n            \n          else:\n            #tf.print('----------MOSAIC---------')\n\n            # select 4 Corners of the image\n            x1 = next(self.iterator) % (W - self.border) + self.border\n            y1 = next(self.iterator) % (H - self.border) + self.border \n          \n\n            \n            # Select the Corners from four random images\n            image1 = images[b] # (H, W, 3)\n            bboxes1 = bboxes[b] # (N, 5)\n            # Select More images(3 extra)\n            idx2 = next(self.iterator) % B\n            idx3 = next(self.iterator) % B\n            idx4 = next(self.iterator) % B \n            image2 = images[idx2]\n            bboxes2 = bboxes[idx2]\n\n            image3 = images[idx3]\n            bboxes3= bboxes[idx3]\n\n            image4 = images[idx4]\n            bboxes4 = bboxes[idx4]\n            \n            # Grab out the corners\n            corner1 = image1[y1:, x1:] # Bottom Right of Image\n            corner2 = image2[:y1, x1:] # Top Right of Image \n            corner3 = image3[y1:, :x1] # Bottom Left of Image\n            corner4 = image4[:y1, :x1] # Top Left of Image\n\n            # Crop and Filter bounding boxes\n            bboxes1 = crop_range(bboxes1, tf.cast(y1, bboxes1.dtype), tf.cast(H, bboxes1.dtype), tf.cast(x1, bboxes1.dtype), tf.cast(W, bboxes1.dtype), thresh = self.thresh)\n            bboxes2 = crop_range(bboxes2, tf.constant(0.0, bboxes2.dtype), tf.cast(y1, bboxes2.dtype), tf.cast(x1, bboxes2.dtype), tf.cast(W, bboxes2.dtype), thresh = self.thresh)\n            bboxes3 = crop_range(bboxes3, tf.cast(y1, bboxes3.dtype), tf.cast(H, bboxes.dtype), tf.constant(0.0, bboxes3.dtype), tf.cast(x1, bboxes.dtype), thresh = self.thresh)\n            bboxes4 = crop_range(bboxes4, tf.constant(0.0, bboxes4.dtype), tf.cast(y1, bboxes4.dtype), tf.constant(0.0, bboxes4.dtype), tf.cast(x1, bboxes4.dtype), thresh = self.thresh)\n            \n            # Concatenate the Corners\n            top_half = tf.concat([corner2, corner1], axis = 0)\n            bottom_half = tf.concat([corner4, corner3], axis = 0)\n            full_image = tf.concat([bottom_half, top_half], axis = 1)\n            # concatenate the bounding boxes.\n            #tf.print(bboxes1.shape, bboxes2.shape, bboxes3.shape, bboxes4.shape)\n            \n            boxes = tf.concat([bboxes1, bboxes2, bboxes3, bboxes4], axis = 0)\n            \n\n            #tf.print(BBOXES.shape, boxes.shape)\n            IMAGES = tf.concat([IMAGES, tf.expand_dims(full_image, axis = 0)], axis = 0)\n            BBOXES = tf.concat([BBOXES, tf.expand_dims(boxes, axis = 0)], axis = 0)\n            #tf.print(BBOXES.shape)\n        return IMAGES, BBOXES","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.619553Z","iopub.execute_input":"2021-07-04T02:51:17.61984Z","iopub.status.idle":"2021-07-04T02:51:17.653243Z","shell.execute_reply.started":"2021-07-04T02:51:17.619812Z","shell.execute_reply":"2021-07-04T02:51:17.652366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mixup","metadata":{}},{"cell_type":"code","source":"class Mixup(object):\n  # object detection Mixup\n  # Just Average 2 Images, and concatenate the bounding boxes.\n\n  def __init__(self, p = 0.5):\n    super().__init__()\n    self.p = p\n    self.generator = tf.data.experimental.RandomDataset()\n    self.iterator = iter(self.generator)\n  def __call__(self, images, bboxes, seeds):\n   \n    # Images: Tensor(B, H, W, 3)\n    # Bboxes: Tensor(B, 5) -> Batched Image Bbox Pairs.\n    \n    B, H, W, C = images.shape\n    _, N, num_channels = bboxes.shape\n    IMAGES = tf.ones((0, H, W, C), images.dtype)\n    BBOXES = tf.ones((0, N * 2, num_channels), images.dtype)\n\n    for b in range(B):\n      base_image = images[b] # (H, W, 3)\n      base_bbox = bboxes[b]  # (N, 5)\n      seed = next(self.iterator) % int(1e5)\n      seed = seed / int(1e5)\n      if seed > self.p:\n        new_images = base_image\n        #tf.print(base_bbox.shape, black_bbox.shape)\n        new_bboxes = tf.concat([base_bbox, tf.zeros_like(base_bbox, dtype = base_bbox.dtype)], 0)\n\n        IMAGES = tf.concat([IMAGES, tf.expand_dims(new_images, axis = 0)], axis = 0) \n        BBOXES = tf.concat([BBOXES, tf.expand_dims(new_bboxes, axis = 0)], axis = 0)\n\n      else:\n        #tf.print('--------Not skip----------')\n        # Select another image\n        selected_index = next(self.iterator) % B\n        selected_image = images[selected_index] # (H, W, 3)\n        selected_bbox = bboxes[selected_index] # (N, 5) \n        #tf.print(images.shape, bboxes.shape)\n        #tf.print(selected_image.shape, selected_bbox.shape)\n        # Mixup\n        #tf.print(base_bbox.shape, selected_bbox.shape)\n\n        new_image = (base_image + selected_image) / 2\n        new_bbox = tf.concat([base_bbox, selected_bbox], axis = 0)\n        #tf.print(IMAGES.shape, new_image.shape)\n        #tf.print(BBOXES.shape, new_bbox.shape)\n        IMAGES = tf.concat([IMAGES, tf.expand_dims(new_image, axis = 0)], axis = 0)\n        BBOXES = tf.concat([BBOXES, tf.expand_dims(new_bbox, axis = 0)], axis = 0)\n\n    IMAGES = tf.reshape(IMAGES, (B, H, W, C))\n    BBOXES = tf.reshape(BBOXES, (B, N * 2, num_channels))\n\n    return IMAGES, BBOXES   ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:17.654491Z","iopub.execute_input":"2021-07-04T02:51:17.65477Z","iopub.status.idle":"2021-07-04T02:51:17.680758Z","shell.execute_reply.started":"2021-07-04T02:51:17.654743Z","shell.execute_reply":"2021-07-04T02:51:17.679747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cut_one_hole(image, min_x, min_y, mask_size):\n  # KEY ASSUMPTION: Batch size must be 1.\n  IMAGE_SIZE = tf.constant(DataModule.IMG_SHAPE[0])\n\n  B, H, W, _ = image.shape\n\n  image = tf.reshape(image, (B, IMAGE_SIZE, IMAGE_SIZE, -1))\n  B, H, W, C = image.shape\n  \n  max_x = min_x + mask_size\n  max_y = min_y + mask_size\n  \n  # Zero Tensor\n  zero_values = tf.zeros((B, mask_size, mask_size, C), image.dtype)\n\n  # Compute the Left Array of Ones\n  left_tensor = tf.ones((B, H, min_x, C), image.dtype)\n  right_tensor = tf.ones((B, H, W - max_x, C), image.dtype) \n\n  # Top Part\n  top_part = tf.ones((B, min_y, mask_size, C), image.dtype)\n  bottom_part = tf.ones((B, H - max_y, mask_size, C), image.dtype)\n\n  middle_portion = tf.concat([top_part, zero_values, bottom_part], axis = 1) # (B, H, mask_size, C)\n  mask = tf.concat([left_tensor, middle_portion, right_tensor], axis = 2)\n\n  return tf.cast(mask * image, dtype = tf.float32)\nclass CustomCutout(object):\n    # Simple Random Cutout, as it is done with Albumentations\n    # SHOULD BE APPLIED AFTER MIXUP and MOSAIC, In a BATCHED manner\n    def __init__(self, mask_size, num_masks, p = 0.5):\n        self.mask_size = mask_size\n        self.num_masks = num_masks \n        self.p = p \n        self.seed = 42\n        self.generator = tf.data.experimental.RandomDataset()\n        self.iterator = iter(self.generator)\n    def __call__(self, images, bboxes, seeds):\n        B, H, W, C = images.shape\n        B, N, num_classes = bboxes.shape\n        IMAGES = tf.ones((0, H, W, C), images.dtype)\n        BBOXES = tf.ones((0, N, num_classes), bboxes.dtype)\n        # Nothing to do with bboxes\n        for b in range(B):\n          seed = next(self.iterator)\n          seed = seed % int(1e5)\n          seed = seed / int(1e5)\n          if seed > self.p:\n            new_images = tf.cast(tf.expand_dims(images[b], 0), dtype = images.dtype)\n            new_bboxes = tf.cast(tf.expand_dims(bboxes[b], 0), dtype = bboxes.dtype)\n            IMAGES = tf.cast(tf.concat([IMAGES, new_images], axis = 0), dtype = images.dtype)\n            BBOXES = tf.cast(tf.concat([BBOXES, new_bboxes], axis = 0), dtype = bboxes.dtype)\n          else:\n            new_images = tf.cast(tf.expand_dims(images[b], 0), dtype = images.dtype)\n            new_bboxes = tf.cast(tf.expand_dims(bboxes[b], 0), dtype = bboxes.dtype)\n            for i in range(self.num_masks):\n              # cutout a box\n              # Compute a Random Place\n              x = next(self.iterator) % (W - self.mask_size)\n              y = next(self.iterator) % (H - self.mask_size)\n\n              new_images = tf.cast(cut_one_hole(new_images, x, y, self.mask_size), dtype = images.dtype)\n            IMAGES = tf.cast(tf.concat([IMAGES, new_images], axis = 0), dtype = images.dtype)\n            BBOXES = tf.cast(tf.concat([BBOXES, new_bboxes], axis = 0), dtype = bboxes.dtype)\n        return IMAGES, BBOXES","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:25.734929Z","iopub.execute_input":"2021-07-04T02:51:25.735307Z","iopub.status.idle":"2021-07-04T02:51:25.755899Z","shell.execute_reply.started":"2021-07-04T02:51:25.735261Z","shell.execute_reply":"2021-07-04T02:51:25.754373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def horizontal_flip(img, bboxes):\n  H, W, C = img.shape\n  IMG_CENTER = W // 2 if W else DataModule.IMG_SHAPE[1] // 2\n  \n  x1 = tf.expand_dims(bboxes[:, 0], 1)\n  y1 = tf.expand_dims(bboxes[:, 1], 1)\n  x2 = tf.expand_dims(bboxes[:, 2], 1)\n  y2 = tf.expand_dims(bboxes[:, 3], 1)\n  obj = bboxes[:, 4:]\n  \n  \n  x1 = x1 + 2 * (IMG_CENTER - x1)\n  x2 = x2 + 2 * (IMG_CENTER - x2) \n\n  box_w = tf.abs(x2 - x1)\n\n  x1 = x1 - box_w\n  x2 = x2 + box_w\n\n  \n  bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n  return bboxes\ndef vertical_flip(img, bboxes):\n  H, W, C = img.shape\n  img_center = H // 2 if H else DataModule.IMG_SHAPE[0] // 2\n  \n  x1 = tf.expand_dims(bboxes[:, 0], 1)\n  y1 = tf.expand_dims(bboxes[:, 1], 1)\n  x2 = tf.expand_dims(bboxes[:, 2], 1)\n  y2 = tf.expand_dims(bboxes[:, 3], 1)\n  obj = bboxes[:, 4:]\n\n  y1 = y1 + 2 * (img_center - y1)\n  y2 = y2 + 2 * (img_center - y2)\n\n  box_h = tf.abs(y2 - y1)\n\n  y1 = y1 - box_h\n  y2 = y2 + box_h\n\n  bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n  bboxes = tf.reshape(bboxes, (-1, 5))\n\n  return bboxes\nclass Resize(object):\n    \"\"\"Resize the image in accordance to `image_letter_box` function in darknet \n    \n    The aspect ratio is maintained. The longer side is resized to the input \n    size of the network, while the remaining space on the shorter side is filled \n    with black color. **This should be the last transform**\n    \n    \n    Parameters\n    ----------\n    inp_dim : tuple(int)\n        tuple containing the size to which the image will be resized.\n        \n    Returns\n    -------\n    \n    numpy.ndaaray\n        Sheared image in the numpy format of shape `HxWxC`\n    \n    numpy.ndarray\n        Resized bounding box co-ordinates of the format `n x 4` where n is \n        number of bounding boxes and 4 represents `x1,y1,x2,y2` of the box\n        \n    \"\"\"\n    \n    def __init__(self, image_size = 1024):\n        self.inp_dim = tf.constant(image_size)\n        \n    def __call__(self, img, bboxes, height, width):\n        \n        w,h = width, height\n        img = tf.image.resize(img, (self.inp_dim, self.inp_dim), antialias = True)\n        \n        if h is None:\n          return img, bboxes\n       \n\n        w_scale = tf.cast(self.inp_dim / w, bboxes.dtype)\n        h_scale = tf.cast(self.inp_dim / h, bboxes.dtype)\n\n        x1 = bboxes[:, 0] * w_scale\n        y1 = bboxes[:, 1] * h_scale\n        x2 = bboxes[:, 2] * w_scale\n        y2 = bboxes[:, 3] * h_scale\n        obj = bboxes[:, 4:]\n\n        x1 = tf.expand_dims(x1, 1)\n        y1 = tf.expand_dims(y1, 1)\n        x2 = tf.expand_dims(x2, 1)\n        y2 = tf.expand_dims(y2, 1)\n\n        bboxes = tf.concat([\n        x1, y1, x2, y2, obj\n        ], axis = 1)\n        return img, bboxes \n    \nclass RandomHorizontalFlip(object):\n\n    \"\"\"Randomly horizontally flips the Image with the probability *p*\n    Parameters\n    ----------\n    p: float\n        The probability with which the image is flipped\n    Returns\n    -------\n    numpy.ndaaray\n        Flipped image in the numpy format of shape `HxWxC`\n    numpy.ndarray\n        Tranformed bounding box co-ordinates of the format `n x 4` where n is\n        number of bounding boxes and 4 represents `x1,y1,x2,y2` of the box\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n        self.random = tf.data.experimental.RandomDataset()\n        self.iterator = iter(self.random)\n    def __call__(self, img, bboxes, seeds):\n        seed = next(self.iterator)\n        seed = seed % int(1e5)\n        seed = seed / int(1e5)\n        if seed > self.p:\n          return img, bboxes\n        H, W, C = img.shape\n        IMG_CENTER = W // 2 if W else DataModule.IMG_SHAPE[1] // 2\n        img = img[:, ::-1]\n        \n        x1 = tf.expand_dims(bboxes[:, 0], 1)\n        y1 = tf.expand_dims(bboxes[:, 1], 1)\n        x2 = tf.expand_dims(bboxes[:, 2], 1)\n        y2 = tf.expand_dims(bboxes[:, 3], 1)\n        obj = bboxes[:, 4:]\n        \n        \n        x1 = x1 + 2 * (IMG_CENTER - x1)\n        x2 = x2 + 2 * (IMG_CENTER - x2) \n\n        box_w = tf.abs(x2 - x1)\n\n        x1 = x1 - box_w\n        x2 = x2 + box_w\n\n        \n        bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n\n        return img, bboxes\nclass RandomVerticalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n        self.random = tf.data.experimental.RandomDataset()\n        self.iterator = iter(self.random)\n\n    def __call__(self, img, bboxes, seeds):\n        seed = next(self.iterator)\n        seed = seed % int(1e5)\n        seed = seed / int(1e5)\n        if seed > self.p:\n          return img, bboxes\n\n        H, W, C = img.shape\n        img_center = H // 2 if H else DataModule.IMG_SHAPE[0] // 2\n        \n        img = img[::-1, :, :]\n        x1 = tf.expand_dims(bboxes[:, 0], 1)\n        y1 = tf.expand_dims(bboxes[:, 1], 1)\n        x2 = tf.expand_dims(bboxes[:, 2], 1)\n        y2 = tf.expand_dims(bboxes[:, 3], 1)\n        obj = bboxes[:, 4:]\n\n        y1 = y1 + 2 * (img_center - y1)\n        y2 = y2 + 2 * (img_center - y2)\n\n        box_h = tf.abs(y2 - y1)\n\n        y1 = y1 - box_h\n        y2 = y2 + box_h\n\n        bboxes = tf.concat([x1, y1, x2, y2, obj], axis = 1)\n        bboxes = tf.reshape(bboxes, (-1, 5))\n\n        return img, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:26.508408Z","iopub.execute_input":"2021-07-04T02:51:26.508858Z","iopub.status.idle":"2021-07-04T02:51:26.545849Z","shell.execute_reply.started":"2021-07-04T02:51:26.50882Z","shell.execute_reply":"2021-07-04T02:51:26.545075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class RandomResizedCrop(object):\n  def __init__(self, min_height, max_height, thresh = 0.25, crop_prob = 0.75, image_size = 1024):\n    self.crop = RandomCrop(min_height, max_height, thresh = thresh, p = crop_prob)\n  def __call__(self, images, bboxes, seeds):\n    #print(images.shape, bboxes.shape)\n    images, bboxes = self.crop(images, bboxes, seeds)\n    return images, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:27.617443Z","iopub.execute_input":"2021-07-04T02:51:27.61808Z","iopub.status.idle":"2021-07-04T02:51:27.624185Z","shell.execute_reply.started":"2021-07-04T02:51:27.618031Z","shell.execute_reply":"2021-07-04T02:51:27.623538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Encoding","metadata":{}},{"cell_type":"code","source":"# BBox Alignment Code\ndef datasets_encode(x, y, overlap_threshold = 0.5, ignore_threshold = 0.4):\n    # x (Batch Size, H, W, 3)\n    # y (Batch Size, N, 5)  x1, y1, x2, y2, class\n\n    # resize image\n    WIDTH, HEIGHT, _ = DataModule.IMG_SHAPE\n    if x.shape == DataModule.IMG_SHAPE:\n        x_scale, y_scale = DataModule.IMG_SHAPE[0] / x.shape[0], DataModule.IMG_SHAPE[1] / x.shape[1]\n        y = y * [x_scale, y_scale, x_scale, y_scale, 1]\n        x = tf.image.resize(x, [DataModule.IMG_SHAPE[0], DataModule.IMG_SHAPE[1]])\n    \n\n    assignment_x = tf.zeros((DataModule.ANCHORS.shape[0]), tf.float32)\n    assignment_y = tf.zeros((DataModule.ANCHORS.shape[0]), tf.float32)\n    assignment_w = tf.zeros((DataModule.ANCHORS.shape[0]), tf.float32)\n    assignment_h = tf.zeros((DataModule.ANCHORS.shape[0]), tf.float32)\n    assignment_classes_idx = tf.zeros((DataModule.ANCHORS.shape[0]), tf.float32) + DataModule.NUM_CLASSES\n    assignment_is_obj = tf.zeros((DataModule.ANCHORS.shape[0]), tf.float32)\n\n\n    # filter correct boxes (area > 0)\n    priors = DataModule.ANCHORS # (1, 196416, 4)\n    width = tf.expand_dims(priors[:, 2] - priors[:, 0], axis = 1)\n    height = tf.expand_dims(priors[:, 3] - priors[:, 1], axis = 1)\n    \n    anchor_wh = tf.concat([width, height], axis = 1) # (196416, 2)\n    \n    box = tf.squeeze(filter_bboxes(y), axis = 1)\n   \n    iou = compute_iou(priors, box)\n\n    iou_max = tf.math.reduce_max(iou, axis=0) # (196416)\n    iou_max_idxs = tf.math.argmax(iou, axis=0) # (196416)\n    \n    # ignore box\n    ignore_mask = tf.math.logical_and(iou_max > ignore_threshold, iou_max < overlap_threshold) # (196416)\n    assignment_is_obj = tf.where(ignore_mask, -1., assignment_is_obj)\n\n    # object box\n    assign_mask = iou_max > overlap_threshold # (196416)\n    #print(tf.where(assign_mask))\n    box_best = tf.gather(box, iou_max_idxs, axis=0) # (?, 5) + (196416) => (196416, 5)_\n    assignment_is_obj = tf.where(assign_mask, 1., assignment_is_obj)\n    assignment_classes_idx = tf.where(assign_mask, box_best[:, 4], assignment_classes_idx)\n\n\n\n    box_center = compute_centers(box_best)\n\n    box_wh = box_best[:,2:4] - box_best[:,:2]  #  (196416, 2)\n    assigned_xy = (box_center - compute_centers(priors)) #  (196416, 2)\n    assigned_wh = tf.math.log(box_wh / anchor_wh) #  (196416, 2)\n    assignment_x = tf.where(assign_mask, assigned_xy[:, 0], assignment_x)\n    assignment_y = tf.where(assign_mask, assigned_xy[:, 1], assignment_y)\n    assignment_w = tf.where(assign_mask, assigned_wh[:, 0], assignment_w)\n    assignment_h = tf.where(assign_mask, assigned_wh[:, 1], assignment_h)\n\n\n    regression_list = tf.stack([\n        assignment_x, assignment_y, \n        assignment_w, assignment_h, \n        assignment_is_obj], axis=-1) # (N, 5)  \n    \n\n    classification_list = tf.one_hot(tf.cast(assignment_classes_idx, tf.int32), DataModule.NUM_CLASSES + 1) \n        \n    \n    return x, (regression_list, classification_list)\ndef batched_dataset_encode(images, bboxes, overlap_threshold = 0.5, ignore_threshold = 0.4):\n  # Images: tensor(B, H, W, 3)\n  # Bboxes: (Tensor(B, N,5), Tensor(B, N, C))\n  B, H, W, C = images.shape\n  _, _, num_channels = bboxes.shape\n  num_anchors = len(DataModule.ANCHORS)\n  num_classes = DataModule.NUM_CLASSES if DataModule.NUM_CLASSES != 1 else 2 \n  IMAGES = tf.ones((0, H, W, C), images.dtype)\n  BBOXES = tf.ones((0, num_anchors, num_channels), images.dtype)\n  CLASSIFICATION = tf.ones((0, num_anchors, num_classes), images.dtype)\n  for b in range(B):\n    image = images[b] # (H, W, 3)\n    bbox = bboxes[b] # (N, 5)\n    \n    # Drop Padded Values: Where bboxes[:, -1] == 0.0\n    keep = tf.equal(bbox[:, -1], 1.0)\n  \n    bbox = tf.boolean_mask(bbox, keep, axis = 0)\n\n    image, (bbox, Class) = datasets_encode(image, bbox, overlap_threshold = overlap_threshold, ignore_threshold = ignore_threshold)\n    \n    IMAGES = tf.concat([IMAGES, tf.expand_dims(image, axis = 0)], axis = 0)\n    BBOXES = tf.concat([BBOXES, tf.expand_dims(bbox, axis = 0)], axis = 0)\n    CLASSIFICATION = tf.concat([CLASSIFICATION, tf.expand_dims(Class, axis = 0)], axis = 0)\n  return IMAGES, (BBOXES, CLASSIFICATION)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:29.542506Z","iopub.execute_input":"2021-07-04T02:51:29.543137Z","iopub.status.idle":"2021-07-04T02:51:29.56695Z","shell.execute_reply.started":"2021-07-04T02:51:29.54308Z","shell.execute_reply":"2021-07-04T02:51:29.566238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mosaic_Plus_Mixup(object):\n    def __init__(self, p = 0.33):\n        self.p = p\n        self.mosaic = CustomMosaic(p = 1.)\n        self.mixup = Mixup(p = 1.)\n    def __call__(self, images, bboxes, seeds):\n        seed = seeds[0]\n        seed = seed % 1000\n        seed = seed / 1000\n    \n        if seed < self.p:\n            zero_bboxes = tf.zeros_like(bboxes, dtype = bboxes.dtype)\n            bboxes = tf.concat([bboxes, zero_bboxes, zero_bboxes, zero_bboxes], axis = 1) \n        elif seed < self.p * 2:\n            # MOSAIC\n            images, bboxes = self.mosaic(images, bboxes, seeds) \n        else:\n            # MIXUP \n            images, bboxes = self.mixup(images, bboxes, seeds)\n            zero_bboxes = tf.zeros_like(bboxes, dtype = bboxes.dtype)\n            bboxes = tf.concat([bboxes, zero_bboxes], axis = 1)\n            \n        return images, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:30.049051Z","iopub.execute_input":"2021-07-04T02:51:30.0494Z","iopub.status.idle":"2021-07-04T02:51:30.057816Z","shell.execute_reply.started":"2021-07-04T02:51:30.049371Z","shell.execute_reply":"2021-07-04T02:51:30.056632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Augments:\n  IMAGE_SIZE = 1024\n  THRESH = 0.25 # Doesn't Matter how large the bounding box is: Just can't be completely outside of the image.\n  BORDER = 100\n  # ------Augmentation----------\n  resize = RandomResizedCrop(800, IMAGE_SIZE, thresh = THRESH, crop_prob = 0.75, image_size = IMAGE_SIZE)\n  hori_flip = RandomHorizontalFlip(p = 0.5)\n  vert_flip = RandomVerticalFlip(p = 0.5)\n  rotate90 = Rotate90(IMAGE_SIZE, p = 0.5, thresh = THRESH)\n  cutout = CustomCutout(64, 8, p = 0.5)\n  sharpen = Sharpen(p = 0.5)\n  mosaic_mixup = Mosaic_Plus_Mixup(p = 0.33) # P = 0.33, Mixup = 0.33, Mosaic = 0.33\n  mosaic = CustomMosaic(p = 0.5)\n  mixup = Mixup(p = 0.5)\n\n  transpose = Transpose(p = 0.5)\n  \n  all_augs = [\n      hori_flip,\n      vert_flip,\n      rotate90,\n      transpose,\n      #sharpen,\n      \n  ]\n  batched_augs = [\n      #mosaic,\n      #mixup,\n      mosaic_mixup,\n      cutout\n  ]\n\n  MAX_BBOX = 200 # Assume Max # of Bboxes, to pad to.(You can tune this to the dataset)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:31.006311Z","iopub.execute_input":"2021-07-04T02:51:31.006648Z","iopub.status.idle":"2021-07-04T02:51:31.120459Z","shell.execute_reply.started":"2021-07-04T02:51:31.006617Z","shell.execute_reply":"2021-07-04T02:51:31.119445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IMAGE_LOAD(example):\n    feature_dict = {\n        'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=''),\n        'bboxes': tf.io.VarLenFeature(dtype = tf.int64)\n    }\n    features = tf.io.parse_single_example(example, features=feature_dict)\n    \n    image = features['image']\n    bboxes = features['bboxes'] \n    \n    # Load image\n    image = tf.io.decode_jpeg(image, channels = 3)\n    #Cast to Desired Dtype\n    image = tf.cast(image, tf.float32) / 255.0\n    # Reshape the image \n    image = tf.reshape(image,  DataModule.IMG_SHAPE)\n    # Load the Bounding Boxes\n    bboxes = tf.cast(tf.sparse.to_dense(bboxes), tf.float32)\n    bboxes = tf.reshape(bboxes, (-1, 4))\n    # all Bounding boxes there are class 1(Obviously.)\n    values = tf.identity(bboxes)[:, 0] * 0.0 + 1.0\n    bboxes = tf.concat([bboxes, tf.expand_dims(values, axis = 1)],axis = 1) \n    \n    return image, bboxes\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:33.297887Z","iopub.execute_input":"2021-07-04T02:51:33.298265Z","iopub.status.idle":"2021-07-04T02:51:33.306748Z","shell.execute_reply.started":"2021-07-04T02:51:33.298231Z","shell.execute_reply":"2021-07-04T02:51:33.305768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def thresh_by_zero(bboxes):\n  img_shape = Augments.IMAGE_SIZE\n  # Bboxes: Tensor(B, 5)\n  x1 = bboxes[:, 0]\n  y1 = bboxes[:, 1]\n  x2 = bboxes[:, 2]\n  y2 = bboxes[:, 3]\n  obj = bboxes[:, 4:]\n\n  # Threshold the Values\n  x1 = tf.expand_dims(tf.clip_by_value(x1, 0.0, img_shape - 1), axis = -1)\n  y1 = tf.expand_dims(tf.clip_by_value(y1, 0.0, img_shape - 1), axis = -1)\n  x2 = tf.expand_dims(tf.clip_by_value(x2, 0.0, img_shape - 1), axis = -1)\n  y2 = tf.expand_dims(tf.clip_by_value(y2, 0.0, img_shape - 1), axis = -1)\n  # Compute Area and Threshold\n  bboxes = tf.concat([x1, y1, x2, y2, bboxes[:, 4:]], axis = 1) # (B, 5)\n  new_area =  compute_area(bboxes) # (N, )\n\n  # Threshold\n  keep = tf.where(tf.greater(new_area, 0.0))\n  kept_bboxes = tf.gather(bboxes, keep, axis = 0) # (M, 5)\n  return kept_bboxes # (N, 5)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:34.333679Z","iopub.execute_input":"2021-07-04T02:51:34.334247Z","iopub.status.idle":"2021-07-04T02:51:34.342466Z","shell.execute_reply.started":"2021-07-04T02:51:34.334194Z","shell.execute_reply":"2021-07-04T02:51:34.341639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_tfrecs(data_dir):\n    TFRECS = glob(f\"{data_dir}/*\")\n    return TFRECS\ndef load_folds(fold_tfrecs, *args):\n    # Args is the list of all other TFRECS, added to train(pseudo + external)\n    FOLDS = []\n    for i in range(len(fold_tfrecs)):\n        TRAIN = []\n        TEST = []\n        for tfrec in fold_tfrecs:\n            ending = ''\n            for idx in range(len(tfrec)-1, -1, -1):\n                if tfrec[idx] == '/':\n                    ending = tfrec[idx + 1:]\n                    break\n            if str(i) in ending:\n                TEST += [tfrec]\n            else:\n                TRAIN += [tfrec]\n        for arg in args:\n            TRAIN += arg\n        FOLDS += [(TRAIN, TEST)]\n    return FOLDS\n        \nclass DataModule:\n    # -------------Load Dataset----------------\n    data_dir = 'gwd2021'\n    base_dir = f'../input/{data_dir}'\n    SAVE_PATH = './'\n    # Load FOLD idx\n    FOLD_TFRECS = load_tfrecs(base_dir)\n    # Load external Data\n    FOLDS = load_folds(FOLD_TFRECS)\n    # -----------OTHER Data Params-------------------\n    IMG_SHAPE = (1024, 1024, 3) # Massive Fricken Images - But EfficientDet + TPUs can handle it.\n    # Generate Anchors, a Constant.\n    # num Classes = 1. Wheat or No Wheat\n    NUM_CLASSES = 1\n    # Max Number of BBoxes == 516, but to be safe, 1024 cap.\n    MAX_BBOXES = 1024\n    # Total Number\n    TRAIN_NUMBER = sum(1 for _ in tf.data.TFRecordDataset(FOLD_TFRECS))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:43.90983Z","iopub.execute_input":"2021-07-04T02:51:43.910158Z","iopub.status.idle":"2021-07-04T02:51:44.001557Z","shell.execute_reply.started":"2021-07-04T02:51:43.91013Z","shell.execute_reply":"2021-07-04T02:51:44.000656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOAD IN THE DATASET\nFILE_PATH = '../input/gwd-069/submission.csv'\ndf = pd.read_csv(FILE_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:55.794203Z","iopub.execute_input":"2021-07-04T02:51:55.794784Z","iopub.status.idle":"2021-07-04T02:51:55.850348Z","shell.execute_reply.started":"2021-07-04T02:51:55.794734Z","shell.execute_reply":"2021-07-04T02:51:55.84956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CONVERT THE DATASET to regular format(2 Column)\nto_be_df = {}\nfor row in df.iterrows():\n    row = row[1]\n    image_id = row.image_name\n    image_path = f\"../input/gwd-test/train/{image_id}.jpg\"\n    \n    if row.PredString == 'no_box':\n        to_be_df[image_path] = [np.zeros((0, 4))]\n        continue\n\n    \n    ALL_BOUNDING_BOX = [x for x in row.PredString.split(\";\")]\n    for bounding_box in ALL_BOUNDING_BOX:\n        bounding_box = [int(x) for x in bounding_box.split(\" \")]\n        x1, y1, x2, y2 = bounding_box\n    \n        bounding_box = np.array([x1, y1, x2, y2])\n\n        if image_path in to_be_df:\n            to_be_df[image_path] += [bounding_box]\n        else:\n            to_be_df[image_path] = [bounding_box]\n\nfor key in to_be_df:\n    if np.stack(to_be_df[key]).shape == (1, 0, 4):\n        to_be_df[key] = to_be_df[key][0]\n        continue\n    to_be_df[key] = np.stack(to_be_df[key])\n    \n\n    \n# Convert to DF\nnew_df = {'image_name': [], 'bounding_box':  []}\nfor key in to_be_df:\n    new_df['image_name'].append(key)\n    new_df['bounding_box'].append(to_be_df[key])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:55.937901Z","iopub.execute_input":"2021-07-04T02:51:55.939982Z","iopub.status.idle":"2021-07-04T02:51:57.168411Z","shell.execute_reply.started":"2021-07-04T02:51:55.93994Z","shell.execute_reply":"2021-07-04T02:51:57.167481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(new_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:57.169752Z","iopub.execute_input":"2021-07-04T02:51:57.170035Z","iopub.status.idle":"2021-07-04T02:51:57.177431Z","shell.execute_reply.started":"2021-07-04T02:51:57.17001Z","shell.execute_reply":"2021-07-04T02:51:57.176166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value, is_list=False):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    \n    if not is_list:\n        value = [value]\n    \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef _float_feature(value, is_list=False):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value, is_list=False):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        \n    if not is_list:\n        value = [value]\n    \n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\ndef serialize_example(image, bboxes):\n    feature = {\n        'image': _bytes_feature(tf.io.encode_jpeg(image)),\n        'bboxes': _int64_feature(tf.reshape(bboxes, (-1, )), is_list = True)\n    }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\ndef load_image(filepath):\n    loaded = tf.io.read_file(filepath)\n    # Load into a PNG\n    loaded= tf.io.decode_png(loaded, channels = 3)\n    return loaded # Cannot Resize, due to Bounding Box coordinates.\ndef write_tf_recs(df, filepath):\n    # Write a Temporary TFRecord to load in the external data easier.\n    with tf.io.TFRecordWriter(filepath) as writer:\n        for row in tqdm(df.iterrows()):\n            row = row[1]\n            image_path = row.image_name\n            image = load_image(image_path)\n            bounding_boxes = row.bounding_box\n            serialized = serialize_example(image, bounding_boxes)\n            writer.write(serialized)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:57.179784Z","iopub.execute_input":"2021-07-04T02:51:57.180202Z","iopub.status.idle":"2021-07-04T02:51:57.192752Z","shell.execute_reply.started":"2021-07-04T02:51:57.180162Z","shell.execute_reply":"2021-07-04T02:51:57.191717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_tf_recs(df, './dataset.tfrec')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:51:57.194466Z","iopub.execute_input":"2021-07-04T02:51:57.194892Z","iopub.status.idle":"2021-07-04T02:53:50.983076Z","shell.execute_reply.started":"2021-07-04T02:51:57.194848Z","shell.execute_reply":"2021-07-04T02:53:50.982139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# RESOLVE THE ISSUE OF THE EMPTY BOUNDING BOXES\n- Mosaic and Mixup aren't of a normal ratio.\n- It could actually be a good thing, to have these kinds of data\n- Or just turn off mixup/mosaic entirely.\n\n# If you add pseudolabels, everything is resolved.","metadata":{}},{"cell_type":"markdown","source":"# Load 30 Sets of Data(~30 Epochs of Training) and Save under each fold","metadata":{}},{"cell_type":"code","source":"RANDOM_Dataset = iter(tf.data.experimental.RandomDataset())\ndef generate_random_num():\n  seed = next(RANDOM_Dataset)\n  seed = seed % int(1e5)\n  seed = seed / int(1e5)\n  return seed\ndef augment_image(images, bboxes, seeds):\n    # image augs from objects\n    # Single Augmentation\n    #print(images.shape, bboxes.shape)\n    images, bboxes = Augments.resize(images, bboxes, seeds)\n    \n    seed = generate_random_num()\n    # Colour and Image Permutation-------------------------\n    if seed < 0.5:\n      seed = next(RANDOM_Dataset)\n      seed1 = next(RANDOM_Dataset)\n      images = tf.image.stateless_random_brightness(images, 0.2, seed = (seed, seed1))\n    \n    \n    seed = generate_random_num()\n    if seed < 0.5:\n      seed = next(RANDOM_Dataset)\n      seed1 = next(RANDOM_Dataset)\n      images = tf.image.stateless_random_contrast(images, 0.8, 1.2, seed = (seed, seed1))\n    \n    \n    seed = generate_random_num()\n    if seed < 0.25:\n      seed = next(RANDOM_Dataset)\n      seed1 = next(RANDOM_Dataset)\n      images = tf.image.stateless_random_hue(images, 0.1, seed = (seed, seed1))\n    \n    seed = generate_random_num()\n    if seed < 0.5:\n      seed = next(RANDOM_Dataset)\n      seed1 = next(RANDOM_Dataset)\n      images = tf.image.stateless_random_saturation(images, 0.8, 1.2, seed = (seed, seed1))\n\n    seed = generate_random_num()\n    if seed < 0.01:\n      # Grayscale very rarely, but should happen sometimes\n      images = tf.image.rgb_to_grayscale(images)\n      images = tf.concat([images, images, images], axis = -1)\n    \n   \n    seed = generate_random_num()\n    # Noise -----------------\n    if seed < 0.5:\n      seed = next(RANDOM_Dataset)\n      seed1 = next(RANDOM_Dataset)\n      gnoise = tf.random.stateless_normal(shape=tf.shape(images), seed = (seed, seed1), mean=0.0, stddev=0.1, dtype=images.dtype)\n      images = images + gnoise\n    \n    seed = generate_random_num()\n    if seed < 0.5:\n      # Blur the image\n      images = tfa.image.gaussian_filter2d(images)\n    # Rest of Custom Augments all at once\n    \n    for augment in Augments.all_augs:\n      images, bboxes = augment(images, bboxes, seeds)\n    # ----------CLIP the BOUNDING BOXES----------------\n    bboxes = tf.reshape(bboxes, (-1, 5))\n    bboxes = thresh_by_zero(bboxes)\n    bboxes = tf.reshape(bboxes, (-1, 5))\n\n    return images, bboxes\ndef batched_augment_image(images, bboxes, seeds):  \n   \n    if Augments.batched_augs != []:\n        for aug in Augments.batched_augs:\n          images, bboxes = aug(images, bboxes, seeds)\n    \n    return images, bboxes\ndef IMAGE_LOAD(example):\n    feature_dict = {\n        'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=''),\n        'bboxes': tf.io.VarLenFeature(dtype = tf.int64)\n        #'classification': tf.io.VarLenFeature(dtype = tf.int64) \n    }\n    features = tf.io.parse_single_example(example, features=feature_dict)\n    \n    image = features['image']\n    bboxes = features['bboxes'] # Classification Exists, but it's just repetition \n    \n    # Load image\n    image = tf.io.decode_jpeg(image, channels = 3)\n    # Cast to Desired Dtype\n    image = tf.cast(image, tf.float32) / 255.0\n    # Reshape the image \n    image = tf.reshape(image,  DataModule.IMG_SHAPE)\n    # Load the Bounding Boxes\n    bboxes = tf.cast(tf.sparse.to_dense(bboxes), tf.float32)\n    bboxes = tf.reshape(bboxes, (-1, 4)) # (N, 4)\n    # Add in the Bounding Box Classes(1)\n    N = bboxes.shape[0]\n    bboxes = tf.concat([bboxes, tf.ones_like(bboxes)[:, 0:1]], axis = 1)\n    return image, bboxes\ndef get_dfs():\n    # Create TFRecordDatasets\n    train_dataset = tf.data.TFRecordDataset(['./dataset.tfrec'], num_parallel_reads= AUTO)\n    \n    \n    \n    # Options\n    options = tf.data.Options()\n    options.experimental_deterministic = False\n    \n    train_dataset = train_dataset.with_options(options)\n    # Map Locations\n    train_dataset = train_dataset.map(lambda x: IMAGE_LOAD(x), num_parallel_calls = AUTO, deterministic = False)\n    \n    # Create 4 Seeders(For x1, x2, y1, y2)\n    seeder1 = tf.data.experimental.RandomDataset()\n    seeder2 = tf.data.experimental.RandomDataset()\n    seeder3 = tf.data.experimental.RandomDataset()\n    seeder4 = tf.data.experimental.RandomDataset() \n\n    # Zip Together\n    seeders = tf.data.Dataset.zip((seeder1, seeder2, seeder3, seeder4))\n    train_dataset = tf.data.Dataset.zip((train_dataset, seeders))\n    # Augment the image in the train set, all single_wise transforms:\n    train_dataset = train_dataset.map(lambda x, seeds: augment_image(x[0], x[1], seeds), num_parallel_calls = AUTO, deterministic = False)\n\n    # SHUFFLE Datasets\n    train_dataset = train_dataset.shuffle(128) \n    \n    # batch the Dataset\n    # TODO: CREATE CUSTOM BATCHING, ALSO CHECK BOUNDING BOXES BEFORE THIS POINT\n    BATCH_SIZE = 16 # need a batch to get mosaic properly.\n    train_dataset = train_dataset.padded_batch(BATCH_SIZE, padded_shapes = ((Augments.IMAGE_SIZE, Augments.IMAGE_SIZE, 3), (Augments.MAX_BBOX, 5)), drop_remainder = True)\n    \n    seeder1 = tf.data.experimental.RandomDataset()\n    seeder2 = tf.data.experimental.RandomDataset()\n    seeder3 = tf.data.experimental.RandomDataset()\n    seeder4 = tf.data.experimental.RandomDataset() \n\n    # Zip Together\n    seeders = tf.data.Dataset.zip((seeder1, seeder2, seeder3, seeder4))\n    train_dataset = tf.data.Dataset.zip((train_dataset, seeders))\n    \n    # Batched Augments\n    # ENSURE PROPER BATCHING.\n    train_dataset = train_dataset.map(lambda x, seeds: batched_augment_image(x[0], x[1], seeds), num_parallel_calls = AUTO, deterministic = False)\n\n    return train_dataset\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:53:50.984678Z","iopub.execute_input":"2021-07-04T02:53:50.984956Z","iopub.status.idle":"2021-07-04T02:53:51.016823Z","shell.execute_reply.started":"2021-07-04T02:53:50.984928Z","shell.execute_reply":"2021-07-04T02:53:51.015786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = get_dfs()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:53:51.018341Z","iopub.execute_input":"2021-07-04T02:53:51.018711Z","iopub.status.idle":"2021-07-04T02:54:11.820477Z","shell.execute_reply.started":"2021-07-04T02:53:51.01868Z","shell.execute_reply":"2021-07-04T02:54:11.81933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ex in datasets.take(1):\n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:11.822295Z","iopub.execute_input":"2021-07-04T02:54:11.822691Z","iopub.status.idle":"2021-07-04T02:54:43.198089Z","shell.execute_reply.started":"2021-07-04T02:54:11.822651Z","shell.execute_reply":"2021-07-04T02:54:43.197083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_bboxes(images, bboxes, idx):\n    bboxes = bboxes[idx][bboxes[idx, :, -1] == 1.0].numpy()\n    image = images.numpy()[idx]\n    for box in bboxes:\n        if box[-1] == 0:\n          continue\n        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), 220, 3)\n    plt.imshow(image)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:43.200729Z","iopub.execute_input":"2021-07-04T02:54:43.201162Z","iopub.status.idle":"2021-07-04T02:54:43.207587Z","shell.execute_reply.started":"2021-07-04T02:54:43.201122Z","shell.execute_reply":"2021-07-04T02:54:43.206631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Saving Pipeline\n- Run through the Dataset, saving every example","metadata":{}},{"cell_type":"code","source":"def scale_down(image):\n    # MinMax Norm\n    # Image: Tensor(H, W, 3)\n    max_value = tf.reduce_max(image)\n    min_value = tf.reduce_min(image)\n    \n    image = (image - min_value) / (max_value - min_value)\n    image = image * 255.0\n    image = tf.cast(image, dtype = tf.uint8)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:43.209192Z","iopub.execute_input":"2021-07-04T02:54:43.209751Z","iopub.status.idle":"2021-07-04T02:54:43.221256Z","shell.execute_reply.started":"2021-07-04T02:54:43.20971Z","shell.execute_reply":"2021-07-04T02:54:43.220478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 10 # max Number\nSTART_EPOCH = 0","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:43.222492Z","iopub.execute_input":"2021-07-04T02:54:43.223071Z","iopub.status.idle":"2021-07-04T02:54:43.238067Z","shell.execute_reply.started":"2021-07-04T02:54:43.223032Z","shell.execute_reply":"2021-07-04T02:54:43.237349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serialize_example(images, bboxes):\n    feature = {\n        'image': _bytes_feature(tf.io.encode_jpeg(images)),\n        'bboxes': _int64_feature(tf.reshape(bboxes, (-1, )), is_list = True),\n        #'classification': _int64_feature(bboxes[:, -1], is_list = True)\n    }\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\ndef write_tf_dataset(num_epochs, start_epoch, fold_idx, is_test = False):\n    # Load in the dataset (1 Fold worth)\n    dataset = get_dfs()\n    for epoch_idx in range(num_epochs):\n        epoch_idx = epoch_idx + start_epoch \n        file_path = f\"{epoch_idx}_fold_{fold_idx}.tfrec\"\n        with tf.io.TFRecordWriter(file_path) as writer:\n            count = 0\n            import time\n            cur_time = time.time()\n            for images, bboxes in dataset:\n                B = images.shape[0]\n                for b in range(B):\n                    img = scale_down(images[b])\n                    bbox = bboxes[b]\n                    try:\n                        ex = serialize_example(img, bbox)\n                        writer.write(ex)\n                        count += 1\n                        #print(count)\n                        if count % 10 == 0:\n                            print(count)\n                    except:\n                        pass\n            ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:43.239428Z","iopub.execute_input":"2021-07-04T02:54:43.239998Z","iopub.status.idle":"2021-07-04T02:54:43.252184Z","shell.execute_reply.started":"2021-07-04T02:54:43.239954Z","shell.execute_reply":"2021-07-04T02:54:43.251433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLD_NUM = 5","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:43.253575Z","iopub.execute_input":"2021-07-04T02:54:43.254159Z","iopub.status.idle":"2021-07-04T02:54:43.264415Z","shell.execute_reply.started":"2021-07-04T02:54:43.254116Z","shell.execute_reply":"2021-07-04T02:54:43.263354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_tf_dataset(NUM_EPOCHS, START_EPOCH, FOLD_NUM, is_test = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T02:54:43.265792Z","iopub.execute_input":"2021-07-04T02:54:43.26612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}